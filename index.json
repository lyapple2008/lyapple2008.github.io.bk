[{"content":" 记录跟踪2026年要做的事情和正在做的事情。\n目标 健康生活\n体重减到60kg 探索可能\n英语能力，熟练3篇演讲短文 独立开发，目标完成4个独立项目 公众号输出，目标输出文章10篇 正在做的项目 iOS双语字幕App\n30天每天一万步（2026.02.22~2026.03.22）\n五万资产配置计划\n","permalink":"https://lyapple2008.github.io/posts/2026-todo/","summary":"\u003cblockquote\u003e\n\u003cp\u003e记录跟踪2026年要做的事情和正在做的事情。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch1 id=\"目标\"\u003e目标\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e健康生活\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e 体重减到60kg\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003cli\u003e\n\u003cp\u003e探索可能\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e 英语能力，熟练3篇演讲短文\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e 独立开发，目标完成4个独立项目\u003c/li\u003e\n\u003cli\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e 公众号输出，目标输出文章10篇\u003c/li\u003e\n\u003c/ul\u003e\n\u003c/li\u003e\n\u003c/ul\u003e\n\u003ch1 id=\"正在做的项目\"\u003e正在做的项目\u003c/h1\u003e\n\u003cul\u003e\n\u003cli\u003e\n\u003cp\u003e\u003cinput disabled=\"\" type=\"checkbox\"\u003e iOS双语字幕App\u003c/p\u003e","title":"【置顶】2026"},{"content":" 这篇继续是开发日志，正在开发一款iOS端的实时双语字幕APP，由于需要用到语音识别，了解了下语音识别任务的现状和主流方案，为后面方案选择做准备。\n模块流程 graph LR A[捕获系统播放音频] --\u0026gt; B[语音识别ASR] B --\u0026gt; C[语言翻译] B --\u0026gt; D[双语字幕] C --\u0026gt; D 什么是ASR任务 ASR（Automatic Speech Recognition，自动语音识别）任务，指的是将连续的音频信号转换为对应的文本序列的过程。这是一种典型的序列到序列（Sequence-to-Sequence）的转换任务：\n1 audio (连续信号) → text (离散 token) 从信号处理的角度来看，传统ASR需要解决以下核心问题：\n声学建模：将音频特征映射到音素或字符 语言建模：捕捉词汇之间的概率关系 对齐问题：音频帧与输出token之间的对齐 不过，在现代端到端深度学习方案中，这三个问题都被统一在一个神经网络中解决，不再需要独立的声学模型和语言模型。模型通过端到端训练自动学习如何从音频特征直接映射到文本输出。\n与降噪任务的区别 在开发iOS双语字幕的过程中，我之前可能接触过降噪任务，这两者有本质区别：\n维度 降噪任务 ASR任务 输入输出 音频 → 音频 音频 → 文本 任务类型 信号回归 序列到序列 评估指标 SNR、PESQ WER、CER 难点 保留语音质量 识别准确性 模型结构 Encoder Encoder-Decoder 简单来说：\n降噪任务：输入一段有噪声的音频，输出干净的音频（同类转换） ASR任务：输入音频，输出文字（跨模态转换） ASR的难点在于它需要\u0026quot;理解\u0026quot;音频内容并转换为语义符号，而不是简单地处理信号波形。同时，ASR任务的输入和输出也不像降噪任务那样是一一对应的关系，还需要处理对齐问题。\n目前主流的实现方案 主流的ASR实现方案主要有三种：CTC、RNN-T和基于Attention的Seq2Seq。\nCTC (Connectionist Temporal Classification) CTC是一种经典的对齐方法，核心思想是不需要显式的对齐标签，而是通过\u0026quot;blank\u0026quot;机制自动学习对齐。\nCTC引入了空白符（blank）和折叠机制：\n重复的字符会被折叠（如 \u0026ldquo;aaabbb\u0026rdquo; → \u0026ldquo;ab\u0026rdquo;） blank符号不产生任何输出 通过动态规划计算所有可能路径的概率 Sequence Modeling With CTC 详细原理可参考这篇文章。\n训练阶段就是使目标token序列路径的概率最大，而推理阶段就是搜索概率最大的token路径并输出，这里通常有两种方法：\n方法 描述 Greedy Search 每一步都选择概率最大的token输出 Beam Search 每一步保留top-N概率的token结果，输出token序列路径概率最大的结果 RNN-T (Recurrent Neural Network Transducer) RNN-T是CTC的扩展，引入了一个额外的预测网络（Prediction Network）来建模输出token之间的依赖关系。\nRNN-T由三部分组成：\n编码器（Encoder）：将音频特征转换为声学表征 预测网络（Prediction Network）：基于已输出的token预测下一个token 联合网络（Joint Network）：结合Encoder和Prediction的输出，预测下一个token RNN-T在输出时，不仅会输入当前帧音频数据，还会输入历史输出作为参考\nSeq2Seq with Attention 基于Attention机制的序列到序列模型是目前最流行的方案，被广泛用于Whisper、Paraformer等现代ASR系统。\n原理 经典结构：\n1 Encoder → Attention → Decoder → Output Encoder：将音频特征编码为高维表征（通常使用Transformer或Conformer） Attention：让Decoder在生成每个token时\u0026quot;关注\u0026quot;输入的不同部分 Decoder：自回归生成输出文本 优缺点 优点：\n可以建模任意长度序列的依赖关系 识别准确率高 易于添加语言模型集成 适合大规模预训练 缺点：\n推理延迟较高（需要完整音频或较大chunk） 流式识别实现复杂 计算资源需求大 流式模型和非流式模型 流式（Streaming）和非流式（Offline）是ASR系统根据实时性要求的两种设计模式。\n什么是非流式模型 非流式模型（Offline ASR）需要等待完整音频输入后才能开始识别。\n特点：\n输入：完整的音频文件或长音频段 延迟：较高，需要等待音频结束 准确率：通常较高，因为可以看到完整的上下文 适用场景：视频字幕生成、会议转录、录音文件处理 什么是流式模型 流式模型（Streaming ASR）可以边接收音频输入边输出识别结果，实现实时识别。\n特点：\n输入：连续的音频流（通常是短片段，如30ms一帧） 延迟：低，可以做到几百毫秒内输出 准确率：通常略低于非流式，因为只有历史和部分未来上下文 适用场景：实时语音对话、语音助手、直播字幕 技术实现差异 维度 流式模型 非流式模型 上下文 有限上下文（lookahead） 完整上下文 延迟 低（\u0026lt;500ms） 高 准确率 略低 较高 模型复杂度 较高（需处理分段） 较低 内存占用 较小 较大 流式模型通常需要特殊设计，如：\nChunked Attention：将音频分成小块处理 CTC Prefix：使用CTC的前缀解码 Lookahead：只考虑有限的未来帧 自回归解码和非自回归解码 解码方式决定了模型如何生成输出文本。\n自回归解码 (Autoregressive Decoding) 自回归解码是目前最主流的方式，特点是逐 token 生成，每个 token 的生成依赖之前所有生成的 token。\n1 2 3 4 5 6 输出: \u0026#34;hello world\u0026#34; 生成过程: 1. 生成 \u0026#34;h\u0026#34; 2. 基于 \u0026#34;h\u0026#34; 生成 \u0026#34;he\u0026#34; 3. 基于 \u0026#34;he\u0026#34; 生成 \u0026#34;hel\u0026#34; 4. ... 特点：\n优点：生成质量高，可以建模长期依赖 缺点：串行生成，推理速度慢（O(n) 复杂度，n为输出长度） 典型模型：Transformer Decoder、RNN-T 非自回归解码 (Non-autoregressive Decoding) 非自回归解码是一种并行生成方式，一次性输出整个序列。\n1 2 3 输出: \u0026#34;hello world\u0026#34; 生成过程: 1. 直接输出完整句子 \u0026#34;hello world\u0026#34; 特点：\n优点：并行生成，推理速度快（O(1) 复杂度） 缺点：难以建模输出token之间的依赖，生成质量可能较低 典型实现：CTC、FastCorrect、Mask-Predict 对比 维度 自回归解码 非自回归解码 生成方式 串行，逐token 并行，一次性输出 推理速度 慢 快 生成质量 高 略低 依赖关系 建模token间依赖 假设条件独立 典型模型 RNN-T、Seq2Seq CTC、FastConformer 方案选择 OK，前面了解了这么多，都是为了后续实现最小原型产品，选择语音识别方案做准备，目标不是要训练一个SOTA模型，因此这里暂时只是粗浅的了解。\n根据需求分析：\n目标设备：iOS移动端（资源有限） 语言支持：多语言 实时性：实时输入音频，实时输出文字 选择优先级 首先能跑：模型大小和计算量必须在移动端可承受范围内 然后多语言：需要支持多种语言识别 最后准确率：在功能可用后再优化性能 优先级一：模型能在移动端跑起来 参数量选择 移动端资源有限，模型参数量直接决定了能否运行。这里暂时没有明确的数值界限，实际运行起来再看，后面也可以用量化技术缩小模型体积。\n量化技术：INT8量化可将模型体积缩小约4倍，准确率损失通常\u0026lt;5%；INT4可进一步缩小到1/8，但准确率下降更明显。\n架构选择 架构 计算量 内存占用 移动端适用性 Transformer Encoder 中 中 ★★★★☆ Conformer 中高 中 ★★★★☆ RNN/LSTM 低 低 ★★★☆☆ 建议：选择Encoder-only或轻量级的Conformer结构，参数量控制在50M以内。\n优先级二：支持多语言 确认模型能在移动端运行后，需要考虑多语言支持能力。\n不同模型架构的多语言能力 模型架构 多语言支持 说明 端到端Seq2Seq ★★★★★ 训练时使用多语言数据，自然支持多语言 RNN-T ★★★☆☆ 可支持，但需要针对性训练多语言版本 CTC ★★☆☆☆ 通常针对单一语言，多语言版本较少 结论 需要支持多语言时，优先选择端到端Seq2Seq架构，这类模型的预训练版本通常已支持数十到上百种语言。\n优先级三：实时性要求 实时字幕要求模型能够在接收音频的同时输出文字。\n流式 vs 非流式 类型 延迟 实现难度 实时字幕适用性 流式模型 \u0026lt;500ms 高 ★★★★★ 非流式模型 \u0026gt;1s 低 ★★☆☆☆ 折中方案：使用非流式模型时，可通过\u0026quot;分块处理\u0026quot;策略模拟流式效果：\n将音频切分为固定长度的chunk（如1秒） 逐块识别并拼接结果 通过缓存历史上下文减少误差 解码方式 解码方式 延迟 实现复杂度 实时性 非自回归（Greedy） 低 简单 ★★★★★ 非自回归（Beam Search） 中 中 ★★★★☆ 自回归 高 复杂 ★★☆☆☆ 建议：实时场景优先选择非自回归解码（Greedy），延迟最低。\n总结 核心理念：先完成端到端流程验证，再根据实际体验进行针对性优化。移动端ASR是一个迭代过程，不必追求一步到位。\n后续我会记录在iOS端的具体实现过程，各位道友记得点赞追番哦。\n","permalink":"https://lyapple2008.github.io/posts/202602/2026-02-14-asr%E4%BB%BB%E5%8A%A1%E5%88%9D%E4%BD%93%E9%AA%8C/","summary":"\u003cblockquote\u003e\n\u003cp\u003e这篇继续是开发日志，正在开发一款iOS端的实时双语字幕APP，由于需要用到语音识别，了解了下语音识别任务的现状和主流方案，为后面方案选择做准备。\u003c/p\u003e","title":"ASR任务初体验"},{"content":" 最近要说最热最火的话题，那一定是 AI，而其中 AI 应用场景 AI Coding 也是发展迅速，大有替代程序员的势头。最近几个月一直在使用 AI Coding，心态上经历了最初的「完了，失业要提前了」，到「稳了，还能苟几年」。下面就分享一些使用上的感受，以及面对 AI Coding 这股后浪，这个号后面的一些规划。\n1. AI Coding 非常有帮助，收费的优于免费的 这几年 AI 发展迅速，早已不是当年的「人工智障」了，确实是可以作为生产力工具帮忙提升效率的。对于 AI Coding 来说，到底能提升多少效率，取决于多方面因素：比如 Coding 在日常开发中占的比重、使用 AI 的熟练程度、AI Coding 工具本身的性能等等。但不管怎么说，AI Coding 对于程序员来说都是有正向作用的，是值得花时间去用起来的。\n我自己用得比较多的是 Cursor 跟 Claude Code。Cursor 是公司给配置的，Claude Code 是我自己订阅了 MiniMax 的 Coding Plan 配置的。在此之前也用过国内一些免费的 AI Coding 工具（比如 Trae 和 Qcoder），短暂的使用给我的感受就是没有付费的好用，一个问题反复折腾好几轮都没有跑通。\n当然这些纯粹是个人拍脑袋的感受，使用时间不长，也没有很深入地去使用。网上也有一些使用 Trae 和 Qcoder 做出可用应用的案例。重要的是 AI Coding 确实能够提升开发效率的，大家都应该用起来。\n2. AI Coding 的能力约等于使用者自身 AI Coding 在目前这个阶段还是一个被动的工具，需要使用者去给它进行规划和任务指引，因此使用者的能力边界就约等于使用 AI Coding 后的能力边界。\n程序开发是一个复杂的系统工程，每个需求拆分任务后就有很多步骤。就算 AI Coding 每一步的成功率为 95%，8 步之后最终能成功的概率也只剩下 66%。另外，AI Coding 获取的需求信息主要来源于使用者，使用者在与 AI 沟通描述时，这里又得损耗一些性能，导致成功率下降。不过好消息是，使用者可以在与 AI 互动过程中不断扩展自己的能力边界，把原来了解的领域变成熟悉领域，所以理论上 AI Coding 是没有边界的。\n这里给我几点启发：\n生产环境只在自己熟悉的范围内使用，最多只触及到了解的区域，再往外扩展能得到什么成果就只能听天由命了，试验性的尝试性的项目不受这个限制。 给 AI 描述需求时，尽可能详细，就像你自己开发一样，想好整个代码的架构和目标，并记录成文档。文档不仅可以给 AI 描述需求，也可以随时给 AI 找回上下文，弥补 AI Coding 上下文有限的问题。 大龄程序员凭借多年积累的经验，辅以AI Coding弥补”编程体力不足“，又可以焕发第二春了。 3. AI Coding 还只是辅助编程，使用者需要为其质量兜底 类似于汽车自动驾驶，AI Coding 现在也还只是 L2 水平，辅助编程阶段，出了事故还是得使用者来负责。所以对于生产环境使用 AI Coding 时，要对 AI Coding 生成的代码进行严格的 Review。\n不过最近也看到另一种解法：随着 AI Coding 生成的代码量级快速上升，传统逐行代码 Review 的方式已经不太现实，而且效率不高。所以也有不 Review 只测试的方式来验收 AI Coding 生成的代码，在修改和生成代码的同时，让AI也生成对应的单元测试，人类负责把关单元测试的通过情况，可能也许也是一种可行的AI Coding协同方式。\n后续的规划 随着 AI 的普及和渗透，获取知识、甚至获取技能都越来越轻而易举。坏的方面是，个人的知识和技能越来越贬值；乐观的方面是，个人可以利用的知识和技能也是越来越多，也越来越轻松。\n最近 OpenClaw 非常火，Peter Steinberger 是其核心开发者。从其 GitHub 主页可以看到，在 Claude 爆火之前，他一直在做迭代各种各样的工具或者产品（具有完整定义功能的程序就算产品，不一定是像 WeChat/WhatsApp/TikTok 这样才算），OpenClaw的爆火不过是其多年程序迭代的一次厚积薄发。后面希望可以学习他这种方式，在做中学，在学中做，这也许是 AI 时代最快的进步方式。\n后面将不再进行纯粹的单点知识分享，希望可以从打造一个真实可用的产品角度去分享打造的过程。这个产品只需要能提供完整可用的功能就行。\n在这个知识和技能获取越来越便捷的时代，迭代速度也许才是制胜法宝。\nOK，以上就是一些个人浅显的、混乱的感想和 2026 年的 Flag，剩下的就交给时间和执行了，各位道友记得点赞追番一起加油吧！\n","permalink":"https://lyapple2008.github.io/posts/202601/2026-02-01-%E5%85%B3%E4%BA%8Eaicoding%E7%9A%84%E4%B8%80%E4%BA%9B%E6%84%9F%E6%83%B3%E5%92%8C%E5%90%8E%E7%BB%AD%E7%9A%84%E8%A7%84%E5%88%92/","summary":"\u003cblockquote\u003e\n\u003cp\u003e最近要说最热最火的话题，那一定是 AI，而其中 AI 应用场景 AI Coding 也是发展迅速，大有替代程序员的势头。最近几个月一直在使用 AI Coding，心态上经历了最初的「完了，失业要提前了」，到「稳了，还能苟几年」。下面就分享一些使用上的感受，以及面对 AI Coding 这股后浪，这个号后面的一些规划。\u003c/p\u003e","title":"关于AICoding的一些感想和后续的规划"},{"content":" 这篇是iOS双语字幕软件的开发日志，目标是在iOS端实现，在观看视频时，实时对播放的内容进行识别和翻译，显示双语字幕，用于打破外语视频内容观看门槛。\n模块流程 graph LR A[捕获系统播放音频] --\u0026gt; B[语音识别ASR] B --\u0026gt; C[语言翻译] B --\u0026gt; D[双语字幕] C --\u0026gt; D iOS音频捕获与数据共享 本文介绍iOS系统音频捕获的实现方案，使用Broadcast Upload Extension捕获系统播放的音频，并通过App Group与主应用共享数据。\n目录 一、系统音频捕获 Broadcast Upload Extension配置 启动与关闭 音频格式转换 二、Extension与主应用数据共享 App Group配置 数据读写实现 Darwin通知 一、系统音频捕获 iOS系统出于安全和隐私考虑，不允许应用直接捕获系统音频（如视频播放、音乐等，使用通话模式的APP播放的声音捕获不到）。必须使用Broadcast Upload Extension，通过屏幕录制的形式获取音频数据。\nBroadcast Upload Extension配置 要让Extension收到ReplayKit的数据，必须同时满足：\n工程里有Broadcast Upload Extension target 主App用系统UI启动broadcast Extension的Info.plist / Capabilities / 类继承全部正确 创建步骤：\n在Xcode中新建Extension类型中选择Broadcast Upload Extension\n配置Info.plist：\n1 2 3 4 5 6 7 \u0026lt;key\u0026gt;NSExtension\u0026lt;/key\u0026gt; \u0026lt;dict\u0026gt; \u0026lt;key\u0026gt;NSExtensionPointIdentifier\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;com.apple.broadcast-services-upload\u0026lt;/string\u0026gt; \u0026lt;key\u0026gt;NSExtensionPrincipalClass\u0026lt;/key\u0026gt; \u0026lt;string\u0026gt;$(PRODUCT_MODULE_NAME).SampleHandler\u0026lt;/string\u0026gt; \u0026lt;/dict\u0026gt; 主App配置UIBackgroundModes： 1 2 3 4 \u0026lt;key\u0026gt;UIBackgroundModes\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;audio\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; 注意：未配置audio可能导致音频接收不到或屏幕锁定后Extension被暂停。\n启动与关闭 Broadcast upload extension不能在代码中直接启动，只能由系统UI触发。Extension只能自己调用finishBroadcastWithError关闭，主App只能\u0026quot;间接控制\u0026quot;关闭。\n1 2 3 4 5 6 let picker = RPSystemBroadcastPickerView( frame: CGRect(x: 0, y: 0, width: 44, height: 44) ) picker.preferredExtension = \u0026#34;com.xxx.broadcast\u0026#34; picker.showsMicrophoneButton = true view.addSubview(picker) 音频格式转换 语音识别引擎接收的音频格式需要是16kHz单声道音频，因此这里需要先进行格式转换。这里需要注意是，并没有官方文档说ReplayKit回调的数据格式类型是怎样的，因此这里需要兼容各种格式。\n格式检测与提取：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 override func processSampleBuffer(_ sampleBuffer: CMSampleBuffer, with sampleBufferType: RPSampleBufferType) { guard case .audioApp = sampleBufferType else { return } guard let formatDescription = CMSampleBufferGetFormatDescription(sampleBuffer), let streamDesc = CMAudioFormatDescriptionGetStreamBasicDescription(formatDescription)?.pointee else { return } let inputSampleRate = streamDesc.mSampleRate let channelCount = Int(streamDesc.mChannelsPerFrame) let bitsPerChannel = streamDesc.mBitsPerChannel let formatFlags = streamDesc.mFormatFlags let isFloat = (formatFlags \u0026amp; kAudioFormatFlagIsFloat) != 0 let isNonInterleaved = (formatFlags \u0026amp; kAudioFormatFlagIsNonInterleaved) != 0 let isBigEndian = (formatFlags \u0026amp; kAudioFormatFlagIsBigEndian) != 0 // 提取音频数据... } 完整的音频处理实现：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 private let targetSampleRate: Double = 16000.0 /// 处理音频样本buffer private func processAudioBuffer(_ sampleBuffer: CMSampleBuffer, source: String) { guard let formatDescription = CMSampleBufferGetFormatDescription(sampleBuffer), let streamDesc = CMAudioFormatDescriptionGetStreamBasicDescription(formatDescription)?.pointee else { return } let inputSampleRate = streamDesc.mSampleRate let channelCount = Int(streamDesc.mChannelsPerFrame) let bitsPerChannel = streamDesc.mBitsPerChannel let formatFlags = streamDesc.mFormatFlags let isFloat = (formatFlags \u0026amp; kAudioFormatFlagIsFloat) != 0 let isNonInterleaved = (formatFlags \u0026amp; kAudioFormatFlagIsNonInterleaved) != 0 let isBigEndian = (formatFlags \u0026amp; kAudioFormatFlagIsBigEndian) != 0 // 获取 AudioBufferList var audioBufferList = AudioBufferList() var blockBuffer: CMBlockBuffer? let status = CMSampleBufferGetAudioBufferListWithRetainedBlockBuffer( sampleBuffer, bufferListSizeNeededOut: nil, bufferListOut: \u0026amp;audioBufferList, bufferListSize: MemoryLayout\u0026lt;AudioBufferList\u0026gt;.size, blockBufferAllocator: nil, blockBufferMemoryAllocator: nil, flags: kCMSampleBufferFlag_AudioBufferList_Assure16ByteAlignment, blockBufferOut: \u0026amp;blockBuffer ) guard status == noErr else { return } let audioBufferListPointer = UnsafeMutableAudioBufferListPointer( UnsafeMutablePointer\u0026lt;AudioBufferList\u0026gt;.allocate(capacity: 1) ) defer { audioBufferListPointer.unsafeMutablePointer.deallocate() } let numBuffers = audioBufferListPointer.count let frameCount = CMSampleBufferGetNumSamples(sampleBuffer) var floatSamples: [Float] = [] // 处理非交错格式 if isNonInterleaved { var channelData: [[Float]] = [] for bufferIndex in 0..\u0026lt;numBuffers { let buffer = audioBufferListPointer[bufferIndex] guard let data = buffer.mData else { continue } let dataByteSize = Int(buffer.mDataByteSize) var channelSamples: [Float] = [] if isFloat \u0026amp;\u0026amp; bitsPerChannel == 32 { let floatPtr = data.assumingMemoryBound(to: Float.self) let count = dataByteSize / MemoryLayout\u0026lt;Float\u0026gt;.size for i in 0..\u0026lt;count { var value = floatPtr[i] if isBigEndian { value = Float(bitPattern: value.bitPattern.bigEndian) } channelSamples.append(value) } } else if bitsPerChannel == 16 { let int16Ptr = data.assumingMemoryBound(to: Int16.self) let count = dataByteSize / MemoryLayout\u0026lt;Int16\u0026gt;.size for i in 0..\u0026lt;count { var value = int16Ptr[i] if isBigEndian { value = value.bigEndian } channelSamples.append(Float(value) / 32768.0) } } channelData.append(channelSamples) } // 混音为单声道 if let firstChannel = channelData.first { if channelData.count == 1 { floatSamples = firstChannel } else { for i in 0..\u0026lt;firstChannel.count { var sum: Float = 0 for ch in channelData where i \u0026lt; ch.count { sum += ch[i] } floatSamples.append(sum / Float(channelData.count)) } } } } else { // 交错格式处理... } // 重采样到16kHz if inputSampleRate != targetSampleRate { floatSamples = resample(floatSamples, from: inputSampleRate, to: targetSampleRate) } sharedBuffer.writeAudioSamples(floatSamples) } /// 使用AVAudioConverter重采样 private func resample(_ samples: [Float], from inputRate: Double, to outputRate: Double) -\u0026gt; [Float] { guard inputRate \u0026gt; 0 \u0026amp;\u0026amp; outputRate \u0026gt; 0, inputRate != outputRate, !samples.isEmpty else { return samples } guard let inputFormat = AVAudioFormat( commonFormat: .pcmFormatFloat32, sampleRate: inputRate, channels: 1, interleaved: false ), let outputFormat = AVAudioFormat( commonFormat: .pcmFormatFloat32, sampleRate: outputRate, channels: 1, interleaved: false ), let converter = AVAudioConverter(from: inputFormat, to: outputFormat) else { return samples } guard let inputBuffer = AVAudioPCMBuffer(pcmFormat: inputFormat, frameCapacity: AVAudioFrameCount(samples.count)), let outputBuffer = AVAudioPCMBuffer(pcmFormat: outputFormat) else { return samples } inputBuffer.frameLength = AVAudioFrameCount(samples.count) let inputData = inputBuffer.floatChannelData! for i in 0..\u0026lt;samples.count { inputData[0][i] = samples[i] } let ratio = outputRate / inputRate let outputFrameCount = Int(ceil(Double(samples.count) * ratio)) outputBuffer.frameCapacity = AVAudioFrameCount(outputFrameCount) var error: NSError? let status = converter.convert(to: outputBuffer, error: \u0026amp;error) { _, outStatus in outStatus.pointee = .haveData return inputBuffer } guard status == .haveData, error == nil else { return samples } let outputData = outputBuffer.floatChannelData! let outputLength = Int(outputBuffer.frameLength) return (0..\u0026lt;outputLength).map { outputData[0][$0] } } 实现要点：\n内存安全：使用 UnsafeMutableAudioBufferListPointer 和 defer 确保内存正确释放 多格式支持：支持 Float32、Int16、Int32 格式 字节序处理：支持大端和小端字节序 非交错格式：正确处理每个通道独立 buffer 的格式 混音：立体声自动混音为单声道 高质量重采样：使用系统 AVAudioConverter 二、Extension与主应用数据共享 Broadcast Extension与主应用运行在不同进程，涉及到进程间通信，这里选择使用实现比较简单的App Group共享容器进行数据交换。\nApp Group配置 Entitlements配置：\n1 2 3 4 \u0026lt;key\u0026gt;com.apple.security.application-groups\u0026lt;/key\u0026gt; \u0026lt;array\u0026gt; \u0026lt;string\u0026gt;group.com.xxx.shared\u0026lt;/string\u0026gt; \u0026lt;/array\u0026gt; 需要在Apple Developer Portal创建App Group，并在XCode中为两个target启用。\n数据读写实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 class AudioSharedBuffer { static let appGroupId = \u0026#34;group.com.xxx.shared\u0026#34; private let sharedContainerURL = FileManager.default.containerURL( forSecurityApplicationGroupIdentifier: Self.appGroupId ) // Extension写入处理后的音频 func writeAudioSamples(_ samples: [Float]) { guard let url = sharedContainerURL?.appendingPathComponent(\u0026#34;audio.raw\u0026#34;) else { return } let data = samples.withUnsafeBufferPointer { Data(buffer: $0) } if FileManager.default.fileExists(atPath: url.path) { let handle = try? FileHandle(forWritingTo: url) handle?.seekToEndOfFile() handle?.write(data) handle?.closeFile() } else { try? data.write(to: url) } postDarwinNotification(\u0026#34;com.xxx.newAudioData\u0026#34;) } // 主应用读取音频 func readAudioSamples() -\u0026gt; [Float]? { guard let url = sharedContainerURL?.appendingPathComponent(\u0026#34;audio.raw\u0026#34;), FileManager.default.fileExists(atPath: url.path) else { return nil } let data = try? Data(contentsOf: url) try? FileManager.default.removeItem(at: url) let floatCount = data?.count ?? 0 / MemoryLayout\u0026lt;Float\u0026gt;.size var samples = [Float](repeating: 0, count: floatCount) data?.copyBytes(to: samples.withUnsafeMutableBufferPointer { $0 }) return samples } private func postDarwinNotification(_ name: String) { let center = CFNotificationCenterGetDarwinNotifyCenter() CFNotificationCenterPostNotification(center, CFNotificationName(name as CFString), nil, nil, true) } } Darwin通知 Extension写入数据后发送Darwin通知，主应用监听后立即读取：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 func startListening() { CFNotificationCenterAddObserver( CFNotificationCenterGetDarwinNotifyCenter(), Unmanaged.passUnretained(self).toOpaque(), { _, observer, _, _, _ in guard let observer = observer else { return } let selfPtr = Unmanaged\u0026lt;YourClass\u0026gt;.fromOpaque(observer).takeUnretainedValue() if let samples = selfPtr.audioBuffer.readAudioSamples() { selfPtr.onAudioReceived?(samples) } }, \u0026#34;com.xxx.newAudioData\u0026#34; as CFString, nil, .deliverImmediately ) } SampleHandler完整示例 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 class SampleHandler: RPBroadcastSampleHandler { private let sharedBuffer = AudioSharedBuffer() private let targetSampleRate: Double = 16000.0 override func broadcastStarted(withSetupInfo setupInfo: [String : NSObject]?) { sharedBuffer.clearAudioData() } override func broadcastPaused() {} override func broadcastResumed() {} override func broadcastFinished() {} override func processSampleBuffer(_ sampleBuffer: CMSampleBuffer, with sampleBufferType: RPSampleBufferType) { switch sampleBufferType { case .audioApp: // 处理应用音频（系统播放的音频） let samples = convertTo16kMono(sampleBuffer) sharedBuffer.writeAudioSamples(samples) case .audioMic: // 忽略麦克风音频 break case .video: // 忽略视频 break } } } ","permalink":"https://lyapple2008.github.io/posts/202601/2026-01-25-ios%E9%9F%B3%E9%A2%91%E6%8D%95%E8%8E%B7/","summary":"\u003cblockquote\u003e\n\u003cp\u003e这篇是iOS双语字幕软件的开发日志，目标是在iOS端实现，在观看视频时，实时对播放的内容进行识别和翻译，显示双语字幕，用于打破外语视频内容观看门槛。\u003c/p\u003e","title":"iOS音频捕获"},{"content":"在深度学习语音降噪模型的部署过程中，选择合适的推理引擎至关重要。ONNX Runtime（ORT）作为微软开源的跨平台推理引擎，在性能、兼容性和易用性方面表现出色，已成为许多生产环境的首选。本文将介绍为什么选择ORT，ORT的核心概念和使用流程，以及在使用ORT进行语音降噪推理时需要注意的关键事项，特别是针对时序模型（如GRU/LSTM）的隐状态管理。\n一、为什么选择ORT？ 1.1 跨平台支持 ORT提供了广泛的平台支持，包括：\nCPU推理：支持x86、ARM等架构，可在Windows、Linux、macOS、Android、iOS等系统运行 GPU加速：支持CUDA（NVIDIA GPU）、DirectML（Windows）、TensorRT等 专用硬件：支持CoreML（Apple Silicon）、OpenVINO（Intel）、QNN（Qualcomm）等 这种跨平台特性使得同一套代码可以在不同设备上运行，大大降低了部署成本。\n1.2 性能优化 ORT在性能方面做了大量优化：\n图优化：自动进行算子融合、常量折叠、死代码消除等优化 执行提供者（Execution Provider）：针对不同硬件提供专门的优化实现 动态形状支持：支持动态batch size和序列长度，适合实时推理场景 1.3 模型格式标准化 ORT基于ONNX（Open Neural Network Exchange）格式，这是业界标准的模型交换格式：\n框架无关：可以从PyTorch、TensorFlow、Keras等框架导出ONNX模型 版本兼容：ONNX规范持续演进，ORT保持向后兼容 工具生态：丰富的模型转换和优化工具 1.4 易于集成 ORT提供了多种语言绑定：\nC++ API：适合高性能场景和嵌入式设备 Python API：便于快速原型开发和调试 C#、Java、JavaScript：支持多种应用场景 1.5 活跃的社区支持 作为微软开源项目，ORT拥有活跃的社区和持续的更新，bug修复和新功能迭代速度快。\n二、ORT基本概念与推理流程 2.1 核心概念 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 ┌───────────────────────────────┐ │ OrtEnv （运行时环境） │ │ └─ 管理全局资源、线程池等 │ └──────────────┬────────────────┘ │ ┌──────────────┴────────────────┐ │ OrtSession （推理会话） │ │ └─ 持有已加载的 ONNX 模型 │ └──────────────┬────────────────┘ │ ┌──────────────┴────────────────────────┐ │ OrtRun（一次推理调用） │ │ ├─ 输入 OrtValue (Tensor 等) │ │ ├─ 输出 OrtValue │ │ └─ 在 Env/Session 的线程池中执行 │ └────────────────────────────────────────┘ OrtEnv（运行时环境） OrtEnv是ORT的全局运行时环境，负责管理线程池、日志等全局资源。通常一个进程只需要创建一个OrtEnv实例：\n1 2 3 4 5 6 7 8 9 10 #include \u0026lt;onnxruntime_c_api.h\u0026gt; // 创建运行时环境 OrtEnv* env = NULL; OrtStatus* status = OrtCreateEnv(ORT_LOGGING_LEVEL_WARNING, \u0026#34;ORT\u0026#34;, \u0026amp;env); if (status != NULL) { // 错误处理 const char* msg = OrtGetErrorMessage(status); OrtReleaseStatus(status); } OrtSession（推理会话） OrtSession负责加载ONNX模型并执行推理。创建会话需要先创建会话选项：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 #include \u0026lt;onnxruntime_c_api.h\u0026gt; // 1. 创建会话选项 OrtSessionOptions* session_options = NULL; OrtCreateSessionOptions(\u0026amp;session_options); // 2. 创建推理会话 OrtSession* session = NULL; const char* model_path = \u0026#34;denoise_model.onnx\u0026#34;; status = OrtCreateSession(env, model_path, session_options, \u0026amp;session); if (status != NULL) { // 错误处理 const char* msg = OrtGetErrorMessage(status); OrtReleaseStatus(status); } // 3. 释放资源（使用完毕后） OrtReleaseSessionOptions(session_options); OrtReleaseSession(session); OrtReleaseEnv(env); Execution Provider (EP) 执行提供者决定了模型在哪个硬件上运行。在C API中，通过OrtSessionOptionsAppendExecutionProvider添加EP：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 // CPU执行（默认，无需显式添加） // 直接创建会话即可使用CPU // CUDA执行（需要NVIDIA GPU） OrtSessionOptionsAppendExecutionProvider_CUDA(session_options, 0); // TensorRT执行（需要NVIDIA GPU和TensorRT） OrtTensorRTProviderOptions trt_options = {0}; OrtSessionOptionsAppendExecutionProvider_TensorRT(session_options, \u0026amp;trt_options); // CoreML执行（macOS/iOS） OrtSessionOptionsAppendExecutionProvider_CoreML(session_options, 0); // 创建会话（会按顺序尝试EP，失败则回退到下一个） OrtCreateSession(env, model_path, session_options, \u0026amp;session); Input/Output 模型的输入输出通过OrtValue传递，需要手动创建和管理：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 // 1. 获取输入输出信息 size_t num_input_nodes; OrtStatus* status = OrtSessionGetInputCount(session, \u0026amp;num_input_nodes); const char* input_name; OrtTypeInfo* input_type_info; OrtSessionGetInputName(session, 0, allocator, \u0026amp;input_name); OrtSessionGetInputTypeInfo(session, 0, \u0026amp;input_type_info); // 2. 准备输入数据 float input_data[] = { /* audio_features数据 */ }; int64_t input_shape[] = {1, 480}; // batch_size, feature_dim size_t input_tensor_size = 480; OrtValue* input_tensor = NULL; OrtMemoryInfo* memory_info; OrtCreateCpuMemoryInfo(OrtArenaAllocator, OrtMemTypeDefault, \u0026amp;memory_info); OrtCreateTensorWithDataAsOrtValue( memory_info, input_data, input_tensor_size * sizeof(float), input_shape, 2, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT, \u0026amp;input_tensor ); // 3. 执行推理 const char* input_names[] = {input_name}; const char* output_names[] = {\u0026#34;output\u0026#34;}; // 根据模型实际输出名称 OrtValue* output_tensor = NULL; status = OrtRun(session, NULL, input_names, \u0026amp;input_tensor, 1, output_names, 1, \u0026amp;output_tensor); // 4. 获取输出数据 float* output_data; OrtGetTensorMutableData(output_tensor, (void**)\u0026amp;output_data); // 使用output_data... // 5. 释放资源 OrtReleaseValue(output_tensor); OrtReleaseValue(input_tensor); OrtReleaseMemoryInfo(memory_info); 2.2 性能优化选项 ORT提供了多种性能优化选项，在C API中通过OrtSessionOptions进行配置：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 OrtSessionOptions* session_options = NULL; OrtCreateSessionOptions(\u0026amp;session_options); // 图优化级别 // ORT_DISABLE_ALL, ORT_ENABLE_BASIC, ORT_ENABLE_EXTENDED, ORT_ENABLE_ALL OrtSetSessionGraphOptimizationLevel(session_options, ORT_ENABLE_ALL); // 线程数设置 OrtSetIntraOpNumThreads(session_options, 4); // 算子内部并行线程数 OrtSetInterOpNumThreads(session_options, 2); // 算子间并行线程数 // 内存模式 OrtEnableMemPattern(session_options); // 启用内存模式优化 OrtEnableCpuMemArena(session_options); // 启用CPU内存池 // 执行模式 OrtSetSessionExecutionMode(session_options, ORT_SEQUENTIAL); // 顺序执行 // OrtSetSessionExecutionMode(session_options, ORT_PARALLEL); // 并行执行 // 优化配置文件（可选，用于更精细的控制） // OrtSetOptimizedModelFilePath(session_options, \u0026#34;optimized_model.onnx\u0026#34;); // 创建会话时应用这些选项 OrtCreateSession(env, model_path, session_options, \u0026amp;session); // 使用完毕后释放 OrtReleaseSessionOptions(session_options); 三、语音降噪推理的特殊注意事项 语音降噪模型通常使用时序建模网络（如GRU、LSTM），这些网络具有隐状态（hidden state），在实时推理时需要特别注意状态管理。\n3.1 为什么ORT不保存隐状态？ ORT（ONNX Runtime）采用**无状态（stateless）**的设计理念，即每次推理调用都是独立的，ORT不会在内部保存任何状态信息。这种设计有以下几个重要原因：\n3.1.1 设计理念：无状态推理 ORT的核心设计原则是每次OrtRun调用都是完全独立的，不依赖之前的调用结果。这种设计带来以下优势：\n线程安全：多个线程可以同时使用同一个OrtSession进行推理，而不会因为共享状态导致竞争条件 可重现性：相同的输入总是产生相同的输出，不受历史状态影响 灵活性：可以灵活控制何时重置状态、何时复用状态，适应不同的应用场景 3.1.2 状态管理的责任归属 在ORT的设计中，状态管理是应用层的责任，而不是推理引擎的责任。这样做的好处是：\n应用层控制：应用可以根据业务需求决定何时重置状态、如何管理多个流的状态 内存管理：应用可以精确控制状态的内存分配和释放时机 多实例支持：同一个模型可以同时处理多个独立的音频流，每个流维护自己的状态 3.1.3 与训练框架的差异 在训练框架（如PyTorch、TensorFlow）中，RNN/LSTM层通常会维护隐状态：\n1 2 3 # PyTorch训练时的行为 lstm = nn.LSTM(input_size, hidden_size) output, (hidden, cell) = lstm(input, (hidden, cell)) # 状态在层内部管理 但在ONNX导出和ORT推理时，隐状态被显式化为模型的输入和输出：\n1 2 3 // ONNX模型结构 // 输入: [audio_features, hidden_state, cell_state] // 显式输入 // 输出: [denoised_features, new_hidden_state, new_cell_state] // 显式输出 这种显式化的设计使得：\n状态在模型外部可见和可控 可以跨框架、跨平台保持一致的行为 便于调试和优化 3.1.4 实际影响 对于语音降噪等时序应用，ORT不保存隐状态意味着：\n必须手动传递状态：每次推理时，需要将上一次的输出状态作为下一次的输入 状态持久化由应用负责：如果需要保存状态（如断点续传），需要应用层实现 多流处理需要独立状态：处理多个音频流时，需要为每个流维护独立的状态变量 这种设计虽然增加了应用层的复杂度，但提供了更大的灵活性和控制力，特别适合生产环境中的复杂场景。\n3.2 实战使用ORT进行Rnnoise降噪推理 RNNoise是一个基于深度学习的实时语音降噪模型，使用了三个GRU层（VAD GRU、Noise GRU、Denoise GRU）进行时序建模。在使用ORT进行推理时，需要特别注意这三个GRU层的隐状态管理。\n3.2.1 转换成ONNX模型时导出GRU隐状态输入输出端口 RNNoise的Keras训练模型通常只接受特征输入，GRU的隐状态在内部管理。但在导出ONNX模型用于ORT推理时，需要将隐状态显式化为模型的输入和输出端口，这样才能在应用层控制状态传递。\n关键步骤：\n重建模型结构：创建一个新的推理模型，为每个GRU层添加initial_state输入和return_state=True输出 复制权重：从训练模型复制所有层的权重到新模型 定义输入输出：新模型有4个输入（features + 3个GRU状态）和5个输出（denoise_output + vad_output + 3个GRU状态） 下图中，左侧为没有导出隐状态的onnx模型可视化图，可以看到gru的隐状态每次都是被重置的；右侧为导出了隐状态的onnx模型可视化图，可以看到gru节点对应了一个gru state输入端口和一个gru state的输出端口。\n以下是完整的转换代码：\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 import keras.backend as K from keras.constraints import Constraint from keras.layers import Input, Dense, GRU, concatenate from keras.models import Model def my_crossentropy(y_true, y_pred): return K.mean(2 * K.abs(y_true - 0.5) * K.binary_crossentropy(y_pred, y_true), axis=-1) def mymask(y_true): return K.minimum(y_true + 1.0, 1.0) def msse(y_true, y_pred): return K.mean(mymask(y_true) * K.square(K.sqrt(y_pred) - K.sqrt(y_true)), axis=-1) def mycost(y_true, y_pred): return K.mean( mymask(y_true) * ( 10 * K.square(K.square(K.sqrt(y_pred) - K.sqrt(y_true))) + K.square(K.sqrt(y_pred) - K.sqrt(y_true)) + 0.01 * K.binary_crossentropy(y_pred, y_true) ), axis=-1, ) def my_accuracy(y_true, y_pred): return K.mean(2 * K.abs(y_true - 0.5) * K.equal(y_true, K.round(y_pred)), axis=-1) class WeightClip(Constraint): # Accept **kwargs to be compatible with Keras deserialization that may pass \u0026#39;name\u0026#39; etc. def __init__(self, c=2, **kwargs): # kwargs may include \u0026#39;name\u0026#39; super().__init__() self.c = c def __call__(self, p): return K.clip(p, -self.c, self.c) def get_config(self): return {\u0026#39;name\u0026#39;: self.__class__.__name__, \u0026#39;c\u0026#39;: self.c} CUSTOM_OBJECTS = { \u0026#39;my_crossentropy\u0026#39;: my_crossentropy, \u0026#39;mymask\u0026#39;: mymask, \u0026#39;msse\u0026#39;: msse, \u0026#39;mycost\u0026#39;: mycost, \u0026#39;my_accuracy\u0026#39;: my_accuracy, \u0026#39;WeightClip\u0026#39;: WeightClip, } def rebuild_model_with_states(training_model: Model) -\u0026gt; Model: \u0026#34;\u0026#34;\u0026#34; 自动重建模型，添加GRU隐状态输入/输出端口。 如果模型已经有GRU状态端口，直接返回原模型。 \u0026#34;\u0026#34;\u0026#34; # 检查是否已有GRU状态端口 if len(training_model.inputs) == 4 and len(training_model.outputs) == 5: print(\u0026#34; Model already has GRU state ports, skipping rebuild\u0026#34;) return training_model print(\u0026#34; Rebuilding model with GRU state inputs/outputs...\u0026#34;) # 新的推理输入（带状态） features_in = Input(shape=(None, 42), name=\u0026#39;features\u0026#39;) vad_state_in = Input(shape=(24,), name=\u0026#39;vad_gru_state\u0026#39;) noise_state_in = Input(shape=(48,), name=\u0026#39;noise_gru_state\u0026#39;) denoise_state_in = Input(shape=(96,), name=\u0026#39;denoise_gru_state\u0026#39;) # 复制训练模型的层配置并加载权重 # 1) input_dense input_dense_src = training_model.get_layer(\u0026#39;input_dense\u0026#39;) input_dense = Dense(24, activation=\u0026#39;tanh\u0026#39;, name=\u0026#39;input_dense_export\u0026#39;, kernel_constraint=input_dense_src.kernel_constraint, bias_constraint=input_dense_src.bias_constraint) tmp_export = input_dense(features_in) input_dense.set_weights(input_dense_src.get_weights()) # 2) vad_gru (return_sequences+return_state) vad_gru_src = training_model.get_layer(\u0026#39;vad_gru\u0026#39;) vad_gru_exp = GRU(24, activation=\u0026#39;tanh\u0026#39;, recurrent_activation=\u0026#39;sigmoid\u0026#39;, return_sequences=True, return_state=True, name=\u0026#39;vad_gru_export\u0026#39;, kernel_regularizer=vad_gru_src.kernel_regularizer, recurrent_regularizer=vad_gru_src.recurrent_regularizer, kernel_constraint=vad_gru_src.kernel_constraint, recurrent_constraint=vad_gru_src.recurrent_constraint, bias_constraint=vad_gru_src.bias_constraint) vad_seq, vad_state_out = vad_gru_exp(tmp_export, initial_state=vad_state_in) vad_gru_exp.set_weights(vad_gru_src.get_weights()) # 3) vad_output vad_output_src = training_model.get_layer(\u0026#39;vad_output\u0026#39;) vad_output_exp_layer = Dense(1, activation=\u0026#39;sigmoid\u0026#39;, name=\u0026#39;vad_output_export\u0026#39;, kernel_constraint=vad_output_src.kernel_constraint, bias_constraint=vad_output_src.bias_constraint) vad_output_exp = vad_output_exp_layer(vad_seq) vad_output_exp_layer.set_weights(vad_output_src.get_weights()) # 4) noise_gru 输入：concat([tmp_export, vad_seq, features_in]) noise_in = concatenate([tmp_export, vad_seq, features_in], name=\u0026#39;noise_concat_export\u0026#39;) noise_gru_src = training_model.get_layer(\u0026#39;noise_gru\u0026#39;) noise_gru_exp = GRU(48, activation=\u0026#39;relu\u0026#39;, recurrent_activation=\u0026#39;sigmoid\u0026#39;, return_sequences=True, return_state=True, name=\u0026#39;noise_gru_export\u0026#39;, kernel_regularizer=noise_gru_src.kernel_regularizer, recurrent_regularizer=noise_gru_src.recurrent_regularizer, kernel_constraint=noise_gru_src.kernel_constraint, recurrent_constraint=noise_gru_src.recurrent_constraint, bias_constraint=noise_gru_src.bias_constraint) noise_seq, noise_state_out = noise_gru_exp(noise_in, initial_state=noise_state_in) noise_gru_exp.set_weights(noise_gru_src.get_weights()) # 5) denoise_gru 输入：concat([vad_seq, noise_seq, features_in]) denoise_in = concatenate([vad_seq, noise_seq, features_in], name=\u0026#39;denoise_concat_export\u0026#39;) denoise_gru_src = training_model.get_layer(\u0026#39;denoise_gru\u0026#39;) denoise_gru_exp = GRU(96, activation=\u0026#39;tanh\u0026#39;, recurrent_activation=\u0026#39;sigmoid\u0026#39;, return_sequences=True, return_state=True, name=\u0026#39;denoise_gru_export\u0026#39;, kernel_regularizer=denoise_gru_src.kernel_regularizer, recurrent_regularizer=denoise_gru_src.recurrent_regularizer, kernel_constraint=denoise_gru_src.kernel_constraint, recurrent_constraint=denoise_gru_src.recurrent_constraint, bias_constraint=denoise_gru_src.bias_constraint) denoise_seq, denoise_state_out = denoise_gru_exp(denoise_in, initial_state=denoise_state_in) denoise_gru_exp.set_weights(denoise_gru_src.get_weights()) # 6) denoise_output denoise_output_src = training_model.get_layer(\u0026#39;denoise_output\u0026#39;) denoise_output_exp_layer = Dense(22, activation=\u0026#39;sigmoid\u0026#39;, name=\u0026#39;denoise_output_export\u0026#39;, kernel_constraint=denoise_output_src.kernel_constraint, bias_constraint=denoise_output_src.bias_constraint) denoise_output_exp = denoise_output_exp_layer(denoise_seq) denoise_output_exp_layer.set_weights(denoise_output_src.get_weights()) export_model = Model( inputs=[features_in, vad_state_in, noise_state_in, denoise_state_in], outputs=[denoise_output_exp, vad_output_exp, vad_state_out, noise_state_out, denoise_state_out], name=\u0026#39;rnnoise_export_with_states\u0026#39; ) print(\u0026#34; ✓ Model rebuilt successfully with GRU state ports\u0026#34;) return export_model def convert(hdf5_path: str, onnx_path: str, opset: int = 13, auto_rebuild: bool = False) -\u0026gt; None: if not os.path.isfile(hdf5_path): raise FileNotFoundError(f\u0026#34;HDF5 model not found: {hdf5_path}\u0026#34;) print(f\u0026#34;Loading Keras model from: {hdf5_path}\u0026#34;) # Load with custom objects registered for deserialization model = keras.models.load_model(hdf5_path, custom_objects=CUSTOM_OBJECTS) # Auto-rebuild model with GRU states if needed if auto_rebuild: print(\u0026#34;\\n=== Auto-Rebuild Mode ===\u0026#34;) print(\u0026#34; Checking if model needs GRU state ports...\u0026#34;) model = rebuild_model_with_states(model) print(\u0026#34; Model ready for conversion with GRU state ports\\n\u0026#34;) # Check if the model has GRU state inputs/outputs num_inputs = len(model.inputs) num_outputs = len(model.outputs) print(f\u0026#34;Model has {num_inputs} input(s) and {num_outputs} output(s)\u0026#34;) # Print input information for i, inp in enumerate(model.inputs): print(f\u0026#34; Input {i}: {inp.name}, shape: {inp.shape}\u0026#34;) # Print output information for i, out in enumerate(model.outputs): print(f\u0026#34; Output {i}: {out.name}, shape: {out.shape}\u0026#34;) # Check if this is a model with GRU states (4 inputs and 5 outputs) if num_inputs == 4 and num_outputs == 5: print(\u0026#34;Detected model with GRU state inputs/outputs\u0026#34;) # Build input signature for model with efficient state management input_specs = [] for inp in model.inputs: inp_name = inp.name.split(\u0026#39;:\u0026#39;)[0] inp_shape = inp.shape.as_list() # Handle different input shapes if len(inp_shape) == 3: # features: (None, None, 42) spec = tf.TensorSpec([None, None, inp_shape[2]], tf.float32, name=inp_name) elif len(inp_shape) == 2: # GRU states: (None, hidden_size) spec = tf.TensorSpec([None, inp_shape[1]], tf.float32, name=inp_name) else: # Fallback: use dynamic shape spec = tf.TensorSpec([None] * len(inp_shape), tf.float32, name=inp_name) input_specs.append(spec) print(f\u0026#34;Converting to ONNX (opset {opset}) with GRU state inputs/outputs...\u0026#34;) # Convert with all input signatures tf2onnx.convert.from_keras(model, input_signature=input_specs, output_path=onnx_path, opset=opset) elif num_inputs == 1: print(\u0026#34;Detected standard model without GRU state ports\u0026#34;) # Use a dynamic input signature (None, None, 42) to preserve time dimension flexibility input_name = model.inputs[0].name.split(\u0026#39;:\u0026#39;)[0] spec = (tf.TensorSpec([None, None, 42], tf.float32, name=input_name),) print(f\u0026#34;Converting to ONNX (opset {opset})...\u0026#34;) # Convert directly from the Keras model tf2onnx.convert.from_keras(model, input_signature=spec, output_path=onnx_path, opset=opset) else: # Generic conversion for models with multiple inputs but unknown structure print(f\u0026#34;Converting to ONNX (opset {opset}) with {num_inputs} inputs...\u0026#34;) input_specs = [] for inp in model.inputs: inp_name = inp.name.split(\u0026#39;:\u0026#39;)[0] inp_shape = inp.shape.as_list() # Use dynamic shapes for flexibility spec = tf.TensorSpec([None] * len(inp_shape), tf.float32, name=inp_name) input_specs.append(spec) tf2onnx.convert.from_keras(model, input_signature=input_specs, output_path=onnx_path, opset=opset) print(f\u0026#34;Saved ONNX model to: {onnx_path}\u0026#34;) def main(): parser = argparse.ArgumentParser(description=\u0026#39;Convert Keras HDF5 model to ONNX for RNNoise.\u0026#39;) parser.add_argument(\u0026#39;--input\u0026#39;, \u0026#39;-i\u0026#39;, required=True, help=\u0026#39;Path to Keras HDF5 model file\u0026#39;) parser.add_argument(\u0026#39;--output\u0026#39;, \u0026#39;-o\u0026#39;, required=False, help=\u0026#39;Path to output ONNX file\u0026#39;) parser.add_argument(\u0026#39;--opset\u0026#39;, type=int, default=13, help=\u0026#39;ONNX opset version (default: 13)\u0026#39;) parser.add_argument(\u0026#39;--auto-rebuild\u0026#39;, action=\u0026#39;store_true\u0026#39;, help=\u0026#39;Automatically rebuild model with GRU state ports if missing\u0026#39;) args = parser.parse_args() input_path = os.path.abspath(args.input) output_path = args.output if not output_path: base, _ = os.path.splitext(input_path) output_path = base + \u0026#39;.onnx\u0026#39; output_path = os.path.abspath(output_path) os.makedirs(os.path.dirname(output_path), exist_ok=True) convert(input_path, output_path, opset=args.opset, auto_rebuild=args.auto_rebuild) if __name__ == \u0026#39;__main__\u0026#39;: main() 3.2.2 推理时对隐状态进行管理 前面导出onnx模型时，已经为每个GRU节点导出了隐状态的输入和输出端口，因此在每一次帧的时候，只需要将上一次推理保存的隐状态信息输入到对应的隐状态输入端口，同时在推理后对GRU节点的隐状态输出端口进行保存，就可以实现流式推理GRU保留历史信息了。\n以下是部分核心函数实现，只需要将其嵌入到原rnnoise降噪代码中就可以实现ort推理了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 // Initialize ONNX model int initialize_onnx_model(RNNoiseContext* ctx, const char* model_path) { // Get ONNX Runtime API const OrtApiBase* api_base = OrtGetApiBase(); if (!api_base) { fprintf(stderr, \u0026#34;Error getting ONNX Runtime API base\\n\u0026#34;); return -1; } ctx-\u0026gt;api = api_base-\u0026gt;GetApi(ORT_API_VERSION); if (!ctx-\u0026gt;api) { fprintf(stderr, \u0026#34;Error getting ONNX Runtime API\\n\u0026#34;); return -1; } // Initialize ONNX Runtime environment OrtStatus* status = ctx-\u0026gt;api-\u0026gt;CreateEnv(ORT_LOGGING_LEVEL_WARNING, \u0026#34;RNNoiseONNX\u0026#34;, \u0026amp;ctx-\u0026gt;env); if (status != NULL) { fprintf(stderr, \u0026#34;Error creating ONNX Runtime environment\\n\u0026#34;); return -1; } // Create session options status = ctx-\u0026gt;api-\u0026gt;CreateSessionOptions(\u0026amp;ctx-\u0026gt;session_options); if (status != NULL) { fprintf(stderr, \u0026#34;Error creating session options\\n\u0026#34;); return -1; } // Set session options status = ctx-\u0026gt;api-\u0026gt;SetIntraOpNumThreads(ctx-\u0026gt;session_options, 1); if (status != NULL) { fprintf(stderr, \u0026#34;Error setting intra-op threads\\n\u0026#34;); return -1; } status = ctx-\u0026gt;api-\u0026gt;SetSessionGraphOptimizationLevel(ctx-\u0026gt;session_options, ORT_ENABLE_EXTENDED); if (status != NULL) { fprintf(stderr, \u0026#34;Error setting optimization level\\n\u0026#34;); return -1; } // Create session status = ctx-\u0026gt;api-\u0026gt;CreateSession(ctx-\u0026gt;env, model_path, ctx-\u0026gt;session_options, \u0026amp;ctx-\u0026gt;session); if (status != NULL) { fprintf(stderr, \u0026#34;Error creating ONNX session\\n\u0026#34;); return -1; } // Get allocator status = ctx-\u0026gt;api-\u0026gt;GetAllocatorWithDefaultOptions(\u0026amp;ctx-\u0026gt;allocator); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting allocator\\n\u0026#34;); return -1; } // Get input/output names size_t num_input_nodes, num_output_nodes; status = ctx-\u0026gt;api-\u0026gt;SessionGetInputCount(ctx-\u0026gt;session, \u0026amp;num_input_nodes); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting input count\\n\u0026#34;); return -1; } status = ctx-\u0026gt;api-\u0026gt;SessionGetOutputCount(ctx-\u0026gt;session, \u0026amp;num_output_nodes); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting output count\\n\u0026#34;); return -1; } printf(\u0026#34;ONNX Model Info:\\n\u0026#34;); printf(\u0026#34; Input nodes: %zu\\n\u0026#34;, num_input_nodes); printf(\u0026#34; Output nodes: %zu\\n\u0026#34;, num_output_nodes); // Detect model type: 4 inputs + 5 outputs = model with GRU states ctx-\u0026gt;has_gru_states = (num_input_nodes == 4 \u0026amp;\u0026amp; num_output_nodes == 5); if (ctx-\u0026gt;has_gru_states) { printf(\u0026#34; Model type: WITH GRU state inputs/outputs\\n\u0026#34;); // Get all input names status = ctx-\u0026gt;api-\u0026gt;SessionGetInputName(ctx-\u0026gt;session, 0, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;input_name); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting features input name\\n\u0026#34;); return -1; } status = ctx-\u0026gt;api-\u0026gt;SessionGetInputName(ctx-\u0026gt;session, 1, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;input_name_vad_state); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting VAD state input name\\n\u0026#34;); return -1; } status = ctx-\u0026gt;api-\u0026gt;SessionGetInputName(ctx-\u0026gt;session, 2, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;input_name_noise_state); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting noise state input name\\n\u0026#34;); return -1; } status = ctx-\u0026gt;api-\u0026gt;SessionGetInputName(ctx-\u0026gt;session, 3, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;input_name_denoise_state); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting denoise state input name\\n\u0026#34;); return -1; } // Get all output names status = ctx-\u0026gt;api-\u0026gt;SessionGetOutputName(ctx-\u0026gt;session, 0, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;output_name_denoise); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting denoise output name\\n\u0026#34;); return -1; } status = ctx-\u0026gt;api-\u0026gt;SessionGetOutputName(ctx-\u0026gt;session, 1, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;output_name_vad); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting VAD output name\\n\u0026#34;); return -1; } status = ctx-\u0026gt;api-\u0026gt;SessionGetOutputName(ctx-\u0026gt;session, 2, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;output_name_vad_state); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting VAD state output name\\n\u0026#34;); return -1; } status = ctx-\u0026gt;api-\u0026gt;SessionGetOutputName(ctx-\u0026gt;session, 3, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;output_name_noise_state); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting noise state output name\\n\u0026#34;); return -1; } status = ctx-\u0026gt;api-\u0026gt;SessionGetOutputName(ctx-\u0026gt;session, 4, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;output_name_denoise_state); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting denoise state output name\\n\u0026#34;); return -1; } printf(\u0026#34; Inputs:\\n\u0026#34;); printf(\u0026#34; [0] %s (features)\\n\u0026#34;, ctx-\u0026gt;input_name); printf(\u0026#34; [1] %s (VAD GRU state)\\n\u0026#34;, ctx-\u0026gt;input_name_vad_state); printf(\u0026#34; [2] %s (noise GRU state)\\n\u0026#34;, ctx-\u0026gt;input_name_noise_state); printf(\u0026#34; [3] %s (denoise GRU state)\\n\u0026#34;, ctx-\u0026gt;input_name_denoise_state); printf(\u0026#34; Outputs:\\n\u0026#34;); printf(\u0026#34; [0] %s (denoise)\\n\u0026#34;, ctx-\u0026gt;output_name_denoise); printf(\u0026#34; [1] %s (VAD)\\n\u0026#34;, ctx-\u0026gt;output_name_vad); printf(\u0026#34; [2] %s (VAD GRU state)\\n\u0026#34;, ctx-\u0026gt;output_name_vad_state); printf(\u0026#34; [3] %s (noise GRU state)\\n\u0026#34;, ctx-\u0026gt;output_name_noise_state); printf(\u0026#34; [4] %s (denoise GRU state)\\n\u0026#34;, ctx-\u0026gt;output_name_denoise_state); } else { printf(\u0026#34; Model type: Standard (without GRU state ports)\\n\u0026#34;); // Get input name (standard model) status = ctx-\u0026gt;api-\u0026gt;SessionGetInputName(ctx-\u0026gt;session, 0, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;input_name); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting input name\\n\u0026#34;); return -1; } // Get output names (standard model) status = ctx-\u0026gt;api-\u0026gt;SessionGetOutputName(ctx-\u0026gt;session, 0, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;output_name_denoise); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting denoise output name\\n\u0026#34;); return -1; } status = ctx-\u0026gt;api-\u0026gt;SessionGetOutputName(ctx-\u0026gt;session, 1, ctx-\u0026gt;allocator, \u0026amp;ctx-\u0026gt;output_name_vad); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting VAD output name\\n\u0026#34;); return -1; } printf(\u0026#34; Input: %s\\n\u0026#34;, ctx-\u0026gt;input_name); printf(\u0026#34; Output denoise: %s\\n\u0026#34;, ctx-\u0026gt;output_name_denoise); printf(\u0026#34; Output VAD: %s\\n\u0026#34;, ctx-\u0026gt;output_name_vad); } // Allocate buffers ctx-\u0026gt;input_buffer = (float*)malloc(FRAME_SIZE * sizeof(float)); ctx-\u0026gt;output_buffer = (float*)malloc(FRAME_SIZE * sizeof(float)); if (!ctx-\u0026gt;input_buffer || !ctx-\u0026gt;output_buffer) { fprintf(stderr, \u0026#34;Error: Memory allocation failed\\n\u0026#34;); return -1; } // Initialize RNNoise state for feature extraction ctx-\u0026gt;denoise_state = rnnoise_create(NULL); if (!ctx-\u0026gt;denoise_state) { fprintf(stderr, \u0026#34;Error: Failed to create RNNoise state\\n\u0026#34;); return -1; } rnnoise_init(ctx-\u0026gt;denoise_state, NULL); // Initialize biquad filter memory ctx-\u0026gt;mem_hp_x[0] = 0.0f; ctx-\u0026gt;mem_hp_x[1] = 0.0f; // Initialize processing buffers memset(ctx-\u0026gt;X, 0, sizeof(ctx-\u0026gt;X)); memset(ctx-\u0026gt;P, 0, sizeof(ctx-\u0026gt;P)); memset(ctx-\u0026gt;Ex, 0, sizeof(ctx-\u0026gt;Ex)); memset(ctx-\u0026gt;Ep, 0, sizeof(ctx-\u0026gt;Ep)); memset(ctx-\u0026gt;Exp, 0, sizeof(ctx-\u0026gt;Exp)); memset(ctx-\u0026gt;lastg, 0, sizeof(ctx-\u0026gt;lastg)); memset(ctx-\u0026gt;synthesis_mem, 0, sizeof(ctx-\u0026gt;synthesis_mem)); // Initialize frame count ctx-\u0026gt;frame_count = 0; // Initialize GRU states if model supports it initialize_gru_states(ctx); printf(\u0026#34;ONNX model loaded successfully: %s\\n\u0026#34;, model_path); return 0; } // Initialize GRU states void initialize_gru_states(RNNoiseContext* ctx) { memset(ctx-\u0026gt;vad_gru_state, 0, sizeof(ctx-\u0026gt;vad_gru_state)); memset(ctx-\u0026gt;noise_gru_state, 0, sizeof(ctx-\u0026gt;noise_gru_state)); memset(ctx-\u0026gt;denoise_gru_state, 0, sizeof(ctx-\u0026gt;denoise_gru_state)); ctx-\u0026gt;gru_states_initialized = 0; } // ONNX inference with external state management int onnx_inference_with_states(RNNoiseContext* ctx, const float* features, float* gains, float* vad) { // Prepare separate input tensors for features and GRU states float features_data[42]; float vad_state_data[24]; float noise_state_data[48]; float denoise_state_data[96]; // Copy features memcpy(features_data, features, 42 * sizeof(float)); // Copy GRU states (use saved states for next frame) memcpy(vad_state_data, ctx-\u0026gt;vad_gru_state, 24 * sizeof(float)); memcpy(noise_state_data, ctx-\u0026gt;noise_gru_state, 48 * sizeof(float)); memcpy(denoise_state_data, ctx-\u0026gt;denoise_gru_state, 96 * sizeof(float)); // Create input tensors const int64_t features_shape[] = {1, 1, 42}; const int64_t vad_state_shape[] = {1, 24}; const int64_t noise_state_shape[] = {1, 48}; const int64_t denoise_state_shape[] = {1, 96}; OrtMemoryInfo* memory_info; OrtStatus* status = ctx-\u0026gt;api-\u0026gt;CreateCpuMemoryInfo(OrtArenaAllocator, OrtMemTypeDefault, \u0026amp;memory_info); if (status != NULL) { fprintf(stderr, \u0026#34;Error creating memory info\\n\u0026#34;); return -1; } // Create input tensors OrtValue* features_tensor = NULL; OrtValue* vad_state_tensor = NULL; OrtValue* noise_state_tensor = NULL; OrtValue* denoise_state_tensor = NULL; status = ctx-\u0026gt;api-\u0026gt;CreateTensorWithDataAsOrtValue( memory_info, features_data, 42 * sizeof(float), features_shape, 3, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT, \u0026amp;features_tensor); if (status != NULL) { fprintf(stderr, \u0026#34;Error creating features tensor\\n\u0026#34;); ctx-\u0026gt;api-\u0026gt;ReleaseMemoryInfo(memory_info); return -1; } status = ctx-\u0026gt;api-\u0026gt;CreateTensorWithDataAsOrtValue( memory_info, vad_state_data, 24 * sizeof(float), vad_state_shape, 2, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT, \u0026amp;vad_state_tensor); if (status != NULL) { fprintf(stderr, \u0026#34;Error creating VAD state tensor\\n\u0026#34;); ctx-\u0026gt;api-\u0026gt;ReleaseValue(features_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseMemoryInfo(memory_info); return -1; } status = ctx-\u0026gt;api-\u0026gt;CreateTensorWithDataAsOrtValue( memory_info, noise_state_data, 48 * sizeof(float), noise_state_shape, 2, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT, \u0026amp;noise_state_tensor); if (status != NULL) { fprintf(stderr, \u0026#34;Error creating noise state tensor\\n\u0026#34;); ctx-\u0026gt;api-\u0026gt;ReleaseValue(features_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseValue(vad_state_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseMemoryInfo(memory_info); return -1; } status = ctx-\u0026gt;api-\u0026gt;CreateTensorWithDataAsOrtValue( memory_info, denoise_state_data, 96 * sizeof(float), denoise_state_shape, 2, ONNX_TENSOR_ELEMENT_DATA_TYPE_FLOAT, \u0026amp;denoise_state_tensor); if (status != NULL) { fprintf(stderr, \u0026#34;Error creating denoise state tensor\\n\u0026#34;); ctx-\u0026gt;api-\u0026gt;ReleaseValue(features_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseValue(vad_state_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseValue(noise_state_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseMemoryInfo(memory_info); return -1; } // Prepare input names and tensors const char* input_names[] = {ctx-\u0026gt;input_name, ctx-\u0026gt;input_name_vad_state, ctx-\u0026gt;input_name_noise_state, ctx-\u0026gt;input_name_denoise_state}; OrtValue* input_tensors[] = {features_tensor, vad_state_tensor, noise_state_tensor, denoise_state_tensor}; // Prepare output names const char* output_names[] = {ctx-\u0026gt;output_name_denoise, ctx-\u0026gt;output_name_vad, ctx-\u0026gt;output_name_vad_state, ctx-\u0026gt;output_name_noise_state, ctx-\u0026gt;output_name_denoise_state}; OrtValue* output_tensors[5] = {NULL, NULL, NULL, NULL, NULL}; // Run inference status = ctx-\u0026gt;api-\u0026gt;Run(ctx-\u0026gt;session, NULL, input_names, (const OrtValue* const*)input_tensors, 4, output_names, 5, output_tensors); if (status != NULL) { fprintf(stderr, \u0026#34;Error running inference\\n\u0026#34;); ctx-\u0026gt;api-\u0026gt;ReleaseValue(features_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseValue(vad_state_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseValue(noise_state_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseValue(denoise_state_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseMemoryInfo(memory_info); return -1; } // Get output data float* denoise_output = NULL; float* vad_output = NULL; float* updated_vad_state = NULL; float* updated_noise_state = NULL; float* updated_denoise_state = NULL; status = ctx-\u0026gt;api-\u0026gt;GetTensorMutableData(output_tensors[0], (void**)\u0026amp;denoise_output); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting denoise output data\\n\u0026#34;); goto cleanup; } status = ctx-\u0026gt;api-\u0026gt;GetTensorMutableData(output_tensors[1], (void**)\u0026amp;vad_output); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting VAD output data\\n\u0026#34;); goto cleanup; } status = ctx-\u0026gt;api-\u0026gt;GetTensorMutableData(output_tensors[2], (void**)\u0026amp;updated_vad_state); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting updated VAD state data\\n\u0026#34;); goto cleanup; } status = ctx-\u0026gt;api-\u0026gt;GetTensorMutableData(output_tensors[3], (void**)\u0026amp;updated_noise_state); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting updated noise state data\\n\u0026#34;); goto cleanup; } status = ctx-\u0026gt;api-\u0026gt;GetTensorMutableData(output_tensors[4], (void**)\u0026amp;updated_denoise_state); if (status != NULL) { fprintf(stderr, \u0026#34;Error getting updated denoise state data\\n\u0026#34;); goto cleanup; } // Store results memcpy(gains, denoise_output, NB_BANDS * sizeof(float)); *vad = vad_output[0]; // Update GRU states with the outputs from the model (for next frame) memcpy(ctx-\u0026gt;vad_gru_state, updated_vad_state, 24 * sizeof(float)); memcpy(ctx-\u0026gt;noise_gru_state, updated_noise_state, 48 * sizeof(float)); memcpy(ctx-\u0026gt;denoise_gru_state, updated_denoise_state, 96 * sizeof(float)); ctx-\u0026gt;gru_states_initialized = 1; cleanup: // Cleanup ctx-\u0026gt;api-\u0026gt;ReleaseValue(features_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseValue(vad_state_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseValue(noise_state_tensor); ctx-\u0026gt;api-\u0026gt;ReleaseValue(denoise_state_tensor); for (int i = 0; i \u0026lt; 5; i++) { if (output_tensors[i]) { ctx-\u0026gt;api-\u0026gt;ReleaseValue(output_tensors[i]); } } ctx-\u0026gt;api-\u0026gt;ReleaseMemoryInfo(memory_info); return 0; } 3.3 推理性能对比 可以看到在不需要自己手搓各个算子的C实现，借助ORT就可以实现接近5倍的性能提升，这投入回报比可是不要太高了。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 === Overall Inference Time Statistics === Total frames processed: 2048 Frames with inference: 2047 ONNX Inference: Total time: 73.568 ms Average per frame: 0.036 ms C Inference: Total time: 349.820 ms Average per frame: 0.171 ms Comparison: ONNX / C ratio: 0.21x Speedup: 4.76x (ONNX faster) ========================================== 四、总结 ORT作为跨平台的推理引擎，在语音降噪模型部署中具有显著优势。正确使用ORT需要：\n理解基本概念：掌握InferenceSession、Execution Provider等核心概念 遵循推理流程：按照标准的加载、准备、执行、获取结果流程 管理隐状态：对于时序模型，必须正确管理隐状态的传递和更新 性能优化：根据场景选择合适的优化选项和执行提供者 对于实时语音降噪场景，隐状态管理是关键，需要仔细设计状态传递逻辑，确保模型能够正确利用历史信息。\n通过合理使用ORT，可以充分发挥深度学习语音降噪模型的性能，实现高效、稳定的实时推理。\n另外，ORT还有很多高级特性，大家可以自己摸索尝试下。\n","permalink":"https://lyapple2008.github.io/posts/202511/2025-11-03-%E4%BD%BF%E7%94%A8ort%E8%BF%9B%E8%A1%8C%E8%AF%AD%E9%9F%B3%E9%99%8D%E5%99%AA%E6%8E%A8%E7%90%86/","summary":"\u003cp\u003e在深度学习语音降噪模型的部署过程中，选择合适的推理引擎至关重要。ONNX Runtime（ORT）作为微软开源的跨平台推理引擎，在性能、兼容性和易用性方面表现出色，已成为许多生产环境的首选。本文将介绍为什么选择ORT，ORT的核心概念和使用流程，以及在使用ORT进行语音降噪推理时需要注意的关键事项，特别是针对时序模型（如GRU/LSTM）的隐状态管理。\u003c/p\u003e","title":"使用ORT进行语音降噪模型推理"},{"content":"语音增强算法评估指南 如今语音增强算法已成为智能设备、视频会议和助听器等应用的核心，它能从嘈杂环境中“拯救”清晰的语音信号，但如何判断一个算法的好坏？这就是评估的意义所在。今天，我们来聊聊语音增强算法的评估体系，通过一个国际挑战赛作为切入点，带你一步步了解关键指标和计算方法。无论你是初学者还是从业者，这篇文章都能帮你理清思路。\n前言：为什么需要评估语音增强算法？ 想象一下，你开发了一个语音增强模型，自认为它能完美去除背景噪音。但在实际应用中，用户反馈“声音听起来怪怪的”或“某些噪音下完全失效”。这就是为什么评估至关重要：它提供了一个客观、量化的标准，帮助开发者识别算法的优缺点、优化性能，并与其他方法进行公平比较。\n评估的作用主要体现在三个方面：\n指导开发：通过指标反馈，迭代模型设计，避免主观偏见。 基准比较：在竞赛或论文中，用统一标准衡量不同算法的进步。 实际部署：确保算法在真实场景（如移动端或低信噪比环境）下的鲁棒性和通用性。 没有评估，算法开发就像盲人摸象；有了评估，它就成了科学工程。接下来，我们以NeurIPS 2024竞赛轨道下的URGENT 2024挑战为例，深入探讨评估体系。这个挑战聚焦于构建通用语音增强模型，强调在不同噪声、采样率和麦克风配置下的表现。\nURGENT 2024挑战：评估体系的典范 URGENT 2024（Universality, Robustness, and Generalizability for EnhancemeNT）挑战旨在解决传统语音增强研究的痛点：许多算法只针对特定条件优化，缺乏跨场景泛化能力。挑战要求参赛者使用统一的公共数据集训练单一模型，处理各种失真（如噪声、混响），并支持不同输入格式（如单/多通道、不同采样率）。\n这个挑战的亮点在于其全面评估框架，包括非侵入式（无参考信号）和侵入式（需参考信号）指标，以及下游任务相关指标（如词错误率WER）。它还引入主观MOS（Mean Opinion Score）评分作为最终盲测环节的补充。挑战提供ESPnet工具包的基线模型，鼓励数据增强，但严格限制训练数据来源，确保公平性。\n通过这个挑战，我们可以看到评估不仅是“打分”，而是推动行业向真实场景迈进的基准。接下来，重点介绍挑战中使用的核心客观指标：PESQ、ESTOI、SDR、MCD、LSD、DNSMOS和NISQA。\n评估指标详解：每个指标都在测什么？ 语音增强评估指标大致分为侵入式（需要干净参考信号）和非侵入式（无需参考，模拟真实场景）。URGENT 2024挑战选用这些指标来全面考察算法的语音质量、可懂度和保真度。下面逐一解释：\nPESQ (Perceptual Evaluation of Speech Quality)：这是一个侵入式指标，通过比较增强后的语音与参考干净信号，评估感知质量。它关注失真和噪声对人类听觉的影响，得分范围通常为-0.5到4.5（越高越好）。在语音增强中，PESQ常用于客观测试算法的整体质量，尤其适合电话或VoIP场景。\nESTOI (Extended Short-Time Objective Intelligibility)：侵入式指标，专注于评估增强语音的可懂度。它分析短时段信号，预测听者在噪声下的理解能力，得分从0到1（越高表示更易懂）。这个指标特别适用于低信噪比环境，帮助算法优化对人类认知的友好度。\nSDR (Signal-to-Distortion Ratio)：侵入式指标，计算期望信号能量与失真（包括噪声和伪影）能量的比率，通常以dB为单位（越高越好）。它评估增强信号的整体保真度，在多通道或复杂噪声场景中非常实用。\nMCD (Mel-Cepstral Distortion)：侵入式指标，量化增强信号与参考信号在梅尔倒谱系数上的差异（越低越好）。它聚焦谱失真，提供对感知质量的洞察，常用于评估算法对语音频谱的保留能力。\nLSD (Log-Spectral Distance)：侵入式指标，测量增强和参考信号功率谱的对数差异（越低越好）。它评估谱准确性，帮助理解算法如何保留原始语音特征，适用于频域分析。\nDNSMOS (Deep Noise Suppression Mean Opinion Score)：非侵入式指标，无需参考信号，使用深度学习模型预测语音质量（模拟人类评分，范围1-5）。它基于人类评级训练，适用于真实场景评估，尤其当干净参考不可用时。\nNISQA (Non-Intrusive Speech Quality Assessment)：同样是非侵入式指标，使用机器学习预测感知质量，无需参考。它评估整体语音质量，在参考信号缺失的实际部署中大放异彩。\n这些指标组合使用，能从质量、可懂度和失真等多维度评估算法。URGENT挑战强调，非侵入式指标如DNSMOS和NISQA更贴近现实，因为真实环境中往往没有干净参考。\n评测数据集：从哪里获取，如何使用？ 要实际计算这些指标，需要可靠的数据集。URGENT 2024挑战通过其GitHub仓库提供官方评测数据集，托管在Hugging Face上，便于下载和使用。\n官方评测数据集：包括验证集、非盲测集和盲测集，地址：https://huggingface.co/datasets/urgent-challenge/urgent2024_official。这些数据集包含各种失真条件下的语音样本，适合测试算法的通用性。 MOS数据集：额外提供带人类标注MOS分数的语音质量评估数据集，地址：https://huggingface.co/datasets/urgent-challenge/urgent2024_mos。用于主观指标验证。 访问方式简单：在Hugging Face平台搜索并下载，或使用Python的datasets库加载。数据集设计覆盖不同噪声、混响和麦克风配置，确保评估的全面性。\n可以参考下面的代码将hugging face中的validataion数据集以wav形式保存在本地，方便后续不同算法进行处理后，对处理结果进行评估。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 from datasets import load_dataset import soundfile as sf import os # ===== 参数 ===== DATASET_NAME = \u0026#34;urgent-challenge/urgent2024_official\u0026#34; SPLIT = \u0026#34;validation\u0026#34; SAVE_DIR = \u0026#34;../data/validation\u0026#34; NUM_PROC = 8 # 并行进程数，可以改成你的 CPU 核心数 # ===== 目录准备 ===== clean_dir = os.path.join(SAVE_DIR, \u0026#34;clean\u0026#34;) noisy_dir = os.path.join(SAVE_DIR, \u0026#34;noisy\u0026#34;) os.makedirs(clean_dir, exist_ok=True) os.makedirs(noisy_dir, exist_ok=True) # ===== 加载数据 ===== dataset = load_dataset(DATASET_NAME, SPLIT) # ===== 保存函数 ===== def save_audio(example): uid = example[\u0026#34;id\u0026#34;] # noisy noisy = example[\u0026#34;noisy_audio\u0026#34;] noisy_samples, noisy_sr = noisy[\u0026#34;array\u0026#34;], noisy[\u0026#34;sampling_rate\u0026#34;] # clean clean = example[\u0026#34;clean_audio\u0026#34;] clean_samples, clean_sr = clean[\u0026#34;array\u0026#34;], clean[\u0026#34;sampling_rate\u0026#34;] # 确保采样率一致 assert noisy_sr == clean_sr, f\u0026#34;Sample rate mismatch: noisy={noisy_sr}, clean={clean_sr}\u0026#34; # 保存文件路径 noisy_path = os.path.join(noisy_dir, f\u0026#34;{uid}.wav\u0026#34;) clean_path = os.path.join(clean_dir, f\u0026#34;{uid}.wav\u0026#34;) # 写文件 sf.write(noisy_path, noisy_samples, noisy_sr) sf.write(clean_path, clean_samples, clean_sr) return {\u0026#34;noisy_path\u0026#34;: noisy_path, \u0026#34;clean_path\u0026#34;: clean_path} # ===== 多进程导出 ===== dataset.map( save_audio, num_proc=NUM_PROC, # 开启多进程 desc=\u0026#34;Exporting audio\u0026#34;, # tqdm 进度条描述 ) 指标计算实践：一步步上手 计算这些指标需要工具和脚本。URGENT挑战的GitHub仓库（https://github.com/urgent-challenge/urgent2024_challenge）提供了evaluation_metrics文件夹下的实用脚本，如calculate_intrusive_se_metrics.py（处理PESQ、ESTOI、SDR、MCD、LSD等侵入式指标，支持无限SDR值处理），calculate_nonintrusive_dnsmos.py和calculate_nonintrusive_nisqa.py分别计算DNSMOS和NISQA。\n计算步骤示例：\n参考evaluation_metrics/README.md\n数据准备对validation数据集中的noisy数据进行处理，得到enhanced的数据，按下面的目录结构进行组织 1 2 3 4 5 6 7 8 9 📁 /path/to/your/data/ ├── 📁 enhanced/ │ ├── 🔈 fileid_1.wav │ ├── 🔈 fileid_2.wav │ └── ... └── 📁 clean/ ├── 🔈 fileid_1.wav ├── 🔈 fileid_2.wav └── ... 生成scp文件 为clean数据和enhance数据生成对应的scp文件，在scp文件中包含两列信息，一列是一个唯一的文件id，一列是文件路径，如下 1 2 3 4 5 6 7 8 9 # enhanced.scp fileid_1 /path/to/your/data/enhanced/fileid_1.flac fileid_2 /path/to/your/data/enhanced/fileid_2.flac ... # reference.scp fileid_1 /path/to/your/data/clean/fileid_1.flac fileid_2 /path/to/your/data/clean/fileid_2.flac ... 运行脚本：例如，对于侵入式指标，运行calculate_intrusive_se_metrics.py输入增强文件和参考文件，输出PESQ等分数。非侵入式如DNSMOS可直接输入增强语音。 示例代码片段： 1 2 3 4 5 6 7 8 9 10 11 12 13 #!/bin/bash nj=8 # Number of parallel CPU jobs for speedup python=python3 output_prefix=metrics_score # PESQ, ESTOI, SDR, MCD, LSD ${python} calculate_intrusive_se_metrics.py \\ --ref_scp reference.scp \\ --inf_scp enhanced_webrtc.scp \\ --output_dir \u0026#34;${output_prefix}\u0026#34;/scoring_webrtc \\ --nj ${nj} \\ --chunksize 60 对于批量计算，仓库脚本支持文件夹输入。 在实践中，建议结合主观听测（如MOS）验证客观指标，避免“高分低体验”的情况。\n结语：评估驱动创新 语音增强算法评估不是终点，而是起点。通过URGENT 2024这样的挑战，我们看到评估体系在推动算法向通用、鲁棒方向演进。未来，随着更多非侵入式指标和多模态数据的融入，评估将更贴近真实世界。\n最后我们来看下之前介绍的WebRTC NS是什么水平吧，最终的指标如下所示。\n可以看到WebRTC NS的评估指标分数只比noisy的分数好一点点，可见还有很大的上升空间。传统算法或多或少都会基于一些假设，而这些假设不只小范围内是生效的，这造成了传统算法的局限性。近年深度学习基于数据驱动的方法，进一步突破这些局限性，极大提高了音频算法的效果上线。下期预告，让我们迈上深度学习时代吧。\n","permalink":"https://lyapple2008.github.io/posts/202508/2025-08-11-%E9%9F%B3%E9%A2%91%E7%AE%97%E6%B3%95%E8%AF%84%E4%BC%B0/","summary":"\u003ch1 id=\"语音增强算法评估指南\"\u003e语音增强算法评估指南\u003c/h1\u003e\n\u003cp\u003e如今语音增强算法已成为智能设备、视频会议和助听器等应用的核心，它能从嘈杂环境中“拯救”清晰的语音信号，但如何判断一个算法的好坏？这就是评估的意义所在。今天，我们来聊聊语音增强算法的评估体系，通过一个国际挑战赛作为切入点，带你一步步了解关键指标和计算方法。无论你是初学者还是从业者，这篇文章都能帮你理清思路。\u003c/p\u003e","title":"语音增强算法评估指南"},{"content":"WebRTC中的Wiener滤波降噪算法 引言 在实时语音通信系统中，背景噪声的抑制对于提升通话清晰度至关重要。Google 的 WebRTC 项目在其开源语音引擎中实现了高质量的语音降噪模块，其中 Wiener 滤波器作为核心组成部分，结合语音概率估计、多特征建模、噪声谱跟踪等模块构建了一个高度实用的增强框架。\n本文将从 Wiener 滤波理论入手，逐步解析 WebRTC 中该算法的工程实现，其中噪声估计模块和语音概率估计模块已经在之前介绍了，有需要的可以翻阅。\n一、Wiener滤波基础 Wiener 滤波器是一种最小均方误差（MMSE）估计器，在频域降噪中用于构造一个频点增益函数，以最大程度还原语音信号。\n其基本形式为：\n$$ G(f) = \\frac{\\xi(f)}{\\xi(f) + 1} $$\n其中： •\t$\\xi(f)$：先验信噪比（prior SNR） •\t$G(f)$：频率点 f 的 Wiener 增益\n此增益用于对输入频谱进行缩放，从而滤除噪声成分。\n二、WebRTC中的Wiener滤波结构 在 WebRTC 降噪模块中，Wiener 滤波不是孤立运行，而是集成在一套完整的噪声估计与语音概率建模框架中：\ngraph TD A[输入音频帧] --\u0026gt; B[STFT 分帧] B --\u0026gt; C[信号谱 \u0026#43; 噪声谱] C --\u0026gt; D[SNR估计（后验\u0026#43;先验）] C --\u0026gt; E[LRT / Flatness / Diff 特征提取] D \u0026amp; E --\u0026gt; F[Wiener 滤波增益计算] F --\u0026gt; G[基于语音概率微调增益] G --\u0026gt; H[频谱乘以增益后逆变换] H --\u0026gt; I[输出增强语音] 三、核心增益计算逻辑 后验SNR与先验SNR估计 •\t后验SNR：使用当前帧信号谱与噪声谱之比 •\t先验SNR： $$ \\xi_t(f) = \\alpha \\cdot \\frac{|S_{t-1}(f)|^2}{N_{t-1}(f)} + (1 - \\alpha) \\cdot \\max(\\gamma_t(f) - 1, 0) $$\n1 2 3 // Previous estimate based on previous frame with gain filter. float prev_tsa = spectrum_prev_process_[i] / (prev_noise_spectrum[i] + 0.0001f) * filter_[i]; spectrum_prev_process_[i] 为前一帧信号的幅度谱，filter_[i]为前帧的Wiener滤波器增益，两者相乘后就是估计的纯净语音信号，因此prev_tsa就是前一帧的TSA (time-smoothed a priori SNR)\n1 2 3 4 5 6 // Post SNR. if (signal_spectrum[i] \u0026gt; noise_spectrum[i]) { post_snr[i] = signal_spectrum[i] / (noise_spectrum[i] + 0.0001f) - 1.f; } else { post_snr[i] = 0.f; } 只有当信号幅度大于噪声幅度时，才认为有语音信号存在，避免负增益\n1 prior_snr[i] = 0.98f * prev_estimate + (1.f - 0.98f) * post_snr[i]; 0.98 是平滑因子，表示以历史先验估计为主，防止突变\n基本增益计算 1 2 3 4 5 // 参变维纳滤波器，参考《语音增强--理论与实践》,通过over_subtraction_factor来控制降噪力度 filter_[i] = snr_prior / (suppression_params_.over_subtraction_factor + snr_prior); filter_[i] = std::max(std::min(filter_[i], 1.f), suppression_params_.minimum_attenuating_gain); 四、增益微调：语音概率辅助因子 虽然 Wiener 滤波器理论上是最优的，但其在以下情况下仍存在问题： •\tSNR 估计不准确 •\t缺乏上下文判断 •\t语音尾音易被吞掉\n因此 WebRTC 引入了语音概率估计模块，通过以下方式微调 Wiener 增益：\nscale = prior_prob * scale1 + (1 - prior_prob) * scale2;\n其中： •\tscale1: 当前是语音 → 增强增益 •\tscale2: 当前是噪声 → 抑制增益 •\tprior_prob: 通过 LRT、Spectral Flatness、Spectral Diff 等特征综合估计\n五、高带处理 在WebRTC的噪声抑制算法中，高频带（8kHz以上）没有直接使用频域Wiener滤波处理，而是在时域统一增益处理，这里即是因为语音大部分是在低带部分，同时也是为了效率减少计算量。接下我们来分析WebRTC的噪声抑制算法是如何通过低带的信息计算高频带增益的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 // Computes the attenuating gain for the noise suppression of the upper bands. float ComputeUpperBandsGain( float minimum_attenuating_gain, rtc::ArrayView\u0026lt;const float, kFftSizeBy2Plus1\u0026gt; filter, rtc::ArrayView\u0026lt;const float\u0026gt; speech_probability, rtc::ArrayView\u0026lt;const float, kFftSizeBy2Plus1\u0026gt; prev_analysis_signal_spectrum, rtc::ArrayView\u0026lt;const float, kFftSizeBy2Plus1\u0026gt; signal_spectrum) { // Average speech prob and filter gain for the end of the lowest band. constexpr int kNumAvgBins = 32; constexpr float kOneByNumAvgBins = 1.f / kNumAvgBins; // 计算低带中最后的的kNumAvgBins个频点（靠近8kHz附近）的平均语音概率和平均增益 float avg_prob_speech = 0.f; float avg_filter_gain = 0.f; for (size_t i = kFftSizeBy2Plus1 - kNumAvgBins - 1; i \u0026lt; kFftSizeBy2Plus1 - 1; i++) { avg_prob_speech += speech_probability[i]; avg_filter_gain += filter[i]; } avg_prob_speech = avg_prob_speech * kOneByNumAvgBins; avg_filter_gain = avg_filter_gain * kOneByNumAvgBins; // If the speech was suppressed by a component between Analyze and Process, an // example being by an AEC, it should not be considered speech for the purpose // of high band suppression. To that end, the speech probability is scaled // accordingly. float sum_analysis_spectrum = 0.f; float sum_processing_spectrum = 0.f; for (size_t i = 0; i \u0026lt; kFftSizeBy2Plus1; ++i) { sum_analysis_spectrum += prev_analysis_signal_spectrum[i]; sum_processing_spectrum += signal_spectrum[i]; } // The magnitude spectrum computation enforces the spectrum to be strictly // positive. RTC_DCHECK_GT(sum_analysis_spectrum, 0.f); avg_prob_speech *= sum_processing_spectrum / sum_analysis_spectrum; // Compute gain based on speech probability. float gain = 0.5f * (1.f + static_cast\u0026lt;float\u0026gt;(tanh(2.f * avg_prob_speech - 1.f))); // Combine gain with low band gain. if (avg_prob_speech \u0026gt;= 0.5f) { gain = 0.25f * gain + 0.75f * avg_filter_gain; } else { gain = 0.5f * gain + 0.5f * avg_filter_gain; } // Make sure gain is within flooring range. return std::min(std::max(gain, minimum_attenuating_gain), 1.f); } 选取低带频谱中最后32个bin（即靠近8kHz的部分），计算平均语音概率和平均Wiener滤波器增益 1 2 avg_prob_speech：语音存在的可能性（0~1） avg_filter_gain：低带最后段的增益（代表频谱高频变化） 处理前后谱能量比修正语音概率 1 avg_prob_speech *= sum_processing_spectrum / sum_analysis_spectrum; sum_analysis_spectrum：分析阶段的谱（原始 noisy 语音） sum_processing_spectrum：处理阶段的谱（可能被 AEC、AGC 等处理） ⚠️ 如果语音信号被其他模块（比如回声消除）削弱了，而估计器没注意到，就会过度抑制高带，这里通过能量比进行修正，防止“假静音”影响高频压制判断。 使用 tanh 平滑映射为增益因子 1 gain = 0.5f * (1.f + tanh(2.f * avg_prob_speech - 1.f)); tanh 函数做了一个S型映射：使得 01 的 avg_prob_speech 映射为 01 的增益，但中心更敏感，平滑、可导。 当语音概率高于 0.5 时增益快速上升；低于 0.5 时快速下降。 将该增益和低带平均滤波器增益融合 1 2 3 4 if (avg_prob_speech \u0026gt;= 0.5f) gain = 0.25f * gain + 0.75f * avg_filter_gain; else gain = 0.5f * gain + 0.5f * avg_filter_gain; 当语音概率较高，以低带滤波器为主（强调保留语音的高频特征） 当语音概率较低，用概率增益和滤波增益各一半权重 高带延时处理 1 DelaySignal(y_band, channels_[ch]-\u0026gt;process_delay_memory[b - 1], delayed_frame); 低带由于滤波器组分析→合成，会产生系统延时，因此在低带和高带合成前，需要将高带信号进行延时操作，与低带信号进行对齐。\n六、小结与启发 WebRTC 的 Wiener 降噪模块体现了“理论 + 工程”的完美结合：\n模块 功能 工程优化 Wiener 增益 降噪核心 加入先验平滑与概率调节 SNR估计 支持增益计算 TSA平滑，避免突变 语音概率 增益微调 三特征融合 + Sigmoid 映射 噪声谱估计 保证准确性 分位数估计 + 启动模型 这种设计保证了算法在实时语音场景中的鲁棒性、稳定性与可听感提升效果。\n","permalink":"https://lyapple2008.github.io/posts/202507/2025-07-13-webrtc%E8%AF%AD%E9%9F%B3%E9%99%8D%E5%99%AA%E4%B9%8Bwiener%E6%BB%A4%E6%B3%A2/","summary":"\u003ch1 id=\"webrtc中的wiener滤波降噪算法\"\u003eWebRTC中的Wiener滤波降噪算法\u003c/h1\u003e\n\u003ch2 id=\"引言\"\u003e引言\u003c/h2\u003e\n\u003cp\u003e在实时语音通信系统中，背景噪声的抑制对于提升通话清晰度至关重要。Google 的 WebRTC 项目在其开源语音引擎中实现了高质量的语音降噪模块，其中 Wiener 滤波器作为核心组成部分，结合语音概率估计、多特征建模、噪声谱跟踪等模块构建了一个高度实用的增强框架。\u003c/p\u003e","title":"WebRTC语音降噪之Wiener滤波"},{"content":"WebRTC的语音降噪算法中实现了一个频点维度的语音概率估计器SpeechProbabilityEstimator，本质是一个多特征融合的线性分类器。统计计算以下三种特征，\nLRT Spectral Flatness 谱平坦度 Spectral Difference 谱差 通过tanh将特征变化映射到概率值，使用不同的width参数来调节敏感度，线性加权融合到得最终的语音概率。 接下来完整分析 WebRTC 中用于语音概率估计的三个核心特征（indicator0, indicator1, indicator2）\n1 2 3 4 5 # 代码位置 modules/audio_processing/ns/speech_probability_estimator.h modules/audio_processing/ns/speech_probability_estimator.cc modules/audio_processing/ns/signal_model_estimator.cc modules/audio_processing/ns/signal_model_estimator.h indicator0: Likelihood Ratio Test (LRT) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 // Updates the log LRT measures. void UpdateSpectralLrt(rtc::ArrayView\u0026lt;const float, kFftSizeBy2Plus1\u0026gt; prior_snr, rtc::ArrayView\u0026lt;const float, kFftSizeBy2Plus1\u0026gt; post_snr, rtc::ArrayView\u0026lt;float, kFftSizeBy2Plus1\u0026gt; avg_log_lrt, float* lrt) { RTC_DCHECK(lrt); for (size_t i = 0; i \u0026lt; kFftSizeBy2Plus1; ++i) { float tmp1 = 1.f + 2.f * prior_snr[i]; float tmp2 = 2.f * prior_snr[i] / (tmp1 + 0.0001f); float bessel_tmp = (post_snr[i] + 1.f) * tmp2; avg_log_lrt[i] += .5f * (bessel_tmp - LogApproximation(tmp1) - avg_log_lrt[i]); } float log_lrt_time_avg_k_sum = 0.f; for (size_t i = 0; i \u0026lt; kFftSizeBy2Plus1; ++i) { log_lrt_time_avg_k_sum += avg_log_lrt[i]; } *lrt = log_lrt_time_avg_k_sum * kOneByFftSizeBy2Plus1; } LRT衡量的是，当前观察到的频谱，更像是语音信号，还是更像噪声，是在两种假设之间进行比较：\n$H_0$: 当前帧是纯噪声帧 $H_1$: 当前帧是语音 + 噪声帧 通过计算：\n$$ \\Lambda = \\log \\left( \\frac{P(X | H_1)}{P(X | H_0)} \\right) $$\n其中：\n$P(X|H_1)$：给定为语音的条件下，观测值 X 出现的概率； $P(X|H_0)$：给定为噪声的条件下，观测值 X 出现的概率； 最终近似简化为（推导过程跳过了，找了一些资料，没看太懂🙈）：\n$$ \\text{LRT}(k) \\approx \\frac{(1 + \\gamma_k) \\cdot 2 \\cdot \\xi_k}{1 + 2 \\cdot \\xi_k} - \\log(1 + 2 \\cdot \\xi_k) $$\n其中：\n$\\xi_k$：频点 k 的先验 SNR； $\\gamma_k$：频点 k 的后验 SNR； LRT 越大，表明信号更像语音； LRT 越小，说明更像噪声。 再通过tanh函数映射到[0, 1]的区间。\n1 2 3 4 5 6 7 8 9 10 11 // Width parameter in sigmoid map for prior model. constexpr float kWidthPrior0 = 4.f; // Width for pause region: lower range, so increase width in tanh map. constexpr float kWidthPrior1 = 2.f * kWidthPrior0; // Average LRT feature: use larger width in tanh map for pause regions. float width_prior = model.lrt \u0026lt; prior_model.lrt ? kWidthPrior1 : kWidthPrior0; // Compute indicator function: sigmoid map. float indicator0 = 0.5f * (tanh(width_prior * (model.lrt - prior_model.lrt)) + 1.f); indicator1: Spectral Flatness 谱平坦度 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // Updates the spectral flatness based on the input spectrum. void UpdateSpectralFlatness( rtc::ArrayView\u0026lt;const float, kFftSizeBy2Plus1\u0026gt; signal_spectrum, float signal_spectral_sum, float* spectral_flatness) { RTC_DCHECK(spectral_flatness); // Compute log of ratio of the geometric to arithmetic mean (handle the log(0) // separately). constexpr float kAveraging = 0.3f; float avg_spect_flatness_num = 0.f; for (size_t i = 1; i \u0026lt; kFftSizeBy2Plus1; ++i) { if (signal_spectrum[i] == 0.f) { *spectral_flatness -= kAveraging * (*spectral_flatness); return; } } for (size_t i = 1; i \u0026lt; kFftSizeBy2Plus1; ++i) { avg_spect_flatness_num += LogApproximation(signal_spectrum[i]); } float avg_spect_flatness_denom = signal_spectral_sum - signal_spectrum[0]; avg_spect_flatness_denom = avg_spect_flatness_denom * kOneByFftSizeBy2Plus1; avg_spect_flatness_num = avg_spect_flatness_num * kOneByFftSizeBy2Plus1; float spectral_tmp = ExpApproximation(avg_spect_flatness_num) / avg_spect_flatness_denom; // Time-avg update of spectral flatness feature. *spectral_flatness += kAveraging * (spectral_tmp - *spectral_flatness); } 谱平坦度的定义 $$ \\text{SFM} = \\frac{\\left( \\prod_{i=1}^{N} X_i \\right)^{1/N}}{\\frac{1}{N} \\sum_{i=1}^{N} X_i} = \\frac{\\text{几何均值}}{\\text{算术均值}} $$\n为什么谱平坦度可以区分语音和噪声 首先从纯数学角度，根据均值不等式有：几何均值 \u0026lt;= 算数均值\n当所有元素相等时，两者相等 当元素差异越大时，几何均值相对算数均值越小，说明“越不平坦” 再对应到语音降噪应用：\n噪声（尤其是白噪声）：频带能量均匀，几何均值 ≈ 算数均值，Flatness ≈ 1 语音信号：存在能量集中（共振峰），几何均值 ≪ 算数均值，Flatness 接近 0 因此谱平坦度 = 能量分布的“均匀性量尺” → 能直接用来做语音/噪声分类特征！\n代码实现 计算几何均值 1 2 3 4 for (size_t i = 1; i \u0026lt; kFftSizeBy2Plus1; ++i) { avg_spect_flatness_num += LogApproximation(signal_spectrum[i]); } avg_spect_flatness_num = avg_spect_flatness_num * kOneByFftSizeBy2Plus1; 等价于下面的式子，先算log版本\n$$ \\exp\\left( \\frac{1}{N} \\sum \\log(X_i) \\right) $$\n再做指数运算还原为几何均值\n1 ExpApproximation(avg_spect_flatness_num) 计算算数均值（去掉DC分量） 1 2 3 float avg_spect_flatness_denom = signal_spectral_sum - signal_spectrum[0]; avg_spect_flatness_denom = avg_spect_flatness_denom * kOneByFftSizeBy2Plus1; 得到谱平坦度 1 2 float spectral_tmp = ExpApproximation(avg_spect_flatness_num) / avg_spect_flatness_denom; 平滑更新 1 2 // Time-avg update of spectral flatness feature. *spectral_flatness += kAveraging * (spectral_tmp - *spectral_flatness); indicator2: Spectral Difference 谱模板差异 Spectral Difference频谱差异是用于衡量当前帧的频谱与已学习噪声模板之间的差异程度。其基本思想是：\n如果当前帧的频谱结构与噪声模板相似，则可能是噪声；如果差异大，则可能是语音。\n总体计算公式 $$ \\text{SpectralDiff} = \\text{Var}(X) - \\frac{[\\text{Cov}(X, Y)]^2}{\\text{Var}(Y)} $$ 其中： •\tX：当前帧的 信号频谱； •\tY：历史平均的 噪声频谱（称为 conservative noise spectrum）； •\t$\\mathrm{Var}$：方差（描述“起伏程度”）； •\t$\\mathrm{Cov}$：协方差（描述“是否联动”）。\n为什么它能衡量相似程序？ 从统计角度看，Var(X) - Cov(X, Y)^2 / Var(Y) 是当前帧中 与过去模板不一致的能量。如果：\n如果 signal ≈ noise（噪声帧）：→ covariance² / noise_variance ≈ signal_variance → spectral_diff ≈ 0 如果 signal 包含语音成分（结构和噪声不一样）：→ covariance 小，spectral_diff 增大 这个公式本质上等价于：\nVar(Residual) = Var(Signal) - Var(ProjectedNoiseComponent)\n即：当前帧中除了可以用噪声解释的部分，剩下有多少“异常能量”\n多特征融合 1 2 3 4 // Combine the indicator function with the feature weights. float ind_prior = prior_model.lrt_weighting * indicator0 + prior_model.flatness_weighting * indicator1 + prior_model.difference_weighting * indicator2; 最终组合这三个指标：\n每个特征都有独立的权重 互补性强，提升稳健性 举例子：\n情况 LRT Flatness Spectral Diff 判断结果 短促辅音 [k] 高 高（像噪声） 低（像模板） 不能仅靠 flatness 判断，indicator2 弥补 背景突发噪声 高 高 高 indicator2 抑制误判 语音暂停期 低 高 高 三项均为低，VAD 静音 计算频点的后验语音概率 平滑更新先验语音概率 1 2 3 4 5 // Compute the prior probability. prior_speech_prob_ += 0.1f * (ind_prior - prior_speech_prob_); // Make sure probabilities are within range: keep floor to 0.01. prior_speech_prob_ = std::max(std::min(prior_speech_prob_, 1.f), 0.01f); 计算后验语音概率 因为prior_speech_prob_是通过历史信息估计的当前帧语音概率，因此这个概率称为先验语音概率。实际使用时我们不需要我们观察到当帧后给出的概率，即后验语音概率。 贝叶斯定理给出后验概率公式：\n$$ P(H_1 \\mid X) = \\frac{P(H_1) \\cdot P(X \\mid H_1)}{P(H_1) \\cdot P(X \\mid H_1) + P(H_0) \\cdot P(X \\mid H_0)} $$\n我们引入： •\t$\\text{Prior} = P(H_1)$ •\t$\\text{Gain} = \\frac{1 - \\text{Prior}}{\\text{Prior}}$ •\t$\\text{LRT} = \\frac{P(X \\mid H_1)}{P(X \\mid H_0)}$\n可得后验语音概率（简化推导）：\n$$ P(H_1 \\mid X) = \\frac{1}{1 + \\text{Gain} \\cdot \\frac{1}{\\text{LRT}}} $$\n这正是代码中这段的实现\n1 2 3 4 5 6 7 8 9 // Final speech probability: combine prior model with LR factor:. float gain_prior = (1.f - prior_speech_prob_) / (prior_speech_prob_ + 0.0001f); std::array\u0026lt;float, kFftSizeBy2Plus1\u0026gt; inv_lrt; ExpApproximationSignFlip(model.avg_log_lrt, inv_lrt); for (size_t i = 0; i \u0026lt; kFftSizeBy2Plus1; ++i) { speech_probability_[i] = 1.f / (1.f + gain_prior * inv_lrt[i]); } 其中： •\tgain_prior = (1 - prior_speech_prob_) / (prior_speech_prob_ + ε) •\tinv_lrt[i] = e^{-avg_log_lrt[i]} ≈ 1 / LRT （指数近似）\n利用语音概率辅助更新噪声谱 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 void NoiseEstimator::PostUpdate( rtc::ArrayView\u0026lt;const float\u0026gt; speech_probability, rtc::ArrayView\u0026lt;const float, kFftSizeBy2Plus1\u0026gt; signal_spectrum) { // Time-avg parameter for noise_spectrum update. constexpr float kNoiseUpdate = 0.9f; float gamma = kNoiseUpdate; for (size_t i = 0; i \u0026lt; kFftSizeBy2Plus1; ++i) { const float prob_speech = speech_probability[i]; const float prob_non_speech = 1.f - prob_speech; // Temporary noise update used for speech frames if update value is less // than previous. float noise_update_tmp = gamma * prev_noise_spectrum_[i] + (1.f - gamma) * (prob_non_speech * signal_spectrum[i] + prob_speech * prev_noise_spectrum_[i]); // Time-constant based on speech/noise_spectrum state. float gamma_old = gamma; // Increase gamma for frame likely to be seech. constexpr float kProbRange = .2f; gamma = prob_speech \u0026gt; kProbRange ? .99f : kNoiseUpdate; // Conservative noise_spectrum update. if (prob_speech \u0026lt; kProbRange) { conservative_noise_spectrum_[i] += 0.05f * (signal_spectrum[i] - conservative_noise_spectrum_[i]); } // Noise_spectrum update. if (gamma == gamma_old) { noise_spectrum_[i] = noise_update_tmp; } else { noise_spectrum_[i] = gamma * prev_noise_spectrum_[i] + (1.f - gamma) * (prob_non_speech * signal_spectrum[i] + prob_speech * prev_noise_spectrum_[i]); // Allow for noise_spectrum update downwards: If noise_spectrum update // decreases the noise_spectrum, it is safe, so allow it to happen. noise_spectrum_[i] = std::min(noise_spectrum_[i], noise_update_tmp); } } } 上面这段代码是webrtc中结合语音概率，对之前基于分位数估计得到的噪声谱，进行进一步修正的过程。\n那为什么已经有了基于分位数的噪声估计，还需要在PostUpdate()中进行进一步修正呢？\n如下表，我们对比与初始分位数估计的关系和区别\n特征 PreUpdate() 中的分位数估计 PostUpdate() 中的时间平均更新 原理 统计过往帧的底噪分布（log-domain） 利用当前帧的语音概率进行时间递归更新 更新维度 横向（跨帧分布） 纵向（帧内时间平滑） 响应特性 对背景缓慢变化有响应，对突发语音稳健 在语音帧期间抑制更新，非语音帧中轻微修正 对应变量 quantile_noise_estimator_.Estimate(\u0026hellip;) → noise_spectrum_[] 初值\tnoise_spectrum_[] → 平滑动态追踪修正 目的 建立初步噪声模型 细化并动态追踪噪声谱 总结：为什么需要 PostUpdate？ 分位数估计（PreUpdate）很强健，但慢。 PostUpdate 提供 快速、平滑、概率驱动 的动态调整机制。 防止语音能量污染噪声估计； 保持噪声谱能持续跟踪 非平稳噪声（如空调开关、风声变化）； 为后续 Wiener 滤波器提供更可靠的噪声谱输入。 Ok! 到这里WebRTC就真正完成了噪声谱的估计，接下继续分享WebRTC语音降噪代码.\nTo Be Continue!!! ","permalink":"https://lyapple2008.github.io/posts/202506/2025-06-30-%E8%AF%AD%E9%9F%B3%E6%A6%82%E7%8E%87%E4%BC%B0%E8%AE%A1/","summary":"\u003cp\u003eWebRTC的语音降噪算法中实现了一个频点维度的语音概率估计器SpeechProbabilityEstimator，本质是一个多特征融合的线性分类器。统计计算以下三种特征，\u003c/p\u003e","title":"WebRTC语音降噪之语音概率估计"},{"content":"噪声估计的作用 噪声估计算法在整个语音降噪系统中起到核心支撑作用，先验SNR和后验SNR的计算都依赖于当前帧的噪声功率谱估计。若噪声估计偏低，会导致保留太多噪声（过度保留）；若噪声估计偏高，会把语音当作噪声过滤掉（语音失真）； 更新不稳定，整体听感时好时坏，忽大忽小，出现”泵声“、”音乐噪声“现象。本文介绍WebRTC中目前使用的基于分位数的噪声估计算法，及其在工程实现中的巧妙之处。\n什么是基于分位数的噪声估计 基于分位数的噪声估计算法是一种利用信号统计特性区分噪声和语音的自适应方法。其核心原理在于：噪声的能量分布通常集中在低分位区域，而语音信号的能量分布会抬高高分位数。\nWebRTC中的实现解读（妙呀） 1 2 3 # 代码位置 modules/audio_processing/ns/quantile_noise_estimator.h modules/audio_processing/ns/quantile_noise_estimator.cc 分位数计算 1 2 3 4 5 if (log_spectrum[i] \u0026gt; log_quantile_[j]) { log_quantile_[j] += 0.25f * multiplier; } else { log_quantile_[j] -= 0.75f * multiplier; } 以上是WebRTC中分位数计算的代码，它表示25%分位数估计，下面我们来逐步说明为什么这段代码可以计算25%分位数。\n什么是分位数？ 以25%分位数为例，它表示：如果你观察一组数，有25%是小于它的，有75%是大于它的。 比如：\n1 2 数据（已排序）：[1, 2, 3, 4, 5, 6, 7, 8, 9] 0.25分位数 ≈ 第3个数 = 3 非对称更新的数学直觉 设：当前估计值为Q，当前观测值为X\n我们每帧更新规则如下：\n情况 更新量 含义 x \u0026gt; Q Q ← Q + 0.25 × step 当前值太大，稍微拉高估计值 x \u0026lt; Q Q ← Q - 0.75 × step 当前值太小，大幅拉低估计值 收敛分析：平衡点 = 25%分位数 考虑连续观察大量值 {x₁, x₂, \u0026hellip;, xₙ}，估计值 Q 如果在一个固定位置附近波动，那它一定满足： 平均上调量 ≈ 平均下调量，也就是说，在那个点：\n上调概率 × 上调步长 = 下调概率 × 下调步长\n其中：\n上调概率 p_up = P(x \u0026gt; Q) 下调概率 p_down = P(x \u0026lt; Q) = 1 - p_up 上调步长 = 0.25 下调步长 = 0.75 计算可以得到：p_down = 0.25 ✅ 说明这个估计最终会逼近 25% 分位数！\n这个分位数的实现真是妙了呀，避免了常规分位计算需要排序的问题，同时还可以实时更新。👍\n多分位数估计分时更新 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 // Loop over simultaneous estimates. for (int s = 0, k = 0; s \u0026lt; kSimult; ++s, k += static_cast\u0026lt;int\u0026gt;(kFftSizeBy2Plus1)) { const float one_by_counter_plus_1 = 1.f / (counter_[s] + 1.f); for (int i = 0, j = k; i \u0026lt; static_cast\u0026lt;int\u0026gt;(kFftSizeBy2Plus1); ++i, ++j) { // Update log quantile estimate. const float delta = density_[j] \u0026gt; 1.f ? 40.f / density_[j] : 40.f; const float multiplier = delta * one_by_counter_plus_1; if (log_spectrum[i] \u0026gt; log_quantile_[j]) { log_quantile_[j] += 0.25f * multiplier; } else { log_quantile_[j] -= 0.75f * multiplier; } // Update density estimate. constexpr float kWidth = 0.01f; constexpr float kOneByWidthPlus2 = 1.f / (2.f * kWidth); if (fabs(log_spectrum[i] - log_quantile_[j]) \u0026lt; kWidth) { density_[j] = (counter_[s] * density_[j] + kOneByWidthPlus2) * one_by_counter_plus_1; } } if (counter_[s] \u0026gt;= kLongStartupPhaseBlocks) { counter_[s] = 0; if (num_updates_ \u0026gt;= kLongStartupPhaseBlocks) { quantile_index_to_return = k; } } ++counter_[s]; } kLongStartupPhaseBlocks=200，意味着分位数估计在200帧后，即2秒，才会更新重置输出估计结果。webrtc为了减少响应延迟，设置了三个错位的独立分位数估计器，如下代码，可以看到每一个分位数估计器的更新计数是错开的，这样可以达到每67帧，即670ms，就会有一个分位数估计器进行重置更新输出估计结果，从而达到快速响应的效果。\n1 2 3 4 constexpr float kOneBySimult = 1.f / kSimult; for (size_t i = 0; i \u0026lt; kSimult; ++i) { counter_[i] = floor(kLongStartupPhaseBlocks * (i + 1.f) * kOneBySimult); } 利用”密度“估计实现自适应步长 分位数估计器在更新的时候，其更新步长与这个density_变量直接相关，现在我们来看下webrtc的实现是如何做到自适应步长的。\ndensity_[j]表示: 当前分位数估计点附近的”局部密度估计“，近似表示这个log频谱点的概率密度函数值。\n现实场景中，噪声频点能量分布是变化的，当低噪声变化时，噪声频点能量分布密集；当语音变化时，噪声频点能量分布稀疏。因此需要估计分布密度，以调整步长动态性，防止在高密度或低密度区域过度抖动或者太慢反应\ndensity_是如何计算的 1 2 3 4 5 6 7 // Update density estimate. constexpr float kWidth = 0.01f; constexpr float kOneByWidthPlus2 = 1.f / (2.f * kWidth); if (fabs(log_spectrum[i] - log_quantile_[j]) \u0026lt; kWidth) { density_[j] = (counter_[s] * density_[j] + kOneByWidthPlus2) * one_by_counter_plus_1; } 这是一种滑动窗口统计估计法：\n步骤 说明 fabs(\u0026hellip;) \u0026lt; kWidth 当前观测值是否落在估计值 ±0.01 范围内 kOneByWidthPlus2 = 1 / (2 × 0.01) 这是一个常数权重（经验值） density_[j] = (\u0026hellip;) 使用 指数滑动平均 来更新密度值 最终的效果是： density_[j] 趋近于“单位宽度窗口”内命中次数的平均值 —— 表示在分位点附近的信号频谱密集程度。\n1 2 const float delta = density_[j] \u0026gt; 1.f ? 40.f / density_[j] : 40.f; const float multiplier = delta * one_by_counter_plus_1; density_变量直接影响了分位数估计的步长，也就是说\n如果 density_ 高：说明这个频率点的能量比较“稳定”，变化较小 → delta 会小 → 更新变慢 如果 density_ 低：说明这个点的能量波动大 → delta 会大 → 更新更激进 总结就是，density_ 表示当前分位点附近的局部频谱密度，用于调节更新速率，帮助 WebRTC 实现稳定、鲁棒、快速收敛的底噪估计。\nOk，这就是WebRTC中基于分位数噪声估计的全部了。总的来说，基于分位数的噪声估计算法原理简单，但WebRTC的实现有很多巧妙的地方，即保证了效果，也提高了效率，绝对是工程精华值得好好研究。\n接下来会继续分享WebRTC语音降噪部分代码，希望对有兴趣的朋友有帮助。\n","permalink":"https://lyapple2008.github.io/posts/202506/2025-06-28-%E5%9F%BA%E4%BA%8E%E5%88%86%E4%BD%8D%E6%95%B0%E7%9A%84%E5%99%AA%E5%A3%B0%E4%BC%B0%E8%AE%A1/","summary":"\u003ch2 id=\"噪声估计的作用\"\u003e噪声估计的作用\u003c/h2\u003e\n\u003cp\u003e噪声估计算法在整个语音降噪系统中起到核心支撑作用，先验SNR和后验SNR的计算都依赖于当前帧的噪声功率谱估计。若噪声估计偏低，会导致保留太多噪声（过度保留）；若噪声估计偏高，会把语音当作噪声过滤掉（语音失真）； 更新不稳定，整体听感时好时坏，忽大忽小，出现”泵声“、”音乐噪声“现象。本文介绍WebRTC中目前使用的基于分位数的噪声估计算法，及其在工程实现中的巧妙之处。\u003c/p\u003e","title":"WebRTC语音降噪之基于分位数的噪声估计"},{"content":"程序员的基本修养之代码编译 | 代码编译过程介绍，避坑指南，一些常用代码查看工具使用介绍\n预处理 1.预处理的作用 宏替换：\n替换 #define 定义的宏。\n1 2 3 4 #define PI 3.14159 double circle_area(double radius) { return PI * radius * radius; // 替换后：3.14159 * radius * radius } 头文件包含 替换 #include 指令为头文件的内容。\n1 #include \u0026lt;iostream\u0026gt;// 替换为 \u0026lt;iostream\u0026gt; 文件的完整内容 条件编译 根据条件选择性地编译代码。\n1 2 3 #ifdef DEBUG std::cout \u0026lt;\u0026lt; \u0026#34;Debug mode is on\u0026#34; \u0026lt;\u0026lt; std::endl; #endif 宏展开 处理函数式宏。\n1 2 #define SQUARE(x) ((x) * (x)) int result = SQUARE(5); // 替换为 ((5) * (5)) 注释删除 移除源代码中的注释内容。\n2.查看预处理结果 通过 编译器选项 可以仅执行预处理步骤。例如gcc/clang：\n1 g++ -E main.cpp -o main.i -E 选项表示仅执行预处理。 输出文件 main.i 包含预处理后的源代码。 cmake可以通过添加配置保存中间产物 1 set_target_properties(${PROJECT_NAME} PROPERTIES COMPILE_FLAGS \u0026#34;-save-temps=obj\u0026#34;) 注：一些复杂的宏操作可以通过这种方式确定最终展开后的形式\n3.预处理注意事项 宏展开陷阱 注意宏的嵌套展开可能引发意外行为，用括号保护表达式。\n1 #define ADD(x, y) ((x) + (y)) 头文件滥用 导出了所有头文件并加入了搜索路径，当存在多个同名头文件时，可能会引起一些诡异的编译问题，或者运行时崩溃\n头文件的搜索顺序 1.搜索当前目录（一般是#include \u0026ldquo;header.h\u0026rdquo;，双引号方式引用头文件） 2.通过-I指定的目录，多个目录按加入的顺序搜索 3.标准系统目录 编译 | 编译是从源文件（.c/.cpp）生成目标文件（*.o）的过程\nQ1：目标文件里面包含了哪些信息？ 1 llvm-objdump -s /path/to/objfile # 显示目标文件中所有Section的内容 1.目标文件类型、目标架构 查看命令：\n1 2 # 可以直接使用ndk里面的工具，目标文件/静态库/动态库/可执行文件都可以查看 llvm-readelf -h 目标文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 File: /Users/marshall/Workspace/projects/test_compile/build/lib_src/CMakeFiles/lib_src.dir/conv1d.o Format: Mach-O arm64 Arch: aarch64 AddressSize: 64bit MachHeader { Magic: Magic64 (0xFEEDFACF) CpuType: Arm64 (0x100000C) CpuSubType: CPU_SUBTYPE_ARM64_ALL (0x0) FileType: Relocatable (0x1) NumOfLoadCommands: 4 SizeOfLoadCommands: 520 Flags [ (0x2000) MH_SUBSECTIONS_VIA_SYMBOLS (0x2000) ] Reserved: 0x0 } 2.只读内容存在__TEXT段 a.__text节保存了编译后的机器码: 内容：源代码编译后的二进制机器指令，对应程序的函数和逻辑 查看命令：\n1 llvm-objdump -d 目标文件 注：通过查看中间产物汇编文件（*.s）可以初步分析是否值得做 比如通过查看conv1d.s文件，发现已经做了循环展开，就不需要在c代码上手动做循环展开了（NEON类的SIMD在代码编译是否会进行编译优化待确定）\nb.__cstring节保存了字符串 c.__const节保存了学常量 3.全局变量和静态变量保存在__DATA段 nm命令介绍 nm命令可以用来分析二进制分析的符号信息\n1 2 # -A 选项在符号名前附加文件名，适用于分析静态库（.a）： nm -A 静态库路径 注：该命令可以用来辅助分析Undefined symbol一类的编译问题 Q2：同一份代码，保持编译参数不变的情况，两次编译最终的目标文件是否是一样的？ 通常一致的场景 如果满足以下条件，两次编译的目标文件大概率相同： 代码完全不变：未修改任何源码文件（包括头文件） 编译参数严格一致：包括优化级别（如 -O2）、调试选项（如 -g）、路径参数（如 -I）等 编译器版本一致：同一版本的编译器（如 GCC 12.3）和链接器 环境无干扰：\na.无时间戳或随机化因素嵌入二进制文件（如代码中未使用 DATE、TIME 宏）\nb.编译路径和文件系统结构相同 可能导致不一致的例外情况 时间戳或随机化因素 若源码使用 DATE、TIME 等宏，编译后生成的二进制文件会包含编译时间戳，导致两次编译结果不同。 1 printf(\u0026#34;Build Time: %s %s\\n\u0026#34;, __DATE__, __TIME__); // 每次编译结果不同 调试信息中的路径差异 调试信息（.debug_line 段）默认包含源码绝对路径。若两次编译的源码目录不同，目标文件会不同。 1 2 3 4 5 # 第一次编译路径：/home/user/project/ gcc -g main.c -o main # 第二次编译路径：/tmp/build/ gcc -g main.c -o main # 调试信息中的路径不同，目标文件哈希值不同 链接 | 链接就是把所有目标文件合并到起，同时目标文件中在未知的地址（如在其它文件中实现的函数调用）替换成最终的地址\n注：左侧是目标文件main.o，右侧是最终的可执行程序main ``` Shell llvm-objdump -d 目标文件 ``` 动态库与静态库对比 macos环境下查看依赖\n1 otool -L /path/to/binary 1 2 3 main: @rpath/liblib_src.dylib (compatibility version 0.0.0, current version 0.0.0) /usr/lib/libSystem.B.dylib (compatibility version 1.0.0, current version 1351.0.0) 注：@rpath 会根据不同应用的配置解析到对应的目录\n","permalink":"https://lyapple2008.github.io/posts/202503/2025-03-27-%E4%BB%A3%E7%A0%81%E7%BC%96%E8%AF%91/","summary":"\u003ch1 id=\"程序员的基本修养之代码编译\"\u003e程序员的基本修养之代码编译\u003c/h1\u003e\n\u003cp\u003e| 代码编译过程介绍，避坑指南，一些常用代码查看工具使用介绍\u003c/p\u003e\n\u003ch2 id=\"预处理\"\u003e预处理\u003c/h2\u003e\n\u003ch3 id=\"1预处理的作用\"\u003e1.预处理的作用\u003c/h3\u003e\n\u003cp\u003e\u003cstrong\u003e宏替换\u003c/strong\u003e：\u003cbr\u003e\n替换 #define 定义的宏。\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e3\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e4\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-C\" data-lang=\"C\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cp\"\u003e#define PI 3.14159\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"cp\"\u003e\u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e \u003cspan class=\"nf\"\u003ecircle_area\u003c/span\u003e\u003cspan class=\"p\"\u003e(\u003c/span\u003e\u003cspan class=\"kt\"\u003edouble\u003c/span\u003e \u003cspan class=\"n\"\u003eradius\u003c/span\u003e\u003cspan class=\"p\"\u003e)\u003c/span\u003e \u003cspan class=\"p\"\u003e{\u003c/span\u003e\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e    \u003cspan class=\"k\"\u003ereturn\u003c/span\u003e \u003cspan class=\"n\"\u003ePI\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eradius\u003c/span\u003e \u003cspan class=\"o\"\u003e*\u003c/span\u003e \u003cspan class=\"n\"\u003eradius\u003c/span\u003e\u003cspan class=\"p\"\u003e;\u003c/span\u003e \u003cspan class=\"c1\"\u003e// 替换后：3.14159 * radius * radius\n\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003e\u003cspan class=\"c1\"\u003e\u003c/span\u003e\u003cspan class=\"p\"\u003e}\u003c/span\u003e\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e\n\u003cp\u003e\u003cstrong\u003e头文件包含\u003c/strong\u003e \u003cbr\u003e\n替换 #include 指令为头文件的内容。\u003c/p\u003e","title":"程序员的基本修养之代码编译"},{"content":" 本文记录在mac mini m2 pro上搭建tensorflow gpu版本的过程，中间因为一些坑，导致环境搭建过程有点反复，希望对有相同需求的同学有帮助。 1. 安装conda环境 这里创建conda环境的时候需要指定python版本为3.9，这是因为后面需要安装的tensorflow需要python 3.6~3.9\n1 conda create -n tf_gpu python=3.9 2. 安装tensorflow 安装tensorflow的时候，需要指定版本为2.14.1，这里是因为后面需要安装的tensorflow-metal，最新版本只支持2.14\n1 pip install tensorflow==2.14.1 3. 安装tensorflow-metal 目前tensorflow官方没有支持apple gpu，需要通过tensorflow-metal插件来支持使用apple gpu\n1 pip install tensorflow-metal==1.1.0 4. 测试验证 1 2 3 4 5 import tensorflow.keras import tensorflow as tf print(f\u0026#34;Tensor Flow Version: {tf.__version__}\u0026#34;) gpu = len(tf.config.list_physical_devices(\u0026#39;GPU\u0026#39;))\u0026gt;0 print(\u0026#34;GPU is\u0026#34;, \u0026#34;available\u0026#34; if gpu else \u0026#34;NOT AVAILABLE\u0026#34;) 成功后会得到下面的输出\n1 2 3 4 Tensor Flow Version: 2.14.1 Scikit-Learn 1.5.0 SciPy 1.13.1 GPU is available ","permalink":"https://lyapple2008.github.io/posts/202406/2024-06-10-macos-m2-tensorflow-gpu%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/","summary":"\u003c!-- {% asset_img title.gif %} --\u003e\n\u003cimg src=\"/images/2024-06-10-title.gif\"/\u003e\n本文记录在mac mini m2 pro上搭建tensorflow gpu版本的过程，中间因为一些坑，导致环境搭建过程有点反复，希望对有相同需求的同学有帮助。","title":"MacOS M2 Tensorflow GPU环境搭建"},{"content":"什么是算子融合 算子融合是一种通过合并计算图中多个算子到一个算子，达到减少计算量和内存访问的优化方法。\nConv + BatchNormalization + ReLu融合 从融合后的最终公式可以看出，可以在初始阶段就通过BN的均值和方差（推理阶段BN的均值和方差是常量）更新Conv层的weights和bias参数，这样融合后的算子相当少了一个BN层的操作，既减少了内存访问，也减少了计算量\n如何做算子融合 算子分类 当存在多个输入，同时存在多种输入-输出映射关系时，最终的Mapping type由最复杂的那一个决定。Mapping type复杂度递增顺序：One-to-One，Reorganize，Shuffle，One-to-Many，Many-to-Many 注：Many-to-Many包含Many-to-One的类型\n算子可融合性 绿色：可以融合，且有收益 黄色：要做profile才能确定是否有收益 红色：没有收益，不融合\n计算图基于融合性分块 分块过程：\n随机挑选一个One-to-One 算子节点做为种子节点 从种子节点往后进行融合，直到没有可以融合的节点，并更新块的Mapping Type 从种子节点往前进行融合，直到没有可以融合的节点，并更新块的Mapping Type 重复执行1、2、3，直到没有可用的种子节点 融合代码生成 基于编译生成融合代码（DNNFusion、TVM）\n参考\nDNNFusion: accelerating deep neural networks execution with advanced operator fusion ","permalink":"https://lyapple2008.github.io/posts/202405/2024-05-17-%E7%AE%97%E5%AD%90%E8%9E%8D%E5%90%88/","summary":"\u003ch2 id=\"什么是算子融合\"\u003e什么是算子融合\u003c/h2\u003e\n\u003cp\u003e算子融合是一种通过合并计算图中多个算子到一个算子，达到减少计算量和内存访问的优化方法。\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003eConv + BatchNormalization + ReLu融合\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- {% asset_img conv_bn_relu.png %} --\u003e\n\u003cimg src=\"/images/2024-05-17-conv_bn_relu.png\"/\u003e\n\u003cp\u003e从融合后的最终公式可以看出，可以在初始阶段就通过BN的均值和方差（推理阶段BN的均值和方差是常量）更新Conv层的weights和bias参数，这样融合后的算子相当少了一个BN层的操作，既减少了内存访问，也减少了计算量\u003c/p\u003e","title":"算子融合"},{"content":"正在研究的项目 《人人都用能英语》 看了后发现之前关于“学”英语很多的观念都是错了\n有意思博文记录 TinyProject 通过记录完成一个小项目的方式来进行学习\u0026hellip;\n思考 2022.07.20 今天乐乐出生了，从些人生多一个父亲的身份，很欢喜，也很彷徨。\n","permalink":"https://lyapple2008.github.io/about_me/","summary":"\u003ch3 id=\"正在研究的项目\"\u003e正在研究的项目\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"http://xiaolai.co/books/c558c667ad9f05ddce38f06df2d15aa3/chapter1.html\"\u003e《人人都用能英语》\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e看了后发现之前关于“学”英语很多的观念都是错了\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"有意思博文记录\"\u003e有意思博文记录\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://tinyprojects.dev/\"\u003eTinyProject\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e通过记录完成一个小项目的方式来进行学习\u0026hellip;\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003ch3 id=\"思考\"\u003e思考\u003c/h3\u003e\n\u003cul\u003e\n\u003cli\u003e2022.07.20\u003c/li\u003e\n\u003c/ul\u003e\n\u003cp\u003e今天乐乐出生了，从些人生多一个父亲的身份，很欢喜，也很彷徨。\u003c/p\u003e","title":"关于我"},{"content":"Android中几种sdkVersion的区别 最近遇到一个由于升级了targetSdkVersion而引起的线上crash，之前一直对于Android里面几个sdkVersion的含义和作用很模糊，正好这次把这几个不同的sdkVersion理清楚。\nminSdkVersion 用于指定应用运行所需最低API级别的整数。如果系统的API级别低于属性中指定的值，Android系统将阻止用户安装应用。\ncompileSdkVersion compileSdkVersion只是用来告诉Gradle用哪个Android SDK版本编译你的应用，当使用到新添加的API时就需要使用对应Level的Android SDK。这里需要强调的是，compileSdkVersion只会影响编译的时候，例如，当前我们想使用Android 12一个新的API功能，这时我们就需要将compileSdkVersion升级到31\n1 2 3 4 android { compileSdkVersion 31 ... } 但是这里我们指定了compileSdkVersion到新版本只是让APP编译可以通过，因为在旧的Android系统上还没有使用的新的API，因此在实际代码中还需要对运行时的系统API级别进行判断，保证使用的新API只会在Android12以上的系统运行。\ntargetSdkVersion 要理解targetSdkVersion，需要知道targetSdkVersion在Android中的作用。targetSdkVersion是Android系统提供向后兼容的主要手段（即：新版本SDK手机兼容旧版本SDK工程）。这是什么意思呢？随着 Android 系统的升级，某个系统的 API 或者模块的行为可能会发生改变，但是为了保证老 APK 的行为还是和以前兼容。只要 APK 的 targetSdkVersion 不变，即使这个 APK 安装在新 Android 系统上，其行为还是保持老的系统上的行为，这样就保证了系统对老应用的前向兼容性。 总结： android更新api大概有两种，一种是完全重写（这种就不干targetSdk什么事了）；另一种，保留了老版本的处理逻辑，同时又新增了新的逻辑（用if else的方式来判断具体运行哪段逻辑）。而targetSdk就是用来判断这个if-else的。\n","permalink":"https://lyapple2008.github.io/posts/202208/2022-08-24-android%E4%B8%AD%E5%87%A0%E7%A7%8Dsdkversion%E7%9A%84%E5%8C%BA%E5%88%AB/","summary":"\u003ch1 id=\"android中几种sdkversion的区别\"\u003eAndroid中几种sdkVersion的区别\u003c/h1\u003e\n\u003cblockquote\u003e\n\u003cp\u003e最近遇到一个由于升级了targetSdkVersion而引起的线上crash，之前一直对于Android里面几个sdkVersion的含义和作用很模糊，正好这次把这几个不同的sdkVersion理清楚。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003c!-- {% asset_img 001.png %} --\u003e\n\u003cimg src=\"/images/2022-08-24-001.png\"/\u003e","title":"Android中几种sdkVersion的区别"},{"content":"分析综合滤波器组的作用 由于音频信号在不同的频率范围具有不同的特性，因此在音频处理之前通常都会使用分析综合滤波器组将音频信号分成不同的频率子带，再分别进行处理。比如，音频编码中常用到的子带编码（Subband coding）；webrtc的VAD中，会对不同的频率子带计算特征，再综合这些特征计算语音概率。\n分析综合滤波器组是如何实现 在WebRTC中使用最多的是基于IIR实现的二通道QMF分析综合滤波器组，通过二通道QMF滤波器组，可以很方便实现N等分的子带分解，因此这里只介绍二通道QMF滤波器组。\n从上图可以看出，分析综合滤波器包含分析部分和综合部分，当中间没有任何处理时，整个系统的输入输出关系如下：\n可以看到分析滤波器部分的高通和低通部分的频率响应网线正好是相对pi/2镜像对称的，QMF叫镜像滤波器的由来。图中Xa0代表的就是输入信号的低频部分，而Xa1代表的就是输入信号的高频部分，这样通过分析滤波器后，就可以对信号的低频部分和高频部分进行分别处理了。\n因此只要按下面的等式进行滤波器设计，就可以让A(z)=0，即消除混叠，实现完善重构。 为了效率，通常会采用多相形式实现QMF组，如下图所示，信号处理前都会进行抽取操作，这些实际处理的数据量就减少了，从而提升了执行效率\n在QMF组的多相形式中对应的低通滤波器和高通滤波器如上式所示。 WebRTC中的实现\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 void WebRtcSpl_AnalysisQMF(const int16_t* in_data, size_t in_data_length, int16_t* low_band, int16_t* high_band, int32_t* filter_state1, int32_t* filter_state2) { size_t i; int16_t k; int32_t tmp; int32_t half_in1[kMaxBandFrameLength]; int32_t half_in2[kMaxBandFrameLength]; int32_t filter1[kMaxBandFrameLength]; int32_t filter2[kMaxBandFrameLength]; const size_t band_length = in_data_length / 2; RTC_DCHECK_EQ(0, in_data_length % 2); RTC_DCHECK_LE(band_length, kMaxBandFrameLength); // Split even and odd samples. Also shift them to Q10. for (i = 0, k = 0; i \u0026lt; band_length; i++, k += 2) { half_in2[i] = ((int32_t)in_data[k]) * (1 \u0026lt;\u0026lt; 10); half_in1[i] = ((int32_t)in_data[k + 1]) * (1 \u0026lt;\u0026lt; 10); } // All pass filter even and odd samples, independently. WebRtcSpl_AllPassQMF(half_in1, band_length, filter1, WebRtcSpl_kAllPassFilter1, filter_state1); WebRtcSpl_AllPassQMF(half_in2, band_length, filter2, WebRtcSpl_kAllPassFilter2, filter_state2); // Take the sum and difference of filtered version of odd and even // branches to get upper \u0026amp; lower band. for (i = 0; i \u0026lt; band_length; i++) { tmp = (filter1[i] + filter2[i] + 1024) \u0026gt;\u0026gt; 11; low_band[i] = WebRtcSpl_SatW32ToW16(tmp); tmp = (filter1[i] - filter2[i] + 1024) \u0026gt;\u0026gt; 11; high_band[i] = WebRtcSpl_SatW32ToW16(tmp); } } 上面是WebRTC中关于分析滤波器部分的实现，从代码中可以看出WebRTC中的分析综合滤波器是基于全通滤波器的QMF多相实现，其中的全通滤器采用了IIR实现，即其中的P0(z)和P1(z)都是全通滤波器。通过参考[2]我们可以梳理这个问题的处理流程，通过分析QMF分析综合滤波器满足完美重构的条件，可以得到H0、H1、G0、G1之间的关系，同时H0和H1是基于pi/2，因此只需要知道H0，最终转化成H0低通滤波器的设计问题。进一步的由于采用了基于IIR的全通滤波器，因此只需要考虑相位失真问题，最终QMF分析综合滤波器问题转换成了滤波器的相位均衡问题。虽然我们知道了设计QMF分析综合滤波器的原理和思路，但是想设计一个完全可用的滤波器还是很有难度的，下面我们直接看下WebRTC中QMF分析综合滤波器的效果，如下图所示，可以看到对应的低通滤波器和高通滤波器都有很窄的过渡带，整个系统的幅值响应几乎接近0dB的，同时除了pi/2附近的频带都是近似线性相位的。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 import scipy.signal as signal import numpy as np import matplotlib.pyplot as plt import control def analysis_synthesis_filter(): filter1_coef = [6418.0 / 65536.0, 36982.0 / 65536.0, 57261.0 / 65536.0] filter2_coef = [21333.0 / 65536.0, 49062.0 / 65536.0, 63010.0 / 65536.0] ha0_0_b = [1, 0, filter1_coef[0]] ha0_0_a = [filter1_coef[0], 0, 1] ha0_1_b = [1, 0, filter1_coef[1]] ha0_1_a = [filter1_coef[1], 0, 1] ha0_2_b = [1, 0, filter1_coef[2]] ha0_2_a = [filter1_coef[2], 0, 1] ha1_0_b = [1, 0, filter2_coef[0]] ha1_0_a = [filter2_coef[0], 0, 1] ha1_1_b = [1, 0, filter2_coef[1]] ha1_1_a = [filter2_coef[1], 0, 1] ha1_2_b = [1, 0, filter2_coef[2]] ha1_2_a = [filter2_coef[2], 0, 1] ha0_b = np.convolve(ha0_2_b, np.convolve(ha0_1_b, ha0_0_b)) ha0_a = np.convolve(ha0_2_a, np.convolve(ha0_1_a, ha0_0_a)) ha1_b = np.convolve([1, 0], np.convolve(ha1_2_b, np.convolve(ha1_1_b, ha1_0_b))) ha1_a = np.convolve(ha1_2_a, np.convolve(ha1_1_a, ha1_0_a)) ha0_sys = control.TransferFunction(ha0_b, ha0_a) ha1_sys = control.TransferFunction(ha1_b, ha1_a) b0_sys = ha1_sys b1_sys = ha0_sys delay_sys = control.TransferFunction([1, 0], [1]) lowpass_sys = 0.5 * (ha0_sys + ha1_sys) highpass_sys = 0.5 * (ha0_sys - ha1_sys) t_sys = delay_sys / 2 * (ha0_sys * b0_sys + ha1_sys * b1_sys) # print(t_sys) lowpass_num = lowpass_sys.num[0][0] lowpass_den = lowpass_sys.den[0][0] highpass_num = highpass_sys.num[0][0] highpass_den = highpass_sys.den[0][0] t_num = t_sys.num[0][0] t_den = t_sys.den[0][0] lowpass_w, lowpass_h = signal.freqz(lowpass_num, lowpass_den) highpass_w, highpass_h = signal.freqz(highpass_num, highpass_den) t_w, t_h = signal.freqz(t_num, t_den) fig, axes = plt.subplots(3, 2) axes[0, 0].plot(lowpass_w/np.pi, 20*np.log10(np.abs(lowpass_h))) axes[0, 1].plot(lowpass_w/np.pi, np.unwrap(np.angle(lowpass_h, deg=True))) axes[1, 0].plot(highpass_w/np.pi, 20*np.log10(np.abs(highpass_h))) axes[1, 1].plot(highpass_w/np.pi, np.unwrap(np.angle(highpass_h, deg=True))) axes[2, 0].plot(t_w/np.pi, 20*np.log10(np.abs(t_h))) axes[2, 1].plot(t_w/np.pi, np.unwrap(np.angle(t_h, deg=True))) plt.show() 最近的一些心得 最后说下近期的两点小心得：\n不懂的知识点请尽早弄懂它。其实对于QMF分析综合滤波器组，在最开始学习音频编码时就遇到了，但是当时没有深入的去搞清楚，最后还是没有躲过去。所以最近也是恶补了很多基础知识，才大致了解了QMF的设计思路。 不断地输出也许是应对焦虑的一种方法。人到中年难免焦虑，就不停地去学习去吸收，但是往往又是很低效的，时间花了，却什么也没有留下。特别是现在这个信息爆炸的时代，各种信息流，碎片化阅读，让我们看起来收获了很多，其实什么也没有。其实我们要对自己是几平米的房子有基本的认知，小房子就应该放少的、小的东西，定期对房间进行整理，永远保持一定的空间的留白，才会有喘息和美的余地。 参考 Book:《数字信号处理：理论、算法与应用》 Paper: 具有良好重建特性的正交镜像IIR滤波器组的设计新方法 Paper: IIR QMF-bank design for speech and audio subband coding Blog: WebRTC VAD 中所用滤波器之分析_book_bbyuan的博客-CSDN博客 ","permalink":"https://lyapple2008.github.io/posts/202202/2022-02-03-qmf%E5%88%86%E6%9E%90%E7%BB%BC%E5%90%88%E6%BB%A4%E6%B3%A2%E5%99%A8/","summary":"\u003ch1 id=\"分析综合滤波器组的作用\"\u003e分析综合滤波器组的作用\u003c/h1\u003e\n\u003cp\u003e由于音频信号在不同的频率范围具有不同的特性，因此在音频处理之前通常都会使用分析综合滤波器组将音频信号分成不同的频率子带，再分别进行处理。比如，音频编码中常用到的子带编码（Subband coding）；webrtc的VAD中，会对不同的频率子带计算特征，再综合这些特征计算语音概率。\u003c/p\u003e\n\u003c!-- {% asset_img 001.png %} --\u003e\n\u003cimg src=\"/images/2022-02-03-001.png\"/\u003e","title":"QMF分析综合滤波器"},{"content":" WebRTC由音频引擎、视频引擎和传输模块，音频处理在WebRTC占了很大一块，本文主要对WebRTC中涉及的音频处理进行简单介绍，不会对具体的实现进行介绍。上图是WebRTC中音频处理的流水线（上面的图是网上的图，如有侵权，通知即删），可以看出主要包含了音频采集播放、音频处理、音频编解码和音频传输。下面对这些模块逐一进行介绍。\n音频采集播放 自然界的声音，包括人说出来的声音，都是模拟信号，这些模拟信号是不能被计算机存储和识别的，也不能通过网络进行传输。音频采集就是声音从模拟信号转换成数字信号的过程，而音频播放就是声音从数字信号转换成模拟信号的过程。对于音频数字信号来说有以下几个最主要的参数：\n采样率 采样率是指录音设备在一秒钟内对声音信号的采样次数，单位是Hz，采样频率越高，声音的还原度越真实越自然。但是人耳可以听到的频度范围是20Hz~20000Hz，然后根据采样定理，也就是说最低只需要40kHz采样率就可以满足人耳的需求了，因此CD音质和音乐音频都是44.1kHz。但是采样率越高意味着数据量越大，因此在一些特殊的场景会使用更低的采样率，如语音通话场景，因为人声都是低于4kHz，因此在语音通话场景更多会使用8kHz或者16kHz的采样率。 采样位宽 声音模拟信号经过采样后得一个个样点的值，这个值需要存储到计算机中，那么使用多少位数来表示这个 就是采样位宽，通常使用最多的是16bit，正好就是一个short类型。 声道数 在使用录音设备进行声音采集时，只能表示到采集点处的声音信号，但是实际声音是有空间信息，为了表达声音的空间信息，就有了声道的概念。这里最有名的可能就是杜比全景声了，大家在电影院看电影的时候，感受到的被声音包围的真实感就是多声道的功劳了，当然这里并不是加多几个通道这么简单啦。 音频编解码 虽然音频的数据量没有视频的量那么大，但是如果直接传输原始的音频数据（原始的音频数据有个名字叫Pulse Code Modulation (PCM)），数据量也是挺大的。现在我们来计算下1秒采样率为48kHz双通道的音频信号的数据量有多大，1秒 * 48000个采样点 * 2个声道 * 每个样点2Bytes，这样1秒的数据量是192kB字节。这样的数据量在互联网发展的早期也是不可以接受的，这时候就需要音频编解码。简单的说，音频编解码就是利用人耳的心理声学特性将也一些不影响听觉的信号丢弃，从而减少信号量的方法，在WebRTC中用得最多的是Opus格式。对于音频编码器来说一个很重要的参数是码率，单位是kbps，即每秒的音频使用多少bits位来表示。音频编解码是音频领域一个很专业的领域，这里就不展开了。\n音频处理 在实时通话场景，需要面对各种复杂的环境，处理各种音频问题，最常见的如噪声、回声、声音过大过小等，WebRTC中有一个专门的音频处理模块来处理这些问题，下面就简单介绍下这些模块。\n回声消除（Acoustic Echo Cancellation） 实际通话场景是一个全双工通信系统非容易产生回声，如下图所示，远端说话声==》近端扬声器播放==》近端麦克风录制==》通过网络传输到远端的扬声器播放，经过这样一个音频环路后，远端又在扬声器里听到自己的声音，也就是回声，如果回声的延时很低时还会产生啸叫，这样是很影响通话体验的。这时就需要回声消除AEC模块了，通常录音数据都会先经过个模块，在传输前先把远端播放的数据消除，这样远端在播放的时候就不会听到回声了。回声消除AEC详细的原理介绍留到后面再介绍，这里先挖个坑。 噪声抑制（Noise Suppression） 噪声抑制NS这个很好理解，实际的通话场景都会存在各种各样的噪声，为了保证通话体验不受影响，这时就需要噪声抑制NS模块了。这里也再挖一坑，后面再根据WebRTC源码进行噪声抑制原理介绍。 自动增益控制（Auto Gain Control） 在实际通话过程中，由于使用设备的差异和通话时离麦克风的远近，导致了通话的音量差异，为了达到统一的体验就需要进行自动增益控制，简单说就是当音量小时调大增益，当音量大时调小增益，达到减少音量起伏的作用。 混音（Mix） 在多人通话场景下，我们需要接收和播放的通常不止一条音频流，但通常只有一个播放设备，因此通常需要对多条音频流先进行混音操作，再进行播放。 音频传输 目前WebRTC音频传输是UDP/RTP/RTCP协议基础上进行传输的，底层UDP协议的不可靠性，导致丢包不可避免，同时音频数据与其它的数据内容有其特殊性，通常一点音频异常都很容易被人感知出来。因此WebRTC针对音频传输做了很多额外的工作，除了最常见的丢包重传，丢包补偿等，还有一个NetEQ模块，会在播放端进行音频播放的加减速来进一步减少由于网络抖动引起的音频异常。\n到这里WebRTC中跟音频相关的技术模块都简单的过了一遍，这样大家对于WebRTC音频处理有个大概的印象，这里的每个模块值得深入去学习，后面也把自己学习的一些心得记录在这里，大家一起学习呀。\n","permalink":"https://lyapple2008.github.io/posts/202112/2021-12-22-webrtc%E4%B8%AD%E7%9A%84%E9%9F%B3%E9%A2%91%E5%A4%84%E7%90%86%E6%A6%82%E8%A7%88/","summary":"\u003c!-- {% asset_img 001.png %} --\u003e\n\u003cimg src=\"/images/2021-12-22-001.png\"/\u003e\n\u003cp\u003eWebRTC由音频引擎、视频引擎和传输模块，音频处理在WebRTC占了很大一块，本文主要对WebRTC中涉及的音频处理进行简单介绍，不会对具体的实现进行介绍。上图是WebRTC中音频处理的流水线（上面的图是网上的图，如有侵权，通知即删），可以看出主要包含了音频采集播放、音频处理、音频编解码和音频传输。下面对这些模块逐一进行介绍。\u003c/p\u003e","title":"WebRTC中的音频处理概览"},{"content":"WebRTC Android源码编译 对于WebRTC的学习来说，首先需要搞定的是源码的编译，由于国内的特殊环境和WebRTC本身的复杂性，导致WebRTC源码编译成了WebRTC学习的第一道门槛。这里把自己在编译WebRTC源码过程中遇到的一些坑进行总结分享，希望对大家有帮助。主要参考官网和WebRTC的编译配置脚本，整个操作过程需要具备科学上学的环境，第一次下载好环境和源码后，后面就不需要了。这里以Android端为例，编译环境为Ubuntu 18.04（再高版本会提示不支持），WebRTC官网有提到Android端暂时只支持在Linux下编译。虽然网上也有资源介绍在Mac环境下编译的，但是用虚拟机装个Ubuntu还是挺方便的，咱就不折腾了。\n安装depots_tools工具包 1 git clone https://chromium.googlesource.com/chromium/tools/depot_tools.git 将depot_tools目录添加到环境变量PATH中\n1 2 3 vim ~/.profile export PATH=\u0026#34;$PATH:/path/to/depot_tools\u0026#34; source ~/.profile 下载WebRTC源码 1 fetch --nohooks webrtc_android 由于WebRTC源码比较大，中间网络问题可能会出错，出错的时候用gclient sync继续就可以了\n下载编译所需依赖包和工具 1 2 ./build/install-build-deps.sh --no-chromeos-fonts . /build/install-build-deps-android.sh 下载指定分支 1 2 $ git checkout -b my_branch refs/remotes/branch-heads/83 $ gclient sync 源码编译 第一条命令是生成编译工程的，可以添加一些参数来控制编译生成，这里可以研究下源码里的webrtc.gni文件，里面有可以指定的参数，这里主要介绍三个：\ntarget_os：因为是在Android平台上运行的，因此这里指定为\u0026quot;android\u0026quot; target_cpu: 这里指定运行的硬件平台，arm平台则是\u0026quot;arm\u0026quot;，如果是arm64平台则是\u0026quot;arm64\u0026quot; is_debug：表示生成是否是debug包\n第二条命令是启动ninja开始编译，编译成功后，会在out_arm/debug目录生成对应的jar包和so库文件\n1 2 gn gen out_arm/debug --args=\u0026#39;is_debug=true target_os=\u0026#34;android\u0026#34; target_cpu=\u0026#34;arm\u0026#34; rtc_include_tests=false rtc_build_tools=false rtc_build_examples=false\u0026#39; ninja -C out_arm/debug 编译问题解决 问题一：gn.py运行失败 gn.py: Could not find checkout in any parent of the current path. This must be run inside a checkout.\n这个问题通常发生在，移动了WebRTC源码目录的时候。这里需要看下是否已经把下载的WebRTC源码都完整拷贝了，进到下载WebRTC源码的目录可以看到这个目录还有几个隐藏目录和文件，这几个文件也是需要一起拷贝过去的，后面用gn命令生成编译工程的时候会去检查这几个文件。\n.cipd .gclient .gclient_entries\n问题二： chromium style问题 clang.gni中关闭chromium style检查，这里只是暂时关闭，为了代码风格的统一介绍还是按照chromium style还添加自己的代码\n1 2 3 4 5 6 7 declare_args() { # Indicates if the build should use the Chrome-specific plugins for enforcing # coding guidelines, etc. Only used when compiling with Clang. clang_use_chrome_plugins = false # is_clang \u0026amp;\u0026amp; !is_nacl \u0026amp;\u0026amp; !use_xcode_clang clang_base_path = default_clang_base_path } 参考：\nWebRTC官网 ","permalink":"https://lyapple2008.github.io/posts/202009/2020-09-07-webrtc-android%E6%BA%90%E7%A0%81%E7%BC%96%E8%AF%91/","summary":"\u003ch1 id=\"webrtc-android源码编译\"\u003eWebRTC Android源码编译\u003c/h1\u003e\n\u003cp\u003e对于WebRTC的学习来说，首先需要搞定的是源码的编译，由于国内的特殊环境和WebRTC本身的复杂性，导致WebRTC源码编译成了WebRTC学习的第一道门槛。这里把自己在编译WebRTC源码过程中遇到的一些坑进行总结分享，希望对大家有帮助。主要参考官网和WebRTC的编译配置脚本，整个操作过程需要具备科学上学的环境，第一次下载好环境和源码后，后面就不需要了。这里以Android端为例，编译环境为Ubuntu 18.04（再高版本会提示不支持），WebRTC官网有提到Android端暂时只支持在Linux下编译。虽然网上也有资源介绍在Mac环境下编译的，但是用虚拟机装个Ubuntu还是挺方便的，咱就不折腾了。\u003c/p\u003e","title":"WebRTC系列-WebRTC_Android源码编译"},{"content":"问题描述 原来的github.io自定义域名博客不能访问，提示如下信息\n1 2 Fastly error: unknown domain: beyoung.xyz. Please check that this domain has been added to a service. Details: cache-lax8629-LAX 问题原因 Github Pages修改了公布的IP，可以到这个网址查到Github Pages目前公布的最新IP\nhttps://help.github.com/en/github/working-with-github-pages/managing-a-custom-domain-for-your-github-pages-site\n解决方案 只需在将原来在阿里云上配置的CNAME，修改到最新的Github Pages上\n在Ping中检验是否已经修改到最新的IP上，如果已经生效，则原来的博客就可以访问了\n","permalink":"https://lyapple2008.github.io/posts/202004/2020-04-25-github-io%E5%8D%9A%E5%AE%A2%E6%97%A0%E6%B3%95%E8%AE%BF%E9%97%AE%E9%97%AE%E9%A2%98/","summary":"\u003ch3 id=\"问题描述\"\u003e问题描述\u003c/h3\u003e\n\u003cp\u003e原来的github.io自定义域名博客不能访问，提示如下信息\u003c/p\u003e\n\u003cdiv class=\"highlight\"\u003e\u003cdiv class=\"chroma\"\u003e\n\u003ctable class=\"lntable\"\u003e\u003ctr\u003e\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode\u003e\u003cspan class=\"lnt\"\u003e1\n\u003c/span\u003e\u003cspan class=\"lnt\"\u003e2\n\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\n\u003ctd class=\"lntd\"\u003e\n\u003cpre tabindex=\"0\" class=\"chroma\"\u003e\u003ccode class=\"language-fallback\" data-lang=\"fallback\"\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eFastly error: unknown domain: beyoung.xyz. Please check that this domain has been added to a service.\n\u003c/span\u003e\u003c/span\u003e\u003cspan class=\"line\"\u003e\u003cspan class=\"cl\"\u003eDetails: cache-lax8629-LAX\u003c/span\u003e\u003c/span\u003e\u003c/code\u003e\u003c/pre\u003e\u003c/td\u003e\u003c/tr\u003e\u003c/table\u003e\n\u003c/div\u003e\n\u003c/div\u003e","title":"github.io博客无法访问问题"},{"content":"\n为什么需要代码优化 提升体验，扩展玩法 减少限制跟要求，降低门槛 ，覆盖更多群体 在谷歌内部，打造AI应用有两条思路，一是让更多人受惠，二是尽可能减少限制和要求\n场景限制必须进行优化 个人隐私越来越被重视，以往的云端处理方案存在局限性 一些场景要求算法要有极低的延时，如实时通信项目\nFlag终极目标：让算法随手可得 传统代码优化 rnnoise优化举例\n函数近似和查表优化sigmoid函数计算 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 static OPUS_INLINE float tansig_approx(float x) { int i; float y, dy; float sign=1; /* Tests are reversed to catch NaNs */ if (!(x\u0026lt;8)) return 1; if (!(x\u0026gt;-8)) return -1; #ifndef FIXED_POINT /* Another check in case of -ffast-math */ if (celt_isnan(x)) return 0; #endif if (x\u0026lt;0) { x=-x; sign=-1; } i = (int)floor(.5f+25*x); x -= .04f*i; y = tansig_table[i]; dy = 1-y*y; y = y + x*dy*(1 - y*x); return sign*y; } static OPUS_INLINE float sigmoid_approx(float x) { return .5 + .5*tansig_approx(.5*x); } 最粗暴的函数近似方法：\n``` atan(pi*x/2)*2/pi 24.1 ns atan(x) 23.0 ns 1/(1+exp(-x)) 20.4 ns 1/sqrt(1+x^2) 13.4 ns erf(sqrt(pi)*x/2) 6.7 ns tanh(x) 5.5 ns x/(1+|x|) 5.5 ns ``` 减少程序跳转优化RNN计算 CPU多级Cache机制 深度学习移动端优化 网络剪枝 网络的参数都存在冗余的，所以可以进行网络剪枝 网络剪枝的流程 权值的重要性：计算L1或者L2\n神经元的重要性：不为零的次数 剪权值VS剪神经元 剪权值 剪神经元\n剪权值：模型不规则，不便于实现和加速\n剪神经元：模型规则，便于实现和加速\n精简模型设计 标准CNN 深度分离卷积Depthwise Separable Convolution 【参见MobileNet】 参数比较 四、深度学习移动端部署工具 与PC端深度学习环境被大厂垄断不同，移动端的部署工具可以说是百家争鸣，很多深度学习的厂商都会推出自家的推理加速工具。 4.1 硬件厂商 公司 硬件架构 开发工具 海思 NPU HiAI Foundation 高通 CPU/GPU/DSP Snapdragon Neural Processing Engine SDK Apple CPU/GPU/NeualEngine CoreML ARM CPU/GPU ARM NN SDK MediaTek CPU/GPU/APU NeuroPilot SDK 4.2 软件厂商 工具 公司 系统支持情况 特点 TensorFlow Lite Google Android/IOS Android结合比较密，支持GPU加速 CoreML Apple IOS 软件硬件结合紧密，更新快 Caffe2 Facebook Android/IOS NCNN Tencent Android/IOS 支持大部分CNN网络，已经落地的应用多 MACE 小米 Android MNN 阿里 Android/IOS 利用Caffe2进行手写数字识别在Android端的部署 五、参考 [Paper] A Survey of Model Compression and Acceleration\nfor Deep Neural Networks [Book] 解析卷积神经网络\u0026ndash;深度学习实践手册 [Video] Toward Efficient Deep Neural Network Deployment: Deep Compression and EIE [Video] 李宏毅-Network Compression [Github] 模型压缩及移动端部署 ","permalink":"https://lyapple2008.github.io/posts/201908/2019-08-04-%E4%BB%A3%E7%A0%81%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95%E8%AE%BA/","summary":"\u003cp\u003e\u003ca name=\"01ca666c\"\u003e\u003c/a\u003e\u003c/p\u003e\n\u003ch2 id=\"为什么需要代码优化\"\u003e为什么需要代码优化\u003c/h2\u003e\n\u003cul\u003e\n\u003cli\u003e提升体验，扩展玩法\u003c/li\u003e\n\u003c/ul\u003e\n\u003c!-- ![image.png](https://user-images.githubusercontent.com/3350865/62418824-63b24f80-b6a5-11e9-9bde-6cd6de0cc6a5.PNG) --\u003e\n\u003cimg src=\"/images/2019-08-04-001.png\"/\u003e","title":"代码优化方法论"},{"content":"还记得刚毕业那会找工作，那是一定要找法工程师这个岗位，当时就只是觉得这个岗位牛逼是公司的核心岗位，但是可能对于算法工程师与一般的软件工程有什么区别其实心理也说不清楚。作为一个到今年6月份就工作满5年了的老菜鸟，这期间工作的title也是算法工程师（虽然有些并不是真的在做算法），反正在以算法工程师的title在企业工作的感受与毕业那会想像中上的算法工程师的工作完全不一样。\n我觉得所谓的算法工程师应该叫算法应用工程师会更合适一些。因为算法工程师的工作流程大概是这样的（以我自身的经历为例，可能不同公司有不同的工作方式），接到一个功能需求，然后开始调研实现这个功能需要的算法，研究过程中挑选两三种可能的方案，对这些挑选的方案进行实现（也可能是github clone），分析对比这几种方案。最后从中选出一种方案，作为最后的方案。接下来就是对最终的算法方案针对自己的功能需求和业务场景进行做优化，这期间你会对这个算法方案进行原理研究，参数调整，性能优化等等，最终的目标就是尽可能满足业务需求方的要求。\n知乎上@Jackpop的回答应该是符合大多数算法工程师的实际工作状态，而我们毕业那会想象中的算法工程师，天天手推工式那种，应该叫科学家或者研究员更合适些。总结一下，我觉得一个优秀的算法工程师应该是能够针对不同的业务场景选择最优的算法方案，并能对现有的算法针对业务场景做一些微调和优化，以更进一步适合业务场景。因为对于企业来说不能落地的算法，是丝毫没有价值的。\n以下转载自知乎：https://www.zhihu.com/question/310484101/answer/644079765\n算法工程师大致做什么的？ 算法，对于大多数理工科学生并不陌生。无论是学计算机还是学数学，或者其他理工科，我们都会接触很多成熟、经过十几年甚至几十年考验的算法。 算法工程师的关键点在“工程师”这三个字，日常所做的工作无非是选择一种或几种成熟、优秀的算法进行组合、验证，来解决特定场景下的问题。在大多数企业里面这一点体现更为明显，**在企业里作为算法工程师是不会创造算法，只是使用经过多年考验的成熟、稳定算法。**其实现在不仅是企业界，就连学术界，创新可用的新算法也是寥寥可数，就拿人工智能常用的优化算法来说：\n随机梯度 共轭梯度 牛顿法粒子群 遗传 贝叶斯 进化策略 这些算法每一个都是经过多年考验的，有的甚至几十年。 再拿计算机视觉来说：\nR-CNN系列 yolo系列 mobilenet \u0026hellip;. 目前大多数机器视觉算法工程师所做的工作基本也是围绕这些成熟的算法做微调，结合特定场景做迁移。**所以，作为算法工程师是不会创造新算法，作为算法工程师日常工作内容无非是根据具体的业务场景，根据自己的知识积累拿出几种成熟、好用的算法提出一个可用的解决方案，去解决业务上的问题。**所以很多答主所说的需要这样那样的知识，我觉得有点夸张了。\n有些答主说需要深入的数学知识，把算法说的神乎其神，作为本硕均为数学系的学生来说，对于日常算法，本科阶段的通识数学知识足够使用了，数学很有价值，但是过于脱离实际，实现难度也比较大，吴恩达在他的教学视频曾提到过共轭梯度法，吴恩达说“共轭梯度法效果不错，但是实现过程复杂，所以人工智能中很少被使用。”共轭梯度法在数学里面已经算是比较基础得了。\n算法工程师需要哪些能力？ 业务学习能力 算法工程师是不可能脱离业务背景的，人工智能算法工程师、交通算法工程师、图像处理算法工程师等等。 在针对一个业务场景设计一个合理的算法，业务知识是非常重要的，**需要结合业务的实际情况、限定条件、各种专业词汇和知识都要有一定的了解，如果脱离场景而一味地琢磨算法，效果不会太好，**比如，做交通算法，需要对交通组织、交通管理、通行损失、周期延误等有所认知。比如，做图像处理，需要对各种图像去噪、图像增广、图像分割、物理成像有所了解，知道像素底层是怎么回事。\n持续学习能力 就像我前面所说的**，算法工程师的主要工作就是拿着现有成熟的算法，结合面临业务场景去做一个合理的方案，如果我们知识面太窄，那显然当用到的时候会有点拮据，眼界也被限制住，不知道还有没有更好效果的算法、目前算法有哪些不足之处、在这个业务中能不能发挥作用，只有持续学习，了解足够多的知识，当我们面临问题的时候能够快速对比、选择，找出最合适的一种算法。**\n灵活的思维 当我们选择一种算法去解决一个问题时，效果肯定无法达到我们预期的那样，比如我们拿mask rcnn做医学图像语义分割，我们看着它在自然图像方面表现效果很好，就拿来用于医学图像，但是医学图像有它的难点和特殊性，当跑出效果时会发现结果不如人意，这时候就需要灵活的思维去发现问题，去调优、改进，或者从数据入手或者从网络模型入手或者从超参数入手。\n编程能力 不同公司对于算法工程师的定位有所差别，比如有些朋友在某公司算法工程师只负责方案的设计，开发由专门的开发人员实施。有的公司算法工程师要完成算法设计到开发全部工作。我认为无论是哪一种形式，编程能力都是必要的，就算是前者这样的形式，有专门的开发人员，那在算法的设计过程中需要验证、对比，对每一个小模块算法进行指标评价，你不可能事事都找别人来帮你做，这样效率低，而且开展工作困难。\n算法验证能力 就像前面提到的那样，**算法验证在算法工程师日常工作中占据很大的比重，**我们拿到一些成熟、优秀的算法后，它的效果如何？能否起作用是未知的，我们需要对它进行验证，包括效率、精度等方面。这就要求算法工程师拥有算法验证能力，能够在众多算法中挑选出一种合适的算法来解决相应问题。\n","permalink":"https://lyapple2008.github.io/posts/201904/2019-04-07-%E7%AE%97%E6%B3%95%E5%B7%A5%E7%A8%8B%E5%B8%88%E5%88%B0%E5%BA%95%E6%98%AF%E5%81%9A%E4%BB%80%E4%B9%88%E7%9A%84/","summary":"\u003cp\u003e还记得刚毕业那会找工作，那是一定要找法工程师这个岗位，当时就只是觉得这个岗位牛逼是公司的核心岗位，但是可能对于算法工程师与一般的软件工程有什么区别其实心理也说不清楚。作为一个到今年6月份就工作满5年了的老菜鸟，这期间工作的title也是算法工程师（虽然有些并不是真的在做算法），反正在以算法工程师的title在企业工作的感受与毕业那会想像中上的算法工程师的工作完全不一样。\u003c/p\u003e","title":"算法工程师到底是做什么的"},{"content":"音乐人声分离概况 音乐人声分离的目的是，从一首歌曲中分离出人声和伴奏声。Project on Music/Voice Separation这个网站比较了一些基于传统方法的效果，但是由于基于了一些假设（比如REPET就假伴奏都是周期重复信号），这些算法在实际测试过程中效果都差强人意。随着深度学习的流行，音乐人声分离这个领域也开始被基于深度学习的方法所占领。在SiSEC MUS上可以看到效果比较好的都是基于深度学习的方法。\n基于深度学习方法的处理框架 - 训练数据 以DSD100数据集为例，下面是DSD100数据集的目录结构 ``` |--DSD100 |--Mixtures |--Dev |--051 - AM Contra - Heart Peripheral |--mixture.wav ==\u003e 混合音频 |--Test |--Sources |--Dev |--051 - AM Contra - Heart Peripheral |--bass.wav ==\u003e 各成分音频 |--drums.wav |--other.wav |--vocals.wav ==\u003e 人声部分音频 |--Test ``` - 训练流程 训练目标\n在本任务中的训练目标是对应输入混合音频幅度谱的一个Mask，通过这个Mask与混合音频幅度谱做mask乘法，就可以得到目标音频的幅度谱，再结合混合音频相位谱，做ISTFT就可以得到目标音频。下面是本任务的损失函数，其中X代表混合音频幅度谱，Y代表目标音频幅度谱，f(X,\u0026amp;)代表模型输出的Mask。 Idea Binary Mask\n理想二值掩蔽”（Ideal Binary Mask）中的分离任务就成为了一个二分类问题。这类方法根据听觉感知特性，把音频信号分成不同的子带，根据每个时频单元上的信噪比，把对应的时频单元的能量设为0（噪音占主导的情况下）或者保持原样（目标语音占主导的情况下）。\nIdea Ratio Mask\nIRM（Ideal Ratio Mask），它同样对每个时频单元进行计算，但不同于IBM的“非零即一”，IRM中会计算语音信号和噪音之间的能量比，得到介于0到1之间的一个数，然后据此改变时频单元的能量大小。IRM是对IBM的演进，反映了各个时频单元上对噪声的抑制程度，可以进一步提高分离后语音的质量和可懂度。\nDeep U-Net方法介绍 ``` 卷积层参数个数 = (kernel_w * kernel_h + 1) * output_channel 归一化层参数个数 = 4 * input_channel ; (gamma, beta, moving_mean, moving_variance) ``` Encode-Decode结构\nU-Net中使用的Encode-Decode结构有点类似于图像中的多尺度金字塔，在高分辨率图像中更多关注的是图像的细节纹理，而在低分辨率图像中则更多关注的是图像的轮廓信息，这样就可以对不同层级的信息进行分开建模。 跳跃连接（Skip-connection）\nskip-connection使得每个节点在进行建模时，能够同时利用到本层的信息和来自下一层的信息 相关尝试 More Skip-Connection UNet++网络结构将Skip-Connection发挥到了极致，每个节点不单单只跟同层相邻节点连接，而是跟同层所有节点进行相连。同时一个大的UNet++网络结构其内部又可以一层层拆分成小的UNet++网络结构，这个特点使得其在做剪枝优化的时候非常简单方便，如图UNet++ L4到UNet++ L1就是简化过程。\n由于UNet++相对于UNet结构增加了大概20%的参数，而训练数据并没有相应增强，最后的效果还没达到UNet的结果。\n结合相位信息 Deep Complex Network 实验验证相位在分离任务中的作用 方法 效果（1-10分，分数越高效果越好） 理想结果 10 基于幅度谱方法的结果 8 基于幅度谱和相位谱方法的结果 7 忽略相位的结果 0 基于幅度谱方法的理想结果 8 相位对于音源分类任务的影响极小，更准确的幅度谱估计才能提升效果，相位沿用混合音频的相位就好。\n一些可能的方向 WAVE U-Net直接在时域上进行分离 WAVE U-Net分离的结果中伴奏都很弱\nMulti-Scale Multi-band 借鉴图像分割领域的一些想法和进展\n对音频信号的短时幅度谱进行分割与对图像进行语义分割一个很大的不同在于，音频信号在幅度谱上是有位置差异的，也就是说同样形状的的信号在不同位置，它代表就是不同的音频内容；而在图像语义分割中，分割的对象是没有位置这个信息的，分割对象是可以存在于图像中不同的位置。 公开的数据集 名称 数据量 时长 年份 链接 DSD100 100首 6h58m20s 2015 https://sigsep.github.io/datasets/dsd100.html ccMixter 50首 3h12m30s 2014 https://members.loria.fr/ALiutkus/kam/ MUSDB18 150首 9h50m 2017 https://sigsep.github.io/datasets/musdb.html 参考 [1] Project on Music/Voice Separation: 一些传统方法\n[2] SigSep：关于音乐人声分离的公开数据集和开源实现方法 [3] Paper: Singing Voice Separation With Deep U-Net Convolution Networks\n[4] Paper: UNet++: A Nested U-Net Architecture for Medical Image Segmentation\n[5] Github: UNet-VocalSeparation-Chainer\n[6] Github: UNetPlusPlus\n[7] Github: Wave-U-Net [8] Github: Deep-Complex-Networks [9] Paper: Multi-Scale Multi-Band Densenets For Audio Source Separation [10] SigSep\u0026ndash;Tutorial\u0026ndash;\u0026ldquo;Music Source Separation with DNNs, Making it work\u0026rdquo;\n[11] 搜狗研究员讲解基于深度学习的语音分离\n[12] Paper: Phase-Aware Speech Enhancement With Deep Complex U-Net [13] SiSEC MUS: 音乐人声分离竞赛\n[14] Project on Music/Voice Separation: 一些传统方法\n","permalink":"https://lyapple2008.github.io/posts/201903/2019-03-31-%E9%9F%B3%E4%B9%90%E4%BA%BA%E5%A3%B0%E5%88%86%E7%A6%BB%E5%B0%8F%E7%BB%93/","summary":"\u003ch3 id=\"音乐人声分离概况\"\u003e音乐人声分离概况\u003c/h3\u003e\n\u003cp\u003e音乐人声分离的目的是，从一首歌曲中分离出人声和伴奏声。\u003ca href=\"https://www.math.ucdavis.edu/~aberrian/research/voice_separation/index.html\"\u003eProject on Music/Voice Separation\u003c/a\u003e这个网站比较了一些基于传统方法的效果，但是由于基于了一些假设（比如REPET就假伴奏都是周期重复信号），这些算法在实际测试过程中效果都差强人意。随着深度学习的流行，音乐人声分离这个领域也开始被基于深度学习的方法所占领。在\u003ca href=\"http://www.sisec17.audiolabs-erlangen.de/#/results/1/4/2\"\u003eSiSEC MUS\u003c/a\u003e上可以看到效果比较好的都是基于深度学习的方法。\u003c/p\u003e","title":"音乐人声分离小结"},{"content":"记录一些音频开发过程中会用到的优质资源\nWebRTC WebRTC是google开源的实时音视频通讯项目，其中的AudioProcess模块包括了AGC/AEC/ANS算法，非常值得学习。同时WebRTC还是一个跨平台项目，代码中对各个平台（Android/Ios/Windows/Linux）硬件接口的封装和抽象可以直接拿来应用到自己的项目中。\nOboe Oboe是Google家开源一个高性能C++库，这个库封装了Android底层OpenSLES和AAudio接口，通过这个库可以方便地在Android实现Low Latency Audio，只需一套代码就可以了。\n","permalink":"https://lyapple2008.github.io/posts/201903/2019-03-10-%E9%9F%B3%E9%A2%91%E5%BC%80%E5%8F%91%E8%B5%84%E6%BA%90/","summary":"\u003cp\u003e记录一些音频开发过程中会用到的优质资源\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://webrtc.org/\"\u003eWebRTC\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eWebRTC是google开源的实时音视频通讯项目，其中的AudioProcess模块包括了AGC/AEC/ANS算法，非常值得学习。同时WebRTC还是一个跨平台项目，代码中对各个平台（Android/Ios/Windows/Linux）硬件接口的封装和抽象可以直接拿来应用到自己的项目中。\u003c/p\u003e\n\u003c/blockquote\u003e\n\u003cul\u003e\n\u003cli\u003e\u003ca href=\"https://github.com/google/oboe\"\u003eOboe\u003c/a\u003e\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003eOboe是Google家开源一个高性能C++库，这个库封装了Android底层OpenSLES和AAudio接口，通过这个库可以方便地在Android实现Low Latency Audio，只需一套代码就可以了。\u003c/p\u003e\n\u003c/blockquote\u003e","title":"音频开发资源"},{"content":"最近由于公司项目的原因开始接触WebRTC，其中Android相关部分由于需要跨越了两种不同的语 言，因此需要一种机制能够让C/C++和JAVA之间进行交互，而JNI就是这样一种机制。通过JNI可 以实现C/C++和JAVA之前需要交互。本篇笔记的首先从一个实际的例子开始介绍JNI操作的完整流 程是怎样的；接着将就这个例子完整介绍JNI中需要注意的点。 JNI操作完整流程介绍 编写Java类及声明Native方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 public class Java2C { static { System.loadLibrary(\u0026#34;jni\u0026#34;); } // 成员变量 private int number = 88; private static double speed = 55.66; private String message = \u0026#34;Hello from Java.\u0026#34;; // 基本数据类型 private native double average(int n1, int n2); // 引用类型 private native String sayHello(String msg); // 基本数据类型数组 private native double[] sumAndAverage(int[] numbers); // 引用类型数组 private native String[] num2Str(int[] numbers); // 操作java成员变量 private native void modifyJavaVariable(); // java成员方法 private void callback(String message) { System.out.println(\u0026#34;In Java with \u0026#34; + message); } private double callbackAverage(int n1, int n2) { return ((double)n1 + n2) / 2.0; } // 静态成员方法 private static String callbackStatic() { return \u0026#34;From static Java method.\u0026#34;; } private native void testCallbackMethod(); public static void main(String[] args) { Java2C javaClass = new Java2C(); double aver = javaClass.average(5, 6); System.out.println(\u0026#34;primitive type: the average of 5 and 6 is \u0026#34; + aver); System.out.println(\u0026#34;Reference type: \u0026#34;); String javaString = javaClass.sayHello(\u0026#34;Hello From Java\u0026#34;); System.out.println(javaString); System.out.println(\u0026#34;primitive type array: \u0026#34;); int[] numbers = {11, 22, 33}; double[] results = javaClass.sumAndAverage(numbers); System.out.println(\u0026#34;In Java, the sum is \u0026#34; + results[0]); System.out.println(\u0026#34;In Java, the average is \u0026#34; + results[1]); System.out.println(\u0026#34;reference type array: \u0026#34;); String[] numStrs = javaClass.num2Str(numbers); System.out.println(\u0026#34;In Java the string is \u0026#34; + numStrs[0] + \u0026#34; \u0026#34; + numStrs[1] + \u0026#34; \u0026#34; + numStrs[2]); System.out.println(\u0026#34;Operate java variable: \u0026#34;); javaClass.modifyJavaVariable(); System.out.println(\u0026#34;C/C++　call java viarable and method\u0026#34;); javaClass.testCallbackMethod(); } } 生成jni native头文件 javac -h \u0026lt;头文件存放路径\u0026gt; \u0026lt;java类\u0026gt;\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 /* DO NOT EDIT THIS FILE - it is machine generated */ #include \u0026lt;jni.h\u0026gt; /* Header for class Java2C */ #ifndef _Included_Java2C #define _Included_Java2C #ifdef __cplusplus extern \u0026#34;C\u0026#34; { #endif /* * Class: Java2C * Method: average * Signature: (II)D */ JNIEXPORT jdouble JNICALL Java_Java2C_average (JNIEnv *, jobject, jint, jint); /* * Class: Java2C * Method: sayHello * Signature: (Ljava/lang/String;)Ljava/lang/String; */ JNIEXPORT jstring JNICALL Java_Java2C_sayHello (JNIEnv *, jobject, jstring); /* * Class: Java2C * Method: sumAndAverage * Signature: ([I)[D */ JNIEXPORT jdoubleArray JNICALL Java_Java2C_sumAndAverage (JNIEnv *, jobject, jintArray); /* * Class: Java2C * Method: num2Str * Signature: ([I)[Ljava/lang/String; */ JNIEXPORT jobjectArray JNICALL Java_Java2C_num2Str (JNIEnv *, jobject, jintArray); /* * Class: Java2C * Method: modifyJavaVariable * Signature: ()V */ JNIEXPORT void JNICALL Java_Java2C_modifyJavaVariable (JNIEnv *, jobject); /* * Class: Java2C * Method: testCallbackMethod * Signature: ()V */ JNIEXPORT void JNICALL Java_Java2C_testCallbackMethod (JNIEnv *, jobject); #ifdef __cplusplus } #endif #endif 在C文件中编写相关jni native方法的实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 #include \u0026lt;jni.h\u0026gt; #include \u0026lt;stdio.h\u0026gt; #include \u0026#34;Java2C.h\u0026#34; JNIEXPORT jdouble JNICALL Java_Java2C_average (JNIEnv *env, jobject thisObj, jint n1, jint n2) { jdouble result; printf(\u0026#34;In C, the numbers are %d and %d\\n\u0026#34;, n1, n2); result = ((jdouble)n1 + n2) / 2.0; return result; } JNIEXPORT jstring JNICALL Java_Java2C_sayHello (JNIEnv *env, jobject thisObj, jstring inJNIStr) { const char *inCStr = (*env)-\u0026gt;GetStringUTFChars(env, inJNIStr, NULL); if (NULL == inCStr) return NULL; printf(\u0026#34;In C, the receiving string is: %s\\n\u0026#34;, inCStr); (*env)-\u0026gt;ReleaseStringUTFChars(env, inJNIStr, inCStr); char outCStr[128] = \u0026#34;Out string from C\u0026#34;; return (*env)-\u0026gt;NewStringUTF(env, outCStr); } JNIEXPORT jdoubleArray JNICALL Java_Java2C_sumAndAverage (JNIEnv *env, jobject thisObj, jintArray inJNIArray) { jint *inCArray = (*env)-\u0026gt;GetIntArrayElements(env, inJNIArray, NULL); if (NULL == inCArray) return NULL; jsize length = (*env)-\u0026gt;GetArrayLength(env, inJNIArray); jint sum = 0; int i; for (i = 0;i\u0026lt;length;i++) { sum += inCArray[i]; } jdouble average = (jdouble)sum / length; (*env)-\u0026gt;ReleaseIntArrayElements(env, inJNIArray, inCArray, 0); jdouble outArray[] = {sum, average}; jdoubleArray outJNIArray = (*env)-\u0026gt;NewDoubleArray(env, 2); if (NULL == outJNIArray) return NULL; (*env)-\u0026gt;SetDoubleArrayRegion(env, outJNIArray, 0, 2, outArray); return outJNIArray; } JNIEXPORT jobjectArray JNICALL Java_Java2C_num2Str (JNIEnv *env, jobject thisObj, jintArray inJNIArray) { jobjectArray ret; jint *inCArray = (*env)-\u0026gt;GetIntArrayElements(env, inJNIArray, NULL); if (NULL == inCArray) return NULL; jsize length = (*env)-\u0026gt;GetArrayLength(env, inJNIArray); char cStrArray[6][128]; int i; for (i = 0;i \u0026lt; length; i++) { snprintf(cStrArray[i], 128, \u0026#34;%d\u0026#34;, inCArray[i]); } jclass strClass = (*env)-\u0026gt;FindClass(env, \u0026#34;java/lang/String\u0026#34;); ret = (*env)-\u0026gt;NewObjectArray(env, length, strClass, (*env)-\u0026gt;NewStringUTF(env, \u0026#34;\u0026#34;)); for (i=0;i\u0026lt;length;i++) { (*env)-\u0026gt;SetObjectArrayElement(env, ret, i, (*env)-\u0026gt;NewStringUTF(env, cStrArray[i])); } return ret; } JNIEXPORT void JNICALL Java_Java2C_modifyJavaVariable (JNIEnv *env, jobject thisObj) { jclass thisClass = (*env)-\u0026gt;GetObjectClass(env, thisObj); jfieldID fidNumber = (*env)-\u0026gt;GetFieldID(env, thisClass, \u0026#34;number\u0026#34;, \u0026#34;I\u0026#34;); if (NULL == fidNumber) return; jint number = (*env)-\u0026gt;GetIntField(env, thisObj, fidNumber); printf(\u0026#34;In C, thi int is %d\\n\u0026#34;, number); number = 99; (*env)-\u0026gt;SetIntField(env, thisObj, fidNumber, number); jfieldID fidMessage = (*env)-\u0026gt;GetFieldID(env, thisClass, \u0026#34;message\u0026#34;, \u0026#34;Ljava/lang/String;\u0026#34;); if (NULL == fidMessage) return ; jstring message = (*env)-\u0026gt;GetObjectField(env, thisObj, fidMessage); const char* cStr = (*env)-\u0026gt;GetStringUTFChars(env, message, NULL); if (NULL == cStr) return ; printf(\u0026#34;In C, the string is %s\\n\u0026#34;, cStr); (*env)-\u0026gt;ReleaseStringUTFChars(env, message, cStr); message = (*env)-\u0026gt;NewStringUTF(env, \u0026#34;Hello from C\u0026#34;); if (NULL == message) return ; (*env)-\u0026gt;SetObjectField(env, thisObj, fidMessage, message); jfieldID fidSpeed = (*env)-\u0026gt;GetStaticFieldID(env, thisClass, \u0026#34;speed\u0026#34;, \u0026#34;D\u0026#34;); if (NULL == fidSpeed) return ; jdouble speed = (*env)-\u0026gt;GetStaticDoubleField(env, thisClass, fidSpeed); printf(\u0026#34;In C, the speed is %f\\n\u0026#34;, speed); speed = 77.99; (*env)-\u0026gt;SetStaticDoubleField(env, thisClass, fidSpeed, speed); } JNIEXPORT void JNICALL Java_Java2C_testCallbackMethod (JNIEnv *env, jobject thisObj) { jclass thisClass = (*env)-\u0026gt;GetObjectClass(env, thisObj); jmethodID midCallback = (*env)-\u0026gt;GetMethodID(env, thisClass, \u0026#34;callback\u0026#34;, \u0026#34;(Ljava/lang/String;)V\u0026#34;); if (NULL == midCallback) return ; printf(\u0026#34;In C, call back Java\u0026#39;s callback(String)\\n\u0026#34;); jstring message = (*env)-\u0026gt;NewStringUTF(env, \u0026#34;Hello from C\u0026#34;); (*env)-\u0026gt;CallVoidMethod(env, thisObj, midCallback, message); jmethodID midCallbackAverage = (*env)-\u0026gt;GetMethodID(env, thisClass, \u0026#34;callbackAverage\u0026#34;, \u0026#34;(II)D\u0026#34;); if (NULL == midCallbackAverage) return ; printf(\u0026#34;In C, call back Java\u0026#39;s callbackAverage\\n\u0026#34;); jdouble average = (*env)-\u0026gt;CallDoubleMethod(env, thisObj, midCallbackAverage, 2, 3); printf(\u0026#34;In C, the average is %f\\n\u0026#34;, average); jmethodID midCallbackStatic = (*env)-\u0026gt;GetStaticMethodID(env, thisClass, \u0026#34;callbackStatic\u0026#34;, \u0026#34;()Ljava/lang/String;\u0026#34;); if (NULL == midCallbackStatic) { printf(\u0026#34;1111\\n\u0026#34;); return ; } jstring resultJNIStr = (*env)-\u0026gt;CallStaticObjectMethod(env, thisClass, midCallbackStatic); if (NULL == resultJNIStr) return ; const char* resultCStr = (*env)-\u0026gt;GetStringUTFChars(env, resultJNIStr, NULL); if (NULL == resultCStr) return; printf(\u0026#34;In C, the returned string is %s\\n\u0026#34;, resultCStr); } 通过gcc/g++ 生成动态库 gcc -fPIC -I\u0026quot;$JAVA_HOME/include\u0026quot; -I\u0026quot;$JAVA_HOME/include/linux\u0026quot; -shared -o libjni.so Java2C.c\n运行进行测试 java -Djava.library.path=. Java2C 这里需要使用-D指定前面生的动态库路径，否则运行的时候会提示找不到动态库的错误\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 In C, the numbers are 5 and 6 primitive type: the average of 5 and 6 is 5.5 Reference type: In C, the receiving string is: Hello From Java Out string from C primitive type array: In Java, the sum is 66.0 In Java, the average is 22.0 reference type array: In Java the string is 11 22 33 Operate java variable: In C, thi int is 88 In C, the string is Hello from Java. In C, the speed is 55.660000 C/C++　call java viarable and method In C, call back Java\u0026#39;s callback(String) In Java with Hello from C In C, call back Java\u0026#39;s callbackAverage In C, the average is 2.500000 In C, the returned string is From static Java method. JNI中类型的对应关系与转换 基于数据类型 Java类型 Native Type 描述 boolean jboolean C/C++无符号的8位整型（unsigned char） byte jbyte C/C++带符号的8位整型（char） char jchar C/C++无符号的16位整型（unsigned short） short jshort C/C++带符号的16位整型 （signed short) int jint C/C++带符号的32位整型（int） long jlong C/C++带符号的64位整型（long） float jfloat C/C++32位浮点型（float） double jdouble C/C++64位浮点型（double） 引用数据类型 Java类型 Native Type 描述 Object jobject 任何Java对象，或者没有对应java类型的对象 Class jclass Class类对象 String jstring 字符串对象 Object[] jobjectArray 任何对象的数组 boolean[] jbooleanArray 布尔型数组 byte[] jbyteArray 比特型数组 char[] jcharArray 字符型数组 short[] jshortArray 短整型数组 int[] jintArray 整型数组 long[] jlongArray 长整型数组 float[] jfloatArray 单精度浮点型数组 double[] jdouble 双精度浮点型数组 void void n/a 引用类型的继承关系\n注意 基本数据类型可以在native层直接使用 引用数据类型则不能直接使用，需要根据JNI函数进行相应的转换才能使用 多维数据（包括二维数组）都是引用类型，需要使用jobjectArray类型存取其值 JNI相关操作方法 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 // native string方法 // UTF-8 String (encoded to 1-3 byte, backward compatible with 7-bit ASCII) // Can be mapped to null-terminated char-array C-string const char * GetStringUTFChars(JNIEnv *env, jstring string, jboolean *isCopy); // Returns a pointer to an array of bytes representing the string in modified UTF-8 encoding. void ReleaseStringUTFChars(JNIEnv *env, jstring string, const char *utf); // Informs the VM that the native code no longer needs access to utf. jstring NewStringUTF(JNIEnv *env, const char *bytes); // Constructs a new java.lang.String object from an array of characters in modified UTF-8 encoding. jsize GetStringUTFLength(JNIEnv *env, jstring string); // Returns the length in bytes of the modified UTF-8 representation of a string. void GetStringUTFRegion(JNIEnv *env, jstring str, jsize start, jsize length, char *buf); // Translates len number of Unicode characters beginning at offset start into modified UTF-8 encoding // and place the result in the given buffer buf. // Unicode Strings (16-bit character) const jchar * GetStringChars(JNIEnv *env, jstring string, jboolean *isCopy); // Returns a pointer to the array of Unicode characters void ReleaseStringChars(JNIEnv *env, jstring string, const jchar *chars); // Informs the VM that the native code no longer needs access to chars. jstring NewString(JNIEnv *env, const jchar *unicodeChars, jsize length); // Constructs a new java.lang.String object from an array of Unicode characters. jsize GetStringLength(JNIEnv *env, jstring string); // Returns the length (the count of Unicode characters) of a Java string. void GetStringRegion(JNIEnv *env, jstring str, jsize start, jsize length, jchar *buf); // Copies len number of Unicode characters beginning at offset start to the given buffer buf 1 2 3 4 5 6 7 8 9 10 11 // JNI Primitive Array Function // ArrayType: jintArray, jbyteArray, jshortArray, jlongArray, jfloatArray, jdoubleArray, jcharArray, jbooleanArray // PrimitiveType: int, byte, short, long, float, double, char, boolean // NativeType: jint, jbyte, jshort, jlong, jfloat, jdouble, jchar, jboolean NativeType * Get\u0026lt;PrimitiveType\u0026gt;ArrayElements(JNIEnv *env, ArrayType array, jboolean *isCopy); void Release\u0026lt;PrimitiveType\u0026gt;ArrayElements(JNIEnv *env, ArrayType array, NativeType *elems, jint mode); void Get\u0026lt;PrimitiveType\u0026gt;ArrayRegion(JNIEnv *env, ArrayType array, jsize start, jsize length, NativeType *buffer); void Set\u0026lt;PrimitiveType\u0026gt;ArrayRegion(JNIEnv *env, ArrayType array, jsize start, jsize length, const NativeType *buffer); ArrayType New\u0026lt;PrimitiveType\u0026gt;Array(JNIEnv *env, jsize length); void * GetPrimitiveArrayCritical(JNIEnv *env, jarray array, jboolean *isCopy); void ReleasePrimitiveArrayCritical(JNIEnv *env, jarray array, void *carray, jint mode); C/C++调用Java类中的成员变量 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 // JNI操作成员变量变量 jclass GetObjectClass(JNIEnv *env, jobject obj); // Returns the class of an object. jfieldID GetFieldID(JNIEnv *env, jclass cls, const char *name, const char *sig); // Returns the field ID for an instance variable of a class. NativeType Get\u0026lt;type\u0026gt;Field(JNIEnv *env, jobject obj, jfieldID fieldID); void Set\u0026lt;type\u0026gt;Field(JNIEnv *env, jobject obj, jfieldID fieldID, NativeType value); // Get/Set the value of an instance variable of an object // \u0026lt;type\u0026gt; includes each of the eight primitive types plus Object. jfieldID GetStaticFieldID(JNIEnv *env, jclass cls, const char *name, const char *sig); // Returns the field ID for a static variable of a class. NativeType GetStatic\u0026lt;type\u0026gt;Field(JNIEnv *env, jclass clazz, jfieldID fieldID); void SetStatic\u0026lt;type\u0026gt;Field(JNIEnv *env, jclass clazz, jfieldID fieldID, NativeType value); // Get/Set the value of a static variable of a class. // \u0026lt;type\u0026gt; includes each of the eight primitive types plus Object. C/C++调用Java类中的成员方法 通过下面这条命令可以获取到java中方法的签名\njavas -s -p Java2C\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 Compiled from \u0026#34;Java2C.java\u0026#34; public class Java2C { private int number; descriptor: I private static double speed; descriptor: D private java.lang.String message; descriptor: Ljava/lang/String; public Java2C(); descriptor: ()V private native double average(int, int); descriptor: (II)D private native java.lang.String sayHello(java.lang.String); descriptor: (Ljava/lang/String;)Ljava/lang/String; private native double[] sumAndAverage(int[]); descriptor: ([I)[D private native java.lang.String[] num2Str(int[]); descriptor: ([I)[Ljava/lang/String; private native void modifyJavaVariable(); descriptor: ()V private void callback(java.lang.String); descriptor: (Ljava/lang/String;)V private double callbackAverage(int, int); descriptor: (II)D private static java.lang.String callbackStatic(); descriptor: ()Ljava/lang/String; private native void testCallbackMethod(); descriptor: ()V public static void main(java.lang.String[]); descriptor: ([Ljava/lang/String;)V static {}; descriptor: ()V } 再通过下面这些JNI方法变可以在C/C++代码中调用到JAVA类的成员方法了\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 jmethodID GetMethodID(JNIEnv *env, jclass cls, const char *name, const char *sig); // Returns the method ID for an instance method of a class or interface. NativeType Call\u0026lt;type\u0026gt;Method(JNIEnv *env, jobject obj, jmethodID methodID, ...); NativeType Call\u0026lt;type\u0026gt;MethodA(JNIEnv *env, jobject obj, jmethodID methodID, const jvalue *args); NativeType Call\u0026lt;type\u0026gt;MethodV(JNIEnv *env, jobject obj, jmethodID methodID, va_list args); // Invoke an instance method of the object. // The \u0026lt;type\u0026gt; includes each of the eight primitive and Object. jmethodID GetStaticMethodID(JNIEnv *env, jclass cls, const char *name, const char *sig); // Returns the method ID for an instance method of a class or interface. NativeType CallStatic\u0026lt;type\u0026gt;Method(JNIEnv *env, jclass clazz, jmethodID methodID, ...); NativeType CallStatic\u0026lt;type\u0026gt;MethodA(JNIEnv *env, jclass clazz, jmethodID methodID, const jvalue *args); NativeType CallStatic\u0026lt;type\u0026gt;MethodV(JNIEnv *env, jclass clazz, jmethodID methodID, va_list args); // Invoke an instance method of the object. // The \u0026lt;type\u0026gt; includes each of the eight primitive and Object. 参考 [1] Java Programming Tutorial Java Native Interface (JNI)\n","permalink":"https://lyapple2008.github.io/posts/201903/2019-03-03-javanativeinterface%E7%AC%94%E8%AE%B0/","summary":"\u003cp\u003e最近由于公司项目的原因开始接触WebRTC，其中Android相关部分由于需要跨越了两种不同的语\n言，因此需要一种机制能够让C/C++和JAVA之间进行交互，而JNI就是这样一种机制。通过JNI可\n以实现C/C++和JAVA之前需要交互。本篇笔记的首先从一个实际的例子开始介绍JNI操作的完整流\n程是怎样的；接着将就这个例子完整介绍JNI中需要注意的点。\n\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://user-images.githubusercontent.com/3350865/61180103-91c2e780-a643-11e9-8132-f85c7a83f8da.jpg\"\u003e\n\u003cimg src=\"/images/2019-03-03-001.jpg\"\u003e\u003c/p\u003e","title":"JavaNativeInterface笔记"},{"content":"2018回顾 先看下2018初始的时候自己立下的flag吧，看看脸被打得有多痛\n咱们一个脸一个脸地来打，轻拍：\n健身 最近一次记录的体重是64.3Kg，距离55Kg还是有很大距离，初始体重是65.9Kg 这一项完成度是14.68%\n跑里程达到400Km，这里有记录的数据是8.20Km 太可怜了，我不算了，直接写**0%**好了\n小爱好 现在连一首\u0026laquo;纸短情长\u0026raquo;都还没练会，太笨了。。。。 完成度是0%\n今年书是看了《小岛经济学》《九败一胜：美团创始人王兴》《跃迁：成为高手的技术》 这个完成度也算是0%，没有输出的阅读，相当于是无效的努力\n技术 音视频应用开发：这个也没有继续跟进下去，然后最后还是逃不掉\n深度学习方面：由于新工作的原因，开始接触实际项目，算是个入门吧\n写博客 2018一共写了7篇文章，也是一个远不及格的成绩，没有输出相当于没有成长。\n2019新的Flag 虽然脸已经被打肿了，但是flag还是不能倒呀，2019-猪年-新的flag还是要立起来。通过回顾2018年flag的完成情况，可以深刻的把自己定位为一个执行力弱不爆的人士了。作为一个立志成为海贼王的男人，怎么可以这样呢，2019必须成长了呀，才能成为海贼王的男人。在成为海贼王的男人前，先把2019年的flag完成好吗，脸真的痛好吗\n健身 体重降到58Kg 小爱好 录制《纸短情长ukelele》的小视频 非技术读书笔记2篇 海贼王系列视频特效准备 技术 WebRTC音视频通讯开发 输出一篇移动端布署深度学习应用的文章 写博客 每月一篇博客，不限主题 ","permalink":"https://lyapple2008.github.io/posts/201901/2019-01-26-2019%E5%B0%8F%E7%9B%AE%E6%A0%87/","summary":"\u003ch1 id=\"2018回顾\"\u003e2018回顾\u003c/h1\u003e\n\u003cp\u003e先看下2018初始的时候自己立下的flag吧，看看脸被打得有多痛\u003c/p\u003e\n\u003cp\u003e\u003cimg alt=\"image\" loading=\"lazy\" src=\"https://user-images.githubusercontent.com/3350865/61180087-427cb700-a643-11e9-8bfd-354561f9d0e2.jpg\"\u003e\n\u003cimg src=\"/images/2019-01-26-001.jpg\" /\u003e\u003c/p\u003e","title":"2019小目标--重新开始打脸之路"},{"content":" 收集一些音频相关的公司和实验室，另外还有一些个人博客，用来了解行业的最新动态。\n技术公司 smule 一些乐器应用挺有意思的 SoundHound 哼唱歌曲识别 Vocaloid 歌声合成 Gracenote 实验室 Interactive audio lab Music Technology Group Neural Acoustic Processing Lab Music and Audio Research Group, Seoul National University 个人博客 Melody Extraction Yun Wang (Maigo) 开源库 Essentia ","permalink":"https://lyapple2008.github.io/posts/201810/2018-10-29-%E6%94%B6%E9%9B%86%E9%9F%B3%E9%A2%91%E6%8A%80%E6%9C%AF%E5%BA%94%E7%94%A8%E5%85%AC%E5%8F%B8%E4%B8%8E%E5%AE%9E%E9%AA%8C%E5%AE%A4/","summary":"\u003c!-- ![](https://user-images.githubusercontent.com/3350865/61180076-fe89b200-a642-11e9-81a7-697446a0860a.jpg)    --\u003e\n\u003cp\u003e\u003cimg alt=\"Image Caption\" loading=\"lazy\" src=\"/images/2018-10-29-001.jpg\"\u003e\u003c/p\u003e\n\u003cp\u003e  收集一些音频相关的公司和实验室，另外还有一些个人博客，用来了解行业的最新动态。\u003c/p\u003e","title":"收集音频技术应用公司与实验室"},{"content":" 今天又一次重新开始健身计划，这次到这里来做个记录，看看能坚持多久！为了更好的身体，为了腹肌，加油，少年，你是要成为海贼王的男人！！！\n器械：瑜伽垫一张 课程：Fit健身 今天买了课程，花了大价钱，达不成目标就要打水漂了！！！\n十月运动记录 日期 体重（Kg） 运动内容 运动时间（分钟） 2018-10-15 65.9 HIIT-畅快流汗 20 2018-10-16 65.0 HIIT-畅快流汗 20 2018-10-17 65.3 HIIT-腹部燃脂 29 2018-10-18 65.1 HIIT-脂肪ByeBye 14 2018-10-19 2018-10-20 2018-10-21 64.7 FIT健身体能测试 10 2018-10-22 65.4 腹肌撕裂者初级\nHIIT-进击的Burpee 25 2018-10-23 65.7 打篮球 60 2018-10-24 65.4 休息恢复 2018-10-25 65.6 HIIT-腹部燃脂 29 2018-10-26 2018-10-27 HIIT-全身活力燃脂 22 2018-10-28 64.1 HIIT-畅快流汗\n入门瑜伽伸展 30 2018-10-29 65.1 核心力量初级训练\nTabata-碎脂机进阶 24 2018-10-30 65.3 打篮球 60 2018-10-26 运动天数 开始体重 结束体重 体重变化（Kg） 13 65.9 65.3 -0.6 十一月运动记录 日期 体重（Kg） 运动内容 运动时间（分钟） 2018-11-1 64.7 腹肌撕裂者初级\nHIIT-减脂巨星 41 2018-11-2 2018-11-3 2018-11-4 2018-11-5 64.6 HIIT-畅快流汗\n核心进阶减脂训练 36 2018-11-6 64.3 强力带全身焕活-减脂初级 49 2018-11-7 2018-11-8 64.7 HIIT-全身活力燃脂\n腹肌撕裂者进阶 40 2018-11-9 2018-11-10 2018-11-11 2018-11-12 64.1 Tabata-碎脂机进阶\n核心进阶减脂训练 29 2018-11-13 64.2 打篮球 60 2018-11-14 2018-11-15 64.6 HIIT-腹部燃脂 28 2018-11-16 2018-11-17 2018-11-18 64.6 十分鐘家中腹肌訓練 10 2018-11-19 64.7 HIIT-进击的Burpee 22 2018-11-20 64.3 打篮球 60 2018-11-21 64.4 HIIT-腹部燃脂 29 2018-11-22 2018-11-23 2018-11-24 2018-11-25 2018-11-26 2018-11-27 63.6 打篮球 60 2018-11-28 63.7 十分鐘家中腹肌訓練 10 2018-11-29 63.8 HIIT-全身活力燃脂 22 运动天数 开始体重 结束体重 体重变化（Kg） 14 65.3 63.8 -1.5 十二月运动记录 日期 体重（Kg） 运动内容 运动时间（分钟） 2018-12-1 2018-12-2 2018-12-3 2018-12-4 64.4 打篮球 60 2018-12-5 63.9 腹肌撕裂者进阶 18 2018-12-6 64.0 HIIT-进击的Burpee 22 2018-12-7 2018-12-8 2018-12-9 2018-12-10 63.6 十分鐘家中腹肌訓練 10 2018-12-11 64.3 打篮球 60 2018-12-12 2018-12-13 2018-12-14 2018-12-15 2018-12-16 2018-12-17 64.3 HIIT-减脂巨星 27 爆开始前的照片 ","permalink":"https://lyapple2008.github.io/posts/201810/2018-10-15-2018%E5%81%A5%E8%BA%AB%E8%AE%B0%E5%BD%95/","summary":"\u003cp\u003e  今天又一次重新开始健身计划，这次到这里来做个记录，看看能坚持多久！为了更好的身体，为了腹肌，加油，少年，你是要成为海贼王的男人！！！\u003c/p\u003e\n\u003cul\u003e\n\u003cli\u003e器械：瑜伽垫一张\u003c/li\u003e\n\u003cli\u003e课程：Fit健身\u003c/li\u003e\n\u003c/ul\u003e\n\u003cblockquote\u003e\n\u003cp\u003e今天买了课程，花了大价钱，达不成目标就要打水漂了！！！\u003c/p\u003e","title":"2018健身记录"},{"content":" 在踩了无数坑之后，才有了这篇文章，现在就一个想法还是Linux系统好呀。编译什么东西都是一个套路，configure/make/make install。之前不知道为什么那么程序员会推崇用mac做开发，编译的这个问题上Mac绝对完胜Windows。\n一、需要下载的工具 Visual Studio 2013 或 Visual Studio 2015 MSYS2 msys2-x86_64 YASM Win64.exe FFmpeg源码 ffmpeg4.0.1 二、编译环境搭建 1. 安装MSYS2，默认安装在C:\\msys64目录，安装完成后打开msys2环境，运行命令安装编译工具 pacman -S make gcc diffutils\n注：安装过程中可能会出现某些包下载失败的情况，这时只需要多试几次就好，直到提示净更新大小为0就说明安装完整了。\n2. 修改C:\\msys64\\msys2_shell.cmd文件，修改下图红框里的部分，这里是为了让msys2能继承VS的环境变量 set MSYS2_PATH_TYPE=inherit\n3. 将C:\\msys64\\usr\\bin\\link.exe重命名为其它名字，如link_bak.exe，这里是为了在链接的时候选择Visual Studio的link.exe工具，避免冲突 4. 将下载的yasm工具，yasm-1.3.0-win32.exe，复制到C:\\msys64\\usr\\bin目录，将重命令为yasm.exe 三、准备编译 1. 打开visual studio命令行，这里要选择vs2015 x64 x86兼容工具提示符，打开方式如下图所示 2. 切换目录到C:\\msys64，运行msy2_shell.cmd，开启msys2环境 3. 测试环境是否正确，检查编译工具和链接工具是不是指向的VS工具 四、开始编译（以下命令均在msys2环境执行） 1. 切换目录到ffmpeg源码目录 2. 运行configure脚本，相关参数参考下面，configure脚本执行的时间会比较长，耐心等待 1 2 3 4 5 6 7 8 9 10 ./configure \\ --prefix=./build_hwaccel \\ ====\u0026gt; 【可选】这里指定编译完成后库的安装目录 --toolchain=msvc \\ ====\u0026gt; 【必须】选择使用VS的工具和环境 --arch=x86 \\ ====\u0026gt; 【必须】x86指定生成win32版本的库 --enable-x86asm \\ ====\u0026gt; 【可选】使能x86的汇编优化 --disable-doc \\ ====\u0026gt; 【可选】关闭ffmpeg文档输出 --enable-shared \\ ====\u0026gt; 【必须】设置输出动态库 --disable-static \\ ====\u0026gt; 【可选】设置关闭输出静态库 --enable-d3d11va \\ ====\u0026gt; 【可选】设置使能d3d11硬件加速 --enable-dxva2 ====\u0026gt; 【可选】设置便能dxva2接口的硬件加速 3. 执行make命令，这个过程可能会有一些错误提示，可以根据提示上网进搜索相关问题， 如编译ffmpeg4.0.1需要注释与变量CC_IDENT相关的语句，存在于cmdutils.c和ff_probe.c文件中\n4. 执行make install命令，将生成的动态库复制到configure时指定的目录中 5. 至此整个windows下的ffmpeg动态库编译完成 ","permalink":"https://lyapple2008.github.io/posts/201808/2018-08-18-windows%E4%B8%8B%E8%BD%BD%E7%BC%96%E8%AF%91ffmpeg%E5%8A%A8%E6%80%81%E5%BA%93%E6%8C%87%E5%8C%97/","summary":"\u003cp\u003e  在踩了无数坑之后，才有了这篇文章，现在就一个想法还是Linux系统好呀。编译什么东西都是一个套路，configure/make/make install。之前不知道为什么那么程序员会推崇用mac做开发，编译的这个问题上Mac绝对完胜Windows。\u003c/p\u003e","title":"Windows下载编译FFmpeg动态库指北"},{"content":" 对DeepLearning最初的印象是，大量的训练样本+机器学习，也就是说原来传统的机器学习会遇到的问题，不能解决的问题，换成DeepLearning同样解决不了。比如目标识别中因为光照变化，目标被遮挡，目标的几何变化造成的识别率大幅下降，在DeepLearning中同样也不能很好解决。但是不是说DeepLearning就一无事处，最近几年这么热也决不是因为名字取得好。DeepLearning比较明显的优势就是在特征选择上，想想之前做生物特征识别时，各种找特征，还得考虑什么光照不变，旋转不变，抗尺寸变换，抗遮挡，那叫一个累呀。现在可好啦，一个Convolution Layer，再配Fully Connected Layer，最后来个Softmax，丢一堆带标签的样本进去自动给你找出特征。当然这个只是一个接触DeepLearning不到一个月的小白的肤浅认识，大家听听就好。\n本文算是最近1个月学习DeepLearning的入门小作业，选择的例子也是DeepLearning最流行的HelloWorld程序MNIST手写数字识别，采用Caffe2进行训练，并在Android端实现一个Demo样例。\nDeepLearning基础 这里推荐下台大李宏毅老师的DeepLearning课程，讲解风趣幽默，生动详细，力荐。B站链接 深度学习框架：caffe2\n模型训练 整个模型训练过程主要包括数据准备、模型建立、模型训练，参考caffe2官网的tutorial\n1. 数据准备 数据准备在MachineLearning类的应用中起到致关重要的作用，相当于煮饭的时候用到的米。再利害的算法，如果没有足够的数据，那也是巧妇难为无米之炊，难道马云会称现在是DT时代。另外DeepLearning作为MachineLearning的一个分支，目前了解到的大部分的DeepLearning算法更多的还是属于SuperviseLearning。SuperviseLearning一个明显的特征是非常依赖数据，而且是人工标注的数据，这也难怪一些DeepLearning大大们说在AI应用中，有多少人工就有多少智能。本文用到的数据链接地址：MNIST手写数字数据集\n2. 模型建立 这个例子采用的是DeepLearning中的经典网络LeNet，关于LeNet可以参考这篇文章。\n3. 模型训练 这里模型训练采用caffe2框架，\n模型在Android端的部署 参考caffe2官网给的AICamera例子，建立Android Studio工程（github工程地址：https://github.com/lyapple2008/MNIST_CNN_APP ），其中最主要的代码如下所示\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 void loadToNetDef(AAssetManager *mgr, caffe2::NetDef *net, const char *filename) { AAsset *asset = AAssetManager_open(mgr, filename, AASSET_MODE_BUFFER); assert(asset != nullptr); const void *data = AAsset_getBuffer(asset); assert(data != nullptr); off_t len = AAsset_getLength(asset); assert(len != 0); if (!net-\u0026gt;ParseFromArray(data, len)) { alog(\u0026#34;Couldn\u0026#39;t parse net from data.\\n\u0026#34;); } AAsset_close(asset); } extern \u0026#34;C\u0026#34; void Java_com_example_beyoung_handwrittendigit_MainActivity_initCaffe2( JNIEnv *env, jobject, jobject assetManager) { AAssetManager *mgr = AAssetManager_fromJava(env, assetManager); alog(\u0026#34;Attempting to load protobuf netdefs...\u0026#34;); loadToNetDef(mgr, \u0026amp;_initNet, \u0026#34;mnist/init_net.pb\u0026#34;); loadToNetDef(mgr, \u0026amp;_predictNet, \u0026#34;mnist/predict_net.pb\u0026#34;); alog(\u0026#34;done.\u0026#34;); alog(\u0026#34;Instantiating predictor...\u0026#34;); _predictor = new caffe2::Predictor(_initNet, _predictNet); if (_predictor) { alog(\u0026#34;done...\u0026#34;); } else { alog(\u0026#34;fail to instantiat predictor...\u0026#34;); } } extern \u0026#34;C\u0026#34; JNIEXPORT jstring JNICALL Java_com_example_beyoung_handwrittendigit_MainActivity_recognitionFromCaffe2( JNIEnv *env, jobject, jint h, jint w, jintArray data) { if (!_predictor) { return env-\u0026gt;NewStringUTF(\u0026#34;Loading...\u0026#34;); } jsize len = env-\u0026gt;GetArrayLength(data); jint *img_data = env-\u0026gt;GetIntArrayElements(data, 0); jint img_size = h * w; assert(img_size \u0026lt;= INPUT_DATA_SIZE); // convert rgb image to grey image and normalize to 0~1 for (auto i = 0; i \u0026lt; h; ++i) { std::ostringstream stringStream; for (auto j = 0; j \u0026lt; w; ++j) { int color = img_data[i * w + j]; //int red = ((color \u0026amp; 0x00FF0000) \u0026gt;\u0026gt; 16); //int green = ((color \u0026amp; 0x0000FF00) \u0026gt;\u0026gt; 8); //int blue = color \u0026amp; 0x000000FF; //float grey = red * 0.3 + green * 0.59 + blue * 0.11; float grey = 0.0; if (color != 0) { grey = 1.0; } input_data[i * w + j] = grey; //alog(\u0026#34;%f\u0026#34;, grey); //alog(\u0026#34;%d\u0026#34;, color); if (color != 0) { color = 1; } stringStream \u0026lt;\u0026lt; color \u0026lt;\u0026lt; \u0026#34; \u0026#34;; } //alog(\u0026#34;\\n\u0026#34;); alog(\u0026#34;%s\u0026#34;, stringStream.str().c_str()); } caffe2::TensorCPU input; input.Resize(std::vector\u0026lt;int\u0026gt;({1, IMG_C, IMG_H, IMG_W})); memcpy(input.mutable_data\u0026lt;float\u0026gt;(), input_data, INPUT_DATA_SIZE * sizeof(float)); caffe2::Predictor::TensorVector input_vec{\u0026amp;input}; caffe2::Predictor::TensorVector output_vec; _predictor-\u0026gt;run(input_vec, \u0026amp;output_vec); constexpr int k = 3; float max[k] = {0}; int max_index[k] = {0}; // Find the top-k result manually if (output_vec.capacity() \u0026gt; 0) { for (auto output : output_vec) { for (auto i = 0; i \u0026lt; output-\u0026gt;size(); ++i) { for (auto j = 0; j \u0026lt; k; ++j) { if (output-\u0026gt;template data\u0026lt;float\u0026gt;()[i] \u0026gt; max[j]) { for (auto _j = k - 1; _j \u0026gt; j; --_j) { max[_j - 1] = max[_j]; max_index[_j - 1] = max_index[_j]; } max[j] = output-\u0026gt;template data\u0026lt;float\u0026gt;()[i]; max_index[j] = i; goto skip; } } skip:; } } } std::ostringstream stringStream; for (auto j = 0; j \u0026lt; k; ++j) { stringStream \u0026lt;\u0026lt; max_index[j] \u0026lt;\u0026lt; \u0026#34;: \u0026#34; \u0026lt;\u0026lt; max[j]*100 \u0026lt;\u0026lt; \u0026#34;%\\n\u0026#34;; } // if (output_vec.capacity() \u0026gt; 0) { // for (auto output: output_vec) { // for (auto i = 0;i\u0026lt;output-\u0026gt;size();++i) { // stringStream \u0026lt;\u0026lt; output-\u0026gt;template data\u0026lt;float\u0026gt;()[i] \u0026lt;\u0026lt; \u0026#34;\\n\u0026#34;; // } // } // } return env-\u0026gt;NewStringUTF(stringStream.str().c_str()); } 总结 通过上面的Demo可以看出，通过MNIST数据训练出来的模型在实际运行的准确率还是很堪忧的。所以一个算法从实验室数据到实际应用还有很长的路要走，虽然最近应用于各个领域的深度学习模型层出不穷，测试数据也很好看，但是在实际应用过程中还有很多路要走。虽然DeepLearning已经表现出很强大的黑魔法属性，在实际应用过程中还是有很多工作要做，不然只能停留在Demo阶段。以本文的手写数字识别为例，实际过程的准确率与测试集上的准确率相差甚远，这时候就需要进行大量的优化工作。由于学习深度学习没多久，暂时只能根据以往在机器学习上的经验来进行优化，目前能想到的优化方向有：训练集与实际运行环境要一致、准备更多的训练集、深度另外的模型方法。\n","permalink":"https://lyapple2008.github.io/posts/201804/2018-04-29-android%E7%AB%AF%E5%AE%9E%E7%8E%B0%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/","summary":"\u003cp\u003e  对DeepLearning最初的印象是，大量的训练样本+机器学习，也就是说原来传统的机器学习会遇到的问题，不能解决的问题，换成DeepLearning同样解决不了。比如目标识别中因为光照变化，目标被遮挡，目标的几何变化造成的识别率大幅下降，在DeepLearning中同样也不能很好解决。但是不是说DeepLearning就一无事处，最近几年这么热也决不是因为名字取得好。DeepLearning比较明显的优势就是在特征选择上，想想之前做生物特征识别时，各种找特征，还得考虑什么光照不变，旋转不变，抗尺寸变换，抗遮挡，那叫一个累呀。现在可好啦，一个Convolution   Layer，再配Fully Connected Layer，最后来个Softmax，丢一堆带标签的样本进去自动给你找出特征。当然这个只是一个接触DeepLearning不到一个月的小白的肤浅认识，大家听听就好。\u003c/p\u003e","title":"Android端实现手写数字识别"},{"content":"在做音视频相关的开发过程大体如下所示，对于其中的编码/解码，整个流程相对比较固定，使用ffmpeg可以很好的完成这部分的开发。对其中的帧数据处理（包括音频和视频数据）则相对要多样化一些，比如对视频做尺寸变换，进行音频音量均衡，直播中的美颜处理，多路流合成等等，这些都是属于流程中的帧数据处理。今天要介绍FFmpeg中的AVFilter模块进行帧数据处理的开发，AVFilter模块对帧数据处理进行了很好的抽象。AVFilter中的filter graph（滤波器图）概念非常适合帧数据处理中的多级滤波处理，同时对滤波器的接口进行了规定，后期添加一些自定义的滤波器也是很方便。网上关于AVFilter的介绍大多是基于ffmpeg的命令使用，基于代码实现的很少，最近项目中正好要使用到了AVFilter，写个小结，希望对有同样需求的小伙伴有帮助。\n原始音视频\u0026ndash;\u0026gt;解码\u0026ndash;\u0026gt;帧数据处理\u0026ndash;\u0026gt;编码\u0026ndash;\u0026gt;输出音视频\n1. 主要结构体和API介绍 1 2 3 4 5 6 // 对filters系统的整体管理 struct AVFilterGraph { AVFilterContext **filters; unsigned nb_filters; } 1 2 3 4 5 6 7 // 定义filter本身的能力，拥有的pads，回调函数接口定义 struct AVFilter { const char *name; const AVFilterPad *inputs; const AVFilterPad *outputs; } 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 // filter实例，管理filter与外部的联系 struct AVFilterContext { const AVFilter *filter; char *name; AVFilterPad *input_pads; AVFilterLink **inputs; unsigned nb_inputs AVFilterPad *output_pads; AVFilterLink **outputs; unsigned nb_outputs; struct AVFilterGraph *graph; } 1 2 3 4 5 6 7 8 9 10 11 // 定义两个filters之间的联接 struct AVFilterLink { AVFilterContext *src; AVFilterPad *srcpad; AVFilterContext *dst; AVFilterPad *dstpad; struct AVFilterGraph *graph; } 1 2 3 4 5 6 7 8 9 // 定义filter的输入/输出接口 struct AVFilterPad { const char *name; AVFrame *(*get_video_buffer)(AVFilterLink *link, int w, int h); AVFrame *(*get_audio_buffer)(AVFilterLink *link, int nb_samples); int (*filter_frame)(AVFilterLink *link, AVFrame *frame); int (*request_frame)(AVFilterLink *link); } 1 2 3 4 5 6 7 struct AVFilterInOut { char *name; AVFilterContext *filter_ctx; int pad_idx; struct AVFilterInOut *next; } 在AVFilter模块中定义了AVFilter结构，很个AVFilter都是具有独立功能的节点，如scale filter的作用就是进行图像尺寸变换，overlay filter的作用就是进行图像的叠加，这里需要重点提的是两个特别的filter，一个是buffer，一个是buffersink，滤波器buffer代表filter graph中的源头，原始数据就往这个filter节点输入的；而滤波器buffersink代表filter graph中的输出节点，处理完成的数据从这个filter节点输出。\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 // 获取FFmpeg中定义的filter，调用该方法前需要先调用avfilter_register_all();进行滤波器注册 AVFilter *avfilter_get_by_name(const char *name); // 往源滤波器buffer中输入待处理的数据 int av_buffersrc_add_frame(AVFilterContext *ctx, AVFrame *frame); // 从目的滤波器buffersink中输出处理完的数据 int av_buffersink_get_frame(AVFilterContext *ctx, AVFrame *frame); // 创建一个滤波器图filter graph AVFilterGraph *avfilter_graph_alloc(void); // 创建一个滤波器实例AVFilterContext，并添加到AVFilterGraph中 int avfilter_graph_create_filter(AVFilterContext **filt_ctx, const AVFilter *filt, const char *name, const char *args, void *opaque, AVFilterGraph *graph_ctx); // 连接两个滤波器节点 int avfilter_link(AVFilterContext *src, unsigned srcpad, AVFilterContext *dst, unsigned dstpad); 2. AVFilter主体框架流程 在利用AVFilter进行音视频数据处理前先将在进行的处理流程绘制出来，现在以FFmpeg filter官方文档中的一个例子为例进行说明。\n1 2 3 4 5 [main] input --\u0026gt; split ---------------------\u0026gt; overlay --\u0026gt; output | ^ |[tmp] [flip]| +-----\u0026gt; crop --\u0026gt; vflip -------+ 这个例子的处理流程如上所示，首先使用split滤波器将input流分成两路流（main和tmp），然后分别对两路流进行处理。对于tmp流，先经过crop滤波器进行裁剪处理，再经过flip滤波器进行垂直方向上的翻转操作，输出的结果命名为flip流。再将main流和flip流输入到overlay滤波器进行合成操作。上图的input就是上面提过的buffer源滤波器，output就是上面的提过的buffersink滤波器。上图中每个节点都是一个AVFilterContext，每个连线就是AVFliterLink。所有这些信息都统一由AVFilterGraph来管理。\n3. 实例实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 extern \u0026#34;C\u0026#34; { #include \u0026lt;libavcodec/avcodec.h\u0026gt; #include \u0026lt;libavformat/avformat.h\u0026gt; #include \u0026lt;libavfilter/avfiltergraph.h\u0026gt; #include \u0026lt;libavfilter/buffersink.h\u0026gt; #include \u0026lt;libavfilter/buffersrc.h\u0026gt; #include \u0026lt;libavutil/opt.h\u0026gt; #include \u0026lt;libavutil/imgutils.h\u0026gt; } int main(int argc, char* argv) { int ret = 0; // input yuv FILE* inFile = NULL; const char* inFileName = \u0026#34;sintel_480x272_yuv420p.yuv\u0026#34;; fopen_s(\u0026amp;inFile, inFileName, \u0026#34;rb+\u0026#34;); if (!inFile) { printf(\u0026#34;Fail to open file\\n\u0026#34;); return -1; } int in_width = 480; int in_height = 272; // output yuv FILE* outFile = NULL; const char* outFileName = \u0026#34;out_crop_vfilter.yuv\u0026#34;; fopen_s(\u0026amp;outFile, outFileName, \u0026#34;wb\u0026#34;); if (!outFile) { printf(\u0026#34;Fail to create file for output\\n\u0026#34;); return -1; } avfilter_register_all(); AVFilterGraph* filter_graph = avfilter_graph_alloc(); if (!filter_graph) { printf(\u0026#34;Fail to create filter graph!\\n\u0026#34;); return -1; } // source filter char args[512]; _snprintf_s(args, sizeof(args), \u0026#34;video_size=%dx%d:pix_fmt=%d:time_base=%d/%d:pixel_aspect=%d/%d\u0026#34;, in_width, in_height, AV_PIX_FMT_YUV420P, 1, 25, 1, 1); AVFilter* bufferSrc = avfilter_get_by_name(\u0026#34;buffer\u0026#34;); AVFilterContext* bufferSrc_ctx; ret = avfilter_graph_create_filter(\u0026amp;bufferSrc_ctx, bufferSrc, \u0026#34;in\u0026#34;, args, NULL, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create filter bufferSrc\\n\u0026#34;); return -1; } // sink filter AVBufferSinkParams *bufferSink_params; AVFilterContext* bufferSink_ctx; AVFilter* bufferSink = avfilter_get_by_name(\u0026#34;buffersink\u0026#34;); enum AVPixelFormat pix_fmts[] = { AV_PIX_FMT_YUV420P, AV_PIX_FMT_NONE }; bufferSink_params = av_buffersink_params_alloc(); bufferSink_params-\u0026gt;pixel_fmts = pix_fmts; ret = avfilter_graph_create_filter(\u0026amp;bufferSink_ctx, bufferSink, \u0026#34;out\u0026#34;, NULL, bufferSink_params, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create filter sink filter\\n\u0026#34;); return -1; } // split filter AVFilter *splitFilter = avfilter_get_by_name(\u0026#34;split\u0026#34;); AVFilterContext *splitFilter_ctx; ret = avfilter_graph_create_filter(\u0026amp;splitFilter_ctx, splitFilter, \u0026#34;split\u0026#34;, \u0026#34;outputs=2\u0026#34;, NULL, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create split filter\\n\u0026#34;); return -1; } // crop filter AVFilter *cropFilter = avfilter_get_by_name(\u0026#34;crop\u0026#34;); AVFilterContext *cropFilter_ctx; ret = avfilter_graph_create_filter(\u0026amp;cropFilter_ctx, cropFilter, \u0026#34;crop\u0026#34;, \u0026#34;out_w=iw:out_h=ih/2:x=0:y=0\u0026#34;, NULL, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create crop filter\\n\u0026#34;); return -1; } // vflip filter AVFilter *vflipFilter = avfilter_get_by_name(\u0026#34;vflip\u0026#34;); AVFilterContext *vflipFilter_ctx; ret = avfilter_graph_create_filter(\u0026amp;vflipFilter_ctx, vflipFilter, \u0026#34;vflip\u0026#34;, NULL, NULL, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create vflip filter\\n\u0026#34;); return -1; } // overlay filter AVFilter *overlayFilter = avfilter_get_by_name(\u0026#34;overlay\u0026#34;); AVFilterContext *overlayFilter_ctx; ret = avfilter_graph_create_filter(\u0026amp;overlayFilter_ctx, overlayFilter, \u0026#34;overlay\u0026#34;, \u0026#34;y=0:H/2\u0026#34;, NULL, filter_graph); if (ret \u0026lt; 0) { printf(\u0026#34;Fail to create overlay filter\\n\u0026#34;); return -1; } // src filter to split filter ret = avfilter_link(bufferSrc_ctx, 0, splitFilter_ctx, 0); if (ret != 0) { printf(\u0026#34;Fail to link src filter and split filter\\n\u0026#34;); return -1; } // split filter\u0026#39;s first pad to overlay filter\u0026#39;s main pad ret = avfilter_link(splitFilter_ctx, 0, overlayFilter_ctx, 0); if (ret != 0) { printf(\u0026#34;Fail to link split filter and overlay filter main pad\\n\u0026#34;); return -1; } // split filter\u0026#39;s second pad to crop filter ret = avfilter_link(splitFilter_ctx, 1, cropFilter_ctx, 0); if (ret != 0) { printf(\u0026#34;Fail to link split filter\u0026#39;s second pad and crop filter\\n\u0026#34;); return -1; } // crop filter to vflip filter ret = avfilter_link(cropFilter_ctx, 0, vflipFilter_ctx, 0); if (ret != 0) { printf(\u0026#34;Fail to link crop filter and vflip filter\\n\u0026#34;); return -1; } // vflip filter to overlay filter\u0026#39;s second pad ret = avfilter_link(vflipFilter_ctx, 0, overlayFilter_ctx, 1); if (ret != 0) { printf(\u0026#34;Fail to link vflip filter and overlay filter\u0026#39;s second pad\\n\u0026#34;); return -1; } // overlay filter to sink filter ret = avfilter_link(overlayFilter_ctx, 0, bufferSink_ctx, 0); if (ret != 0) { printf(\u0026#34;Fail to link overlay filter and sink filter\\n\u0026#34;); return -1; } // check filter graph ret = avfilter_graph_config(filter_graph, NULL); if (ret \u0026lt; 0) { printf(\u0026#34;Fail in filter graph\\n\u0026#34;); return -1; } char *graph_str = avfilter_graph_dump(filter_graph, NULL); FILE* graphFile = NULL; fopen_s(\u0026amp;graphFile, \u0026#34;graphFile.txt\u0026#34;, \u0026#34;w\u0026#34;); fprintf(graphFile, \u0026#34;%s\u0026#34;, graph_str); av_free(graph_str); AVFrame *frame_in = av_frame_alloc(); unsigned char *frame_buffer_in = (unsigned char *)av_malloc(av_image_get_buffer_size(AV_PIX_FMT_YUV420P, in_width, in_height, 1)); av_image_fill_arrays(frame_in-\u0026gt;data, frame_in-\u0026gt;linesize, frame_buffer_in, AV_PIX_FMT_YUV420P, in_width, in_height, 1); AVFrame *frame_out = av_frame_alloc(); unsigned char *frame_buffer_out = (unsigned char *)av_malloc(av_image_get_buffer_size(AV_PIX_FMT_YUV420P, in_width, in_height, 1)); av_image_fill_arrays(frame_out-\u0026gt;data, frame_out-\u0026gt;linesize, frame_buffer_out, AV_PIX_FMT_YUV420P, in_width, in_height, 1); frame_in-\u0026gt;width = in_width; frame_in-\u0026gt;height = in_height; frame_in-\u0026gt;format = AV_PIX_FMT_YUV420P; while (1) { if (fread(frame_buffer_in, 1, in_width*in_height * 3 / 2, inFile) != in_width*in_height * 3 / 2) { break; } //input Y,U,V frame_in-\u0026gt;data[0] = frame_buffer_in; frame_in-\u0026gt;data[1] = frame_buffer_in + in_width*in_height; frame_in-\u0026gt;data[2] = frame_buffer_in + in_width*in_height * 5 / 4; if (av_buffersrc_add_frame(bufferSrc_ctx, frame_in) \u0026lt; 0) { printf(\u0026#34;Error while add frame.\\n\u0026#34;); break; } /* pull filtered pictures from the filtergraph */ ret = av_buffersink_get_frame(bufferSink_ctx, frame_out); if (ret \u0026lt; 0) break; //output Y,U,V if (frame_out-\u0026gt;format == AV_PIX_FMT_YUV420P) { for (int i = 0; i \u0026lt; frame_out-\u0026gt;height; i++) { fwrite(frame_out-\u0026gt;data[0] + frame_out-\u0026gt;linesize[0] * i, 1, frame_out-\u0026gt;width, outFile); } for (int i = 0; i \u0026lt; frame_out-\u0026gt;height / 2; i++) { fwrite(frame_out-\u0026gt;data[1] + frame_out-\u0026gt;linesize[1] * i, 1, frame_out-\u0026gt;width / 2, outFile); } for (int i = 0; i \u0026lt; frame_out-\u0026gt;height / 2; i++) { fwrite(frame_out-\u0026gt;data[2] + frame_out-\u0026gt;linesize[2] * i, 1, frame_out-\u0026gt;width / 2, outFile); } } printf(\u0026#34;Process 1 frame!\\n\u0026#34;); av_frame_unref(frame_out); } fclose(inFile); fclose(outFile); av_frame_free(\u0026amp;frame_in); av_frame_free(\u0026amp;frame_out); avfilter_graph_free(\u0026amp;filter_graph); return 0; } github代码仓库\n4. 建立定义Filter滤波器 这部分暂时还没有实践，参考ffmpeg源码中已有的filter和ffmpeg源码中的文档writing_filter.txt，应该实践起来也来难，等后面有时间再补上这部分介绍。\n","permalink":"https://lyapple2008.github.io/posts/201803/2018-3-24-ffmpeg%E4%B8%ADavfilter%E6%A8%A1%E5%9D%97%E5%AE%9E%E8%B7%B5%E6%8C%87%E5%8D%97/","summary":"\u003cp\u003e在做音视频相关的开发过程大体如下所示，对于其中的编码/解码，整个流程相对比较固定，使用ffmpeg可以很好的完成这部分的开发。对其中的帧数据处理（包括音频和视频数据）则相对要多样化一些，比如对视频做尺寸变换，进行音频音量均衡，直播中的美颜处理，多路流合成等等，这些都是属于流程中的帧数据处理。今天要介绍FFmpeg中的AVFilter模块进行帧数据处理的开发，AVFilter模块对帧数据处理进行了很好的抽象。AVFilter中的filter graph（滤波器图）概念非常适合帧数据处理中的多级滤波处理，同时对滤波器的接口进行了规定，后期添加一些自定义的滤波器也是很方便。网上关于AVFilter的介绍大多是基于ffmpeg的命令使用，基于代码实现的很少，最近项目中正好要使用到了AVFilter，写个小结，希望对有同样需求的小伙伴有帮助。\u003c/p\u003e","title":"FFmpeg中AVFilter模块实践指南"},{"content":"大年初四立个flag，明年来看看能完成多少，看看脸会不会被打肿。\n1. 健身 体重：55Kg 跑步里程：400Km\n2. 小爱好 Ukelele弹： 《活着》《带你去旅行》\n阅读：5篇读书总结\n3. 技术 短视频应用开发 深度学习\n4. 写博客 至少每月一篇博客\n","permalink":"https://lyapple2008.github.io/posts/201802/2018-2-19-2018%E5%B0%8F%E7%9B%AE%E6%A0%87/","summary":"\u003cp\u003e大年初四立个flag，明年来看看能完成多少，看看脸会不会被打肿。\u003c/p\u003e","title":"2018小目标"},{"content":"1. JNI的作用 JNI（Java Native Interface）提供了一种java与C/C++ 代码进行交互的方式，可以通过java方法调用C/C++ 的实现，和已有的C/C++ 库。\n2. JNI使用方法 首先在java类中声明native方法，与一般的函数声明类似，只是多了个native关键字 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 package com.hello.jni; public class HelloJNI { static { System.loadLibrary(\u0026#34;hello\u0026#34;); // Load native library at runtime // hello.dll (Windows) or libhello.so (Unixes) } // Declare a native method sayHello() that receives nothing and returns void private native void sayHello(); // Test Driver public static void main(String[] args) { new HelloJNI().sayHello(); // invoke the native method } } 使用javah命令生成相应的头文件（到elipse工程的bin目录下，打开命令行，注意不用带class后缀） javah com.hello.jni.HelloJNI 这时在bin目录下会生成对应的头文件com_hello_jni_HelloJNI.h\n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 /* DO NOT EDIT THIS FILE - it is machine generated */ #include \u0026lt;jni.h\u0026gt; /* Header for class com_hello_jni_HelloJNI */ #ifndef _Included_com_hello_jni_HelloJNI #define _Included_com_hello_jni_HelloJNI #ifdef __cplusplus extern \u0026#34;C\u0026#34; { #endif /* * Class: com_hello_jni_HelloJNI * Method: sayHello * Signature: ()V */ JNIEXPORT void JNICALL Java_com_hello_jni_HelloJNI_sayHello (JNIEnv *, jobject); #ifdef __cplusplus } #endif #endif 然后就是要去实现之前声明的native方法，这里我们使用Visual Studio新建一个dll工程， 这里需要将之前头文件所依赖的jni.h和jni_md.h所在的jdk目录添加到包含目录中，并将项目类型切换到与平台类型一致，否则会出现这样的不匹配错误 1 2 3 4 5 6 7 #include \u0026#34;com_hello_jni_HelloJNI.h\u0026#34; #include \u0026lt;stdio.h\u0026gt; JNIEXPORT void JNICALL Java_com_hello_jni_HelloJNI_sayHello(JNIEnv *, jobject) { printf(\u0026#34;Hello World\u0026#34;); } 1 2 3 4 5 6 7 Exception in thread \u0026#34;main\u0026#34; java.lang.UnsatisfiedLinkError: D:\\Workspace\\AppProject\\MyProjects\\Debug\\hello.dll: Can\u0026#39;t load IA 32-bit .dll on a AMD 64-bit platform at java.lang.ClassLoader$NativeLibrary.load(Native Method) at java.lang.ClassLoader.loadLibrary0(Unknown Source) at java.lang.ClassLoader.loadLibrary(Unknown Source) at java.lang.Runtime.loadLibrary0(Unknown Source) at java.lang.System.loadLibrary(Unknown Source) at com.hello.jni.HelloJNI.\u0026lt;clinit\u0026gt;(HelloJNI.java:5) 将生成的dll文件的路径添加到环境变量中，这样JAVA类就可以调用dll中的C实现方法了 Hello World\n","permalink":"https://lyapple2008.github.io/posts/201802/2018-2-15-jni%E5%85%A5%E9%97%A8%E4%B9%8Bhelloworld%E7%AF%87/","summary":"\u003ch4 id=\"1-jni的作用\"\u003e1. JNI的作用\u003c/h4\u003e\n\u003cp\u003eJNI（Java Native Interface）提供了一种java与C/C++ 代码进行交互的方式，可以通过java方法调用C/C++ 的实现，和已有的C/C++ 库。\u003c/p\u003e","title":"JNI入门之HelloWorld篇"}]